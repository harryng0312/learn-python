{
 "cells": [
  {
   "cell_type": "code",
   "id": "db8d9365cceec9e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:24.912019Z",
     "start_time": "2024-10-19T11:11:23.398163Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# import os\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from torchvision import datasets\n",
    "# from torchvision.transforms import ToTensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "# matplotlib.use(\"QTAgg\")\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:24.937608Z",
     "start_time": "2024-10-19T11:11:24.917660Z"
    }
   },
   "source": [
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0) / 1024 ** 3, 1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0) / 1024 ** 3, 1), 'GB')\n",
    "    # torch.cuda.is_available()\n",
    "    # torch.cuda.device_count()\n",
    "    # torch.cuda.current_device()\n",
    "    # torch.cuda.device(0)\n",
    "    # torch.cuda.get_device_name(0)\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using mps device\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "2a9447ef65c49589",
   "metadata": {},
   "source": [
    "Each training and test example is assigned to one of the following labels:\n",
    "|Label|Description|\n",
    "|-|-----|\n",
    "|0|T-shirt/top|\n",
    "|1|Trouser|\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag|\n",
    "|9|Ankle boot|"
   ]
  },
  {
   "cell_type": "code",
   "id": "61ada92eda4fabda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:24.976056Z",
     "start_time": "2024-10-19T11:11:24.974202Z"
    }
   },
   "source": [
    "M_LABEL = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
    "           5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:24.983719Z",
     "start_time": "2024-10-19T11:11:24.981088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(kind: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "    fashion_mnist_path = \"/data/sample/fashion_mnist\"\n",
    "    images_path = \"\".join([PROJECT_DIR, fashion_mnist_path, f\"/{kind}-images-idx3-ubyte.gz\"])\n",
    "    labels_path = \"\".join([PROJECT_DIR, fashion_mnist_path, f\"/{kind}-labels-idx1-ubyte.gz\"])\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 28, 28)\n",
    "\n",
    "    return images, labels"
   ],
   "id": "733011bea6f745f3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.181909Z",
     "start_time": "2024-10-19T11:11:24.988679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, Y_train = load_data(kind='train')\n",
    "X_test, Y_test = load_data(kind='t10k')"
   ],
   "id": "4cc681056cce02",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.190297Z",
     "start_time": "2024-10-19T11:11:25.188070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Training labels shape:\", Y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", Y_test.shape)"
   ],
   "id": "346047d794239d84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (60000, 28, 28)\n",
      "Training labels shape: (60000,)\n",
      "Test data shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.252954Z",
     "start_time": "2024-10-19T11:11:25.196465Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test = X_train / 255.0, X_test / 255.0",
   "id": "a535396cc9f9cd23",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.333213Z",
     "start_time": "2024-10-19T11:11:25.259730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ind = 1\n",
    "fig, ax = plt.subplots(figsize=(1.5, 1.5))\n",
    "ax.imshow(X=X_train[ind], cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Y_train[{ind}]:{Y_train[ind]}: {M_LABEL[Y_train[ind]]}\")"
   ],
   "id": "c83a139854f0b4db",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 150x150 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train[1]:0: T-shirt/top\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. PyTorch",
   "id": "e0ecc555bd639849"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1. Define model:",
   "id": "612e594b6b09b430"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.344119Z",
     "start_time": "2024-10-19T11:11:25.341389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=28 * 28, out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=10),\n",
    "            # nn.Softmax(dim=1)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Override forward pass ()\n",
    "        :param x: x input data \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # return logits\n",
    "        return F.log_softmax(logits, dim=1)"
   ],
   "id": "bfaa0cd71df8840",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.362736Z",
     "start_time": "2024-10-19T11:11:25.352515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ],
   "id": "e59fe271eee98a44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2. Define Train and Test functions:",
   "id": "24f2029189f18890"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.712446Z",
     "start_time": "2024-10-19T11:11:25.375493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer_g = torch.optim.Adam(params=model.parameters(), lr=1e-3)"
   ],
   "id": "5b728651deb44a5b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.722062Z",
     "start_time": "2024-10-19T11:11:25.719378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(dataloader: DataLoader, model: nn.Module, loss_fn: CrossEntropyLoss, optimizer: Optimizer) -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device=device), y.to(device=device)\n",
    "        # reset grad before start new batch\n",
    "        optimizer.zero_grad()   \n",
    "        # Compute prediction error\n",
    "        pred:torch.Tensor = model(X)\n",
    "        loss:torch.Tensor = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()     # calculate the final grad to first grad\n",
    "        optimizer.step()    # update W and b\n",
    "        \n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    return"
   ],
   "id": "e2e156962cfa3f84",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.730866Z",
     "start_time": "2024-10-19T11:11:25.728165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test(dataloader: DataLoader, model: nn.Module, loss_fn: CrossEntropyLoss) -> None:\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device=device), y.to(device=device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            pred_probab: torch.Tensor = nn.Softmax(dim=1)(pred)\n",
    "            correct += (pred_probab.argmax(dim=1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return"
   ],
   "id": "bda181affb4b5012",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:11:25.739527Z",
     "start_time": "2024-10-19T11:11:25.736726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Lớp Convolutional 1: đầu vào (1, 28, 28), đầu ra (10, 24, 24)\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # Lớp Convolutional 2: đầu vào (10, 12, 12), đầu ra (20, 8, 8)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # Lớp fully connected 1: đầu vào 320, đầu ra 50\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        # Lớp fully connected 2: đầu vào 50, đầu ra 10\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional + ReLU + Max pooling\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # Convolutional + ReLU + Max pooling\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        # Flatten the tensor\n",
    "        x = x.view(-1, 320)\n",
    "        # Fully connected layer + ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Fully connected layer (Output)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "id": "720a940bdb9a1bf9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:12:20.818081Z",
     "start_time": "2024-10-19T11:12:20.813804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(train_loader, model, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # Xóa gradient\n",
    "        output = model(data)  # Forward pass\n",
    "        loss = F.nll_loss(output, target)  # Tính toán loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Cập nhật các tham số\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "# Kiểm tra mô hình trên tập test\n",
    "def test(test_loader, model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')"
   ],
   "id": "ea22ccf25d0e6d04",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3. Perform train:",
   "id": "3c580d430a28a30d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:12:23.435138Z",
     "start_time": "2024-10-19T11:12:23.262964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rand_seed = np.random.randint(low=0, high=1000)\n",
    "X_train_20 = train_test_split(X_train, test_size=0.2, random_state=rand_seed)[1]\n",
    "Y_train_20 = train_test_split(Y_train, test_size=0.2, random_state=rand_seed)[1]\n",
    "rand_seed = np.random.randint(low=0, high=1000)\n",
    "X_test_20 = train_test_split(X_train, test_size=0.1, random_state=rand_seed)[1]\n",
    "Y_test_20 = train_test_split(Y_train, test_size=0.1, random_state=rand_seed)[1]"
   ],
   "id": "6ebfdc50a960d892",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-19T11:14:11.991733Z",
     "start_time": "2024-10-19T11:13:04.295331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "X_train_tensor, Y_train_tensor = torch.from_numpy(X_train_20).float(), torch.from_numpy(Y_train_20).long()\n",
    "X_test_tensor, Y_test_tensor = torch.from_numpy(X_test_20).float(), torch.from_numpy(Y_test_20).long()\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, Y_train_tensor )\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # train(train_dataloader, model, loss_func, optimizer_g)\n",
    "    # test(test_dataloader, model, loss_func)\n",
    "    train(train_dataloader, model, optimizer_g, t)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")\n"
   ],
   "id": "f6d5b9d5ef4fca2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Epoch: 0 [0/12000] Loss: 2.293509\n",
      "Train Epoch: 0 [3200/12000] Loss: 0.865962\n",
      "Train Epoch: 0 [6400/12000] Loss: 0.579229\n",
      "Train Epoch: 0 [9600/12000] Loss: 0.754481\n",
      "\n",
      "Test set: Average loss: 0.5255, Accuracy: 4866/6000 (81%)\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Epoch: 1 [0/12000] Loss: 0.464463\n",
      "Train Epoch: 1 [3200/12000] Loss: 0.591405\n",
      "Train Epoch: 1 [6400/12000] Loss: 0.538891\n",
      "Train Epoch: 1 [9600/12000] Loss: 0.577575\n",
      "\n",
      "Test set: Average loss: 0.4498, Accuracy: 5026/6000 (84%)\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Epoch: 2 [0/12000] Loss: 0.353124\n",
      "Train Epoch: 2 [3200/12000] Loss: 0.614971\n",
      "Train Epoch: 2 [6400/12000] Loss: 0.580936\n",
      "Train Epoch: 2 [9600/12000] Loss: 0.588719\n",
      "\n",
      "Test set: Average loss: 0.4128, Accuracy: 5109/6000 (85%)\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Epoch: 3 [0/12000] Loss: 0.283742\n",
      "Train Epoch: 3 [3200/12000] Loss: 0.657858\n",
      "Train Epoch: 3 [6400/12000] Loss: 0.579834\n",
      "Train Epoch: 3 [9600/12000] Loss: 0.527850\n",
      "\n",
      "Test set: Average loss: 0.3964, Accuracy: 5145/6000 (86%)\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Epoch: 4 [0/12000] Loss: 0.257926\n",
      "Train Epoch: 4 [3200/12000] Loss: 0.663409\n",
      "Train Epoch: 4 [6400/12000] Loss: 0.531877\n",
      "Train Epoch: 4 [9600/12000] Loss: 0.495499\n",
      "\n",
      "Test set: Average loss: 0.4234, Accuracy: 5110/6000 (85%)\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Epoch: 5 [0/12000] Loss: 0.249636\n",
      "Train Epoch: 5 [3200/12000] Loss: 0.617909\n",
      "Train Epoch: 5 [6400/12000] Loss: 0.613017\n",
      "Train Epoch: 5 [9600/12000] Loss: 0.514512\n",
      "\n",
      "Test set: Average loss: 0.3883, Accuracy: 5182/6000 (86%)\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Epoch: 6 [0/12000] Loss: 0.214177\n",
      "Train Epoch: 6 [3200/12000] Loss: 0.653317\n",
      "Train Epoch: 6 [6400/12000] Loss: 0.622714\n",
      "Train Epoch: 6 [9600/12000] Loss: 0.546015\n",
      "\n",
      "Test set: Average loss: 0.3912, Accuracy: 5224/6000 (87%)\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Epoch: 7 [0/12000] Loss: 0.209914\n",
      "Train Epoch: 7 [3200/12000] Loss: 0.609951\n",
      "Train Epoch: 7 [6400/12000] Loss: 0.613545\n",
      "Train Epoch: 7 [9600/12000] Loss: 0.445056\n",
      "\n",
      "Test set: Average loss: 0.3950, Accuracy: 5201/6000 (87%)\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Epoch: 8 [0/12000] Loss: 0.179432\n",
      "Train Epoch: 8 [3200/12000] Loss: 0.566876\n",
      "Train Epoch: 8 [6400/12000] Loss: 0.626297\n",
      "Train Epoch: 8 [9600/12000] Loss: 0.444063\n",
      "\n",
      "Test set: Average loss: 0.3886, Accuracy: 5226/6000 (87%)\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Epoch: 9 [0/12000] Loss: 0.146656\n",
      "Train Epoch: 9 [3200/12000] Loss: 0.594355\n",
      "Train Epoch: 9 [6400/12000] Loss: 0.485198\n",
      "Train Epoch: 9 [9600/12000] Loss: 0.498721\n",
      "\n",
      "Test set: Average loss: 0.3828, Accuracy: 5267/6000 (88%)\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Epoch: 10 [0/12000] Loss: 0.123772\n",
      "Train Epoch: 10 [3200/12000] Loss: 0.549519\n",
      "Train Epoch: 10 [6400/12000] Loss: 0.552150\n",
      "Train Epoch: 10 [9600/12000] Loss: 0.518429\n",
      "\n",
      "Test set: Average loss: 0.3954, Accuracy: 5247/6000 (87%)\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Epoch: 11 [0/12000] Loss: 0.114336\n",
      "Train Epoch: 11 [3200/12000] Loss: 0.571800\n",
      "Train Epoch: 11 [6400/12000] Loss: 0.550678\n",
      "Train Epoch: 11 [9600/12000] Loss: 0.392875\n",
      "\n",
      "Test set: Average loss: 0.3981, Accuracy: 5268/6000 (88%)\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Epoch: 12 [0/12000] Loss: 0.110538\n",
      "Train Epoch: 12 [3200/12000] Loss: 0.453861\n",
      "Train Epoch: 12 [6400/12000] Loss: 0.527931\n",
      "Train Epoch: 12 [9600/12000] Loss: 0.457853\n",
      "\n",
      "Test set: Average loss: 0.4204, Accuracy: 5232/6000 (87%)\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Epoch: 13 [0/12000] Loss: 0.179820\n",
      "Train Epoch: 13 [3200/12000] Loss: 0.488312\n",
      "Train Epoch: 13 [6400/12000] Loss: 0.493216\n",
      "Train Epoch: 13 [9600/12000] Loss: 0.407445\n",
      "\n",
      "Test set: Average loss: 0.4197, Accuracy: 5262/6000 (88%)\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Epoch: 14 [0/12000] Loss: 0.128661\n",
      "Train Epoch: 14 [3200/12000] Loss: 0.524568\n",
      "Train Epoch: 14 [6400/12000] Loss: 0.480078\n",
      "Train Epoch: 14 [9600/12000] Loss: 0.326480\n",
      "\n",
      "Test set: Average loss: 0.4183, Accuracy: 5270/6000 (88%)\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Epoch: 15 [0/12000] Loss: 0.113547\n",
      "Train Epoch: 15 [3200/12000] Loss: 0.421971\n",
      "Train Epoch: 15 [6400/12000] Loss: 0.571380\n",
      "Train Epoch: 15 [9600/12000] Loss: 0.359898\n",
      "\n",
      "Test set: Average loss: 0.4424, Accuracy: 5232/6000 (87%)\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Epoch: 16 [0/12000] Loss: 0.191311\n",
      "Train Epoch: 16 [3200/12000] Loss: 0.404147\n",
      "Train Epoch: 16 [6400/12000] Loss: 0.377041\n",
      "Train Epoch: 16 [9600/12000] Loss: 0.193405\n",
      "\n",
      "Test set: Average loss: 0.4827, Accuracy: 5230/6000 (87%)\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Epoch: 17 [0/12000] Loss: 0.268285\n",
      "Train Epoch: 17 [3200/12000] Loss: 0.423744\n",
      "Train Epoch: 17 [6400/12000] Loss: 0.625014\n",
      "Train Epoch: 17 [9600/12000] Loss: 0.228392\n",
      "\n",
      "Test set: Average loss: 0.5078, Accuracy: 5249/6000 (87%)\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Epoch: 18 [0/12000] Loss: 0.224555\n",
      "Train Epoch: 18 [3200/12000] Loss: 0.368599\n",
      "Train Epoch: 18 [6400/12000] Loss: 0.459168\n",
      "Train Epoch: 18 [9600/12000] Loss: 0.380984\n",
      "\n",
      "Test set: Average loss: 0.4707, Accuracy: 5276/6000 (88%)\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Epoch: 19 [0/12000] Loss: 0.108510\n",
      "Train Epoch: 19 [3200/12000] Loss: 0.422680\n",
      "Train Epoch: 19 [6400/12000] Loss: 0.452101\n",
      "Train Epoch: 19 [9600/12000] Loss: 0.187274\n",
      "\n",
      "Test set: Average loss: 0.4786, Accuracy: 5300/6000 (88%)\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Epoch: 20 [0/12000] Loss: 0.070632\n",
      "Train Epoch: 20 [3200/12000] Loss: 0.416979\n",
      "Train Epoch: 20 [6400/12000] Loss: 0.390481\n",
      "Train Epoch: 20 [9600/12000] Loss: 0.157984\n",
      "\n",
      "Test set: Average loss: 0.4940, Accuracy: 5264/6000 (88%)\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Epoch: 21 [0/12000] Loss: 0.155667\n",
      "Train Epoch: 21 [3200/12000] Loss: 0.366795\n",
      "Train Epoch: 21 [6400/12000] Loss: 0.303980\n",
      "Train Epoch: 21 [9600/12000] Loss: 0.112824\n",
      "\n",
      "Test set: Average loss: 0.5522, Accuracy: 5198/6000 (87%)\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Epoch: 22 [0/12000] Loss: 0.114553\n",
      "Train Epoch: 22 [3200/12000] Loss: 0.495103\n",
      "Train Epoch: 22 [6400/12000] Loss: 0.411101\n",
      "Train Epoch: 22 [9600/12000] Loss: 0.182452\n",
      "\n",
      "Test set: Average loss: 0.5732, Accuracy: 5144/6000 (86%)\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Epoch: 23 [0/12000] Loss: 0.193276\n",
      "Train Epoch: 23 [3200/12000] Loss: 0.378863\n",
      "Train Epoch: 23 [6400/12000] Loss: 0.312136\n",
      "Train Epoch: 23 [9600/12000] Loss: 0.131521\n",
      "\n",
      "Test set: Average loss: 0.5392, Accuracy: 5265/6000 (88%)\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Epoch: 24 [0/12000] Loss: 0.215692\n",
      "Train Epoch: 24 [3200/12000] Loss: 0.276419\n",
      "Train Epoch: 24 [6400/12000] Loss: 0.311224\n",
      "Train Epoch: 24 [9600/12000] Loss: 0.121734\n",
      "\n",
      "Test set: Average loss: 0.5415, Accuracy: 5259/6000 (88%)\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Epoch: 25 [0/12000] Loss: 0.145174\n",
      "Train Epoch: 25 [3200/12000] Loss: 0.510888\n",
      "Train Epoch: 25 [6400/12000] Loss: 0.274360\n",
      "Train Epoch: 25 [9600/12000] Loss: 0.050625\n",
      "\n",
      "Test set: Average loss: 0.5853, Accuracy: 5228/6000 (87%)\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Epoch: 26 [0/12000] Loss: 0.122204\n",
      "Train Epoch: 26 [3200/12000] Loss: 0.354565\n",
      "Train Epoch: 26 [6400/12000] Loss: 0.307347\n",
      "Train Epoch: 26 [9600/12000] Loss: 0.222245\n",
      "\n",
      "Test set: Average loss: 0.5224, Accuracy: 5272/6000 (88%)\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Epoch: 27 [0/12000] Loss: 0.122994\n",
      "Train Epoch: 27 [3200/12000] Loss: 0.260243\n",
      "Train Epoch: 27 [6400/12000] Loss: 0.278958\n",
      "Train Epoch: 27 [9600/12000] Loss: 0.070141\n",
      "\n",
      "Test set: Average loss: 0.5715, Accuracy: 5275/6000 (88%)\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Epoch: 28 [0/12000] Loss: 0.066435\n",
      "Train Epoch: 28 [3200/12000] Loss: 0.264014\n",
      "Train Epoch: 28 [6400/12000] Loss: 0.261549\n",
      "Train Epoch: 28 [9600/12000] Loss: 0.098071\n",
      "\n",
      "Test set: Average loss: 0.5246, Accuracy: 5320/6000 (89%)\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Epoch: 29 [0/12000] Loss: 0.069092\n",
      "Train Epoch: 29 [3200/12000] Loss: 0.312941\n",
      "Train Epoch: 29 [6400/12000] Loss: 0.267523\n",
      "Train Epoch: 29 [9600/12000] Loss: 0.128586\n",
      "\n",
      "Test set: Average loss: 0.5594, Accuracy: 5302/6000 (88%)\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train Epoch: 30 [0/12000] Loss: 0.072990\n",
      "Train Epoch: 30 [3200/12000] Loss: 0.384383\n",
      "Train Epoch: 30 [6400/12000] Loss: 0.333866\n",
      "Train Epoch: 30 [9600/12000] Loss: 0.146214\n",
      "\n",
      "Test set: Average loss: 0.5337, Accuracy: 5304/6000 (88%)\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train Epoch: 31 [0/12000] Loss: 0.095980\n",
      "Train Epoch: 31 [3200/12000] Loss: 0.211824\n",
      "Train Epoch: 31 [6400/12000] Loss: 0.375706\n",
      "Train Epoch: 31 [9600/12000] Loss: 0.087412\n",
      "\n",
      "Test set: Average loss: 0.5418, Accuracy: 5361/6000 (89%)\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train Epoch: 32 [0/12000] Loss: 0.118419\n",
      "Train Epoch: 32 [3200/12000] Loss: 0.148579\n",
      "Train Epoch: 32 [6400/12000] Loss: 0.417598\n",
      "Train Epoch: 32 [9600/12000] Loss: 0.192152\n",
      "\n",
      "Test set: Average loss: 0.5702, Accuracy: 5321/6000 (89%)\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train Epoch: 33 [0/12000] Loss: 0.114843\n",
      "Train Epoch: 33 [3200/12000] Loss: 0.137179\n",
      "Train Epoch: 33 [6400/12000] Loss: 0.348617\n",
      "Train Epoch: 33 [9600/12000] Loss: 0.030061\n",
      "\n",
      "Test set: Average loss: 0.5556, Accuracy: 5330/6000 (89%)\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train Epoch: 34 [0/12000] Loss: 0.102162\n",
      "Train Epoch: 34 [3200/12000] Loss: 0.173341\n",
      "Train Epoch: 34 [6400/12000] Loss: 0.325087\n",
      "Train Epoch: 34 [9600/12000] Loss: 0.085814\n",
      "\n",
      "Test set: Average loss: 0.6253, Accuracy: 5309/6000 (88%)\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train Epoch: 35 [0/12000] Loss: 0.086430\n",
      "Train Epoch: 35 [3200/12000] Loss: 0.128250\n",
      "Train Epoch: 35 [6400/12000] Loss: 0.222164\n",
      "Train Epoch: 35 [9600/12000] Loss: 0.174361\n",
      "\n",
      "Test set: Average loss: 0.5589, Accuracy: 5319/6000 (89%)\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train Epoch: 36 [0/12000] Loss: 0.060405\n",
      "Train Epoch: 36 [3200/12000] Loss: 0.092897\n",
      "Train Epoch: 36 [6400/12000] Loss: 0.298958\n",
      "Train Epoch: 36 [9600/12000] Loss: 0.102321\n",
      "\n",
      "Test set: Average loss: 0.6189, Accuracy: 5304/6000 (88%)\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train Epoch: 37 [0/12000] Loss: 0.063333\n",
      "Train Epoch: 37 [3200/12000] Loss: 0.094424\n",
      "Train Epoch: 37 [6400/12000] Loss: 0.336553\n",
      "Train Epoch: 37 [9600/12000] Loss: 0.118807\n",
      "\n",
      "Test set: Average loss: 0.6147, Accuracy: 5310/6000 (88%)\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train Epoch: 38 [0/12000] Loss: 0.123993\n",
      "Train Epoch: 38 [3200/12000] Loss: 0.140659\n",
      "Train Epoch: 38 [6400/12000] Loss: 0.110962\n",
      "Train Epoch: 38 [9600/12000] Loss: 0.169368\n",
      "\n",
      "Test set: Average loss: 0.6486, Accuracy: 5280/6000 (88%)\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train Epoch: 39 [0/12000] Loss: 0.074843\n",
      "Train Epoch: 39 [3200/12000] Loss: 0.134799\n",
      "Train Epoch: 39 [6400/12000] Loss: 0.221022\n",
      "Train Epoch: 39 [9600/12000] Loss: 0.037229\n",
      "\n",
      "Test set: Average loss: 0.6455, Accuracy: 5307/6000 (88%)\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train Epoch: 40 [0/12000] Loss: 0.059720\n",
      "Train Epoch: 40 [3200/12000] Loss: 0.149794\n",
      "Train Epoch: 40 [6400/12000] Loss: 0.142632\n",
      "Train Epoch: 40 [9600/12000] Loss: 0.019340\n",
      "\n",
      "Test set: Average loss: 0.5859, Accuracy: 5360/6000 (89%)\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train Epoch: 41 [0/12000] Loss: 0.042043\n",
      "Train Epoch: 41 [3200/12000] Loss: 0.060668\n",
      "Train Epoch: 41 [6400/12000] Loss: 0.099007\n",
      "Train Epoch: 41 [9600/12000] Loss: 0.164755\n",
      "\n",
      "Test set: Average loss: 0.6247, Accuracy: 5324/6000 (89%)\n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train Epoch: 42 [0/12000] Loss: 0.050686\n",
      "Train Epoch: 42 [3200/12000] Loss: 0.103910\n",
      "Train Epoch: 42 [6400/12000] Loss: 0.059914\n",
      "Train Epoch: 42 [9600/12000] Loss: 0.039923\n",
      "\n",
      "Test set: Average loss: 0.6591, Accuracy: 5376/6000 (90%)\n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Train Epoch: 43 [0/12000] Loss: 0.052076\n",
      "Train Epoch: 43 [3200/12000] Loss: 0.153424\n",
      "Train Epoch: 43 [6400/12000] Loss: 0.070044\n",
      "Train Epoch: 43 [9600/12000] Loss: 0.198122\n",
      "\n",
      "Test set: Average loss: 0.6273, Accuracy: 5352/6000 (89%)\n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train Epoch: 44 [0/12000] Loss: 0.041769\n",
      "Train Epoch: 44 [3200/12000] Loss: 0.262915\n",
      "Train Epoch: 44 [6400/12000] Loss: 0.186953\n",
      "Train Epoch: 44 [9600/12000] Loss: 0.070361\n",
      "\n",
      "Test set: Average loss: 0.7253, Accuracy: 5266/6000 (88%)\n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train Epoch: 45 [0/12000] Loss: 0.094893\n",
      "Train Epoch: 45 [3200/12000] Loss: 0.183074\n",
      "Train Epoch: 45 [6400/12000] Loss: 0.052429\n",
      "Train Epoch: 45 [9600/12000] Loss: 0.045661\n",
      "\n",
      "Test set: Average loss: 0.6261, Accuracy: 5362/6000 (89%)\n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train Epoch: 46 [0/12000] Loss: 0.046946\n",
      "Train Epoch: 46 [3200/12000] Loss: 0.125206\n",
      "Train Epoch: 46 [6400/12000] Loss: 0.058825\n",
      "Train Epoch: 46 [9600/12000] Loss: 0.076893\n",
      "\n",
      "Test set: Average loss: 0.6670, Accuracy: 5346/6000 (89%)\n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train Epoch: 47 [0/12000] Loss: 0.065830\n",
      "Train Epoch: 47 [3200/12000] Loss: 0.075184\n",
      "Train Epoch: 47 [6400/12000] Loss: 0.153162\n",
      "Train Epoch: 47 [9600/12000] Loss: 0.184287\n",
      "\n",
      "Test set: Average loss: 0.6329, Accuracy: 5318/6000 (89%)\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train Epoch: 48 [0/12000] Loss: 0.053796\n",
      "Train Epoch: 48 [3200/12000] Loss: 0.125767\n",
      "Train Epoch: 48 [6400/12000] Loss: 0.023160\n",
      "Train Epoch: 48 [9600/12000] Loss: 0.115920\n",
      "\n",
      "Test set: Average loss: 0.6512, Accuracy: 5363/6000 (89%)\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Epoch: 49 [0/12000] Loss: 0.084003\n",
      "Train Epoch: 49 [3200/12000] Loss: 0.089100\n",
      "Train Epoch: 49 [6400/12000] Loss: 0.031352\n",
      "Train Epoch: 49 [9600/12000] Loss: 0.118187\n",
      "\n",
      "Test set: Average loss: 0.6589, Accuracy: 5379/6000 (90%)\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "5227505ea4e11eda",
   "metadata": {},
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab: torch.Tensor = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(dim=1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b490957bdc8f3c78",
   "metadata": {},
   "source": [
    "ts = torch.from_numpy(X_test).float()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "489fb15cf1d6c2e6",
   "metadata": {},
   "source": [
    "torch.manual_seed(1729)\n",
    "r1 = torch.rand(2, 2)\n",
    "print('A random tensor:')\n",
    "print(r1)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
