{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:25:26.863027Z",
     "start_time": "2025-03-25T14:25:20.603955Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:25:29.276733Z",
     "start_time": "2025-03-25T14:25:29.274645Z"
    }
   },
   "source": [
    "NO_SAMPLES = 5000\n",
    "NO_FEATURES = 100"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:25:29.864129Z",
     "start_time": "2025-03-25T14:25:29.861478Z"
    }
   },
   "source": [
    "def create_random_data() -> None:\n",
    "    global x_train, y_train\n",
    "    x_train = np.random.rand(NO_SAMPLES, NO_FEATURES).astype(dtype=np.float16)\n",
    "    y_train = (np.random.randn(NO_SAMPLES, 1) < 0).astype(dtype=np.int32)\n",
    "    return"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:25:33.818333Z",
     "start_time": "2025-03-25T14:25:33.809653Z"
    }
   },
   "source": "# create_random_data()",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:25:37.634047Z",
     "start_time": "2025-03-25T14:25:37.594634Z"
    }
   },
   "source": [
    "X = tf.Variable(initial_value=x_train, dtype=tf.float16, name=\"X\")\n",
    "X.shape"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 21:25:37.602801: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n",
      "2025-03-25 21:25:37.603009: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2025-03-25 21:25:37.603022: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2025-03-25 21:25:37.603355: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-03-25 21:25:37.603371: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([5000, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:30:15.077269Z",
     "start_time": "2025-03-25T14:30:15.072754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "#\n",
    "# sys.getsizeof(X)\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# gpus"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:33:17.038194Z",
     "start_time": "2025-03-25T14:33:17.009788Z"
    }
   },
   "source": [
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "W = tf.Variable(initializer(shape=(X.shape[1], 512), dtype=tf.float16), trainable=True)\n",
    "b = tf.Variable(tf.zeros(shape=(512,), dtype=tf.float16), trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    Z = tf.add(tf.matmul(X, W), b)\n",
    "    A = tf.nn.sigmoid(Z)\n",
    "    pass\n",
    "grads = tape.gradient(target=A, sources=[W, b])\n",
    "grads"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(100, 512), dtype=float16, numpy=\n",
       " array([[589.5, 613.5, 613.5, ..., 607. , 610.5, 609.5],\n",
       "        [602. , 624.5, 624. , ..., 617. , 621. , 620. ],\n",
       "        [598. , 621.5, 621.5, ..., 615. , 619. , 618. ],\n",
       "        ...,\n",
       "        [599.5, 624. , 624. , ..., 618. , 622. , 621. ],\n",
       "        [596. , 619.5, 618. , ..., 612. , 615.5, 616. ],\n",
       "        [605.5, 629. , 627.5, ..., 622. , 626. , 625. ]], dtype=float16)>,\n",
       " <tf.Tensor: shape=(512,), dtype=float16, numpy=\n",
       " array([1194., 1241., 1239., 1154., 1218., 1231., 1244., 1236., 1231.,\n",
       "        1215., 1222., 1228., 1198., 1237., 1238., 1242., 1238., 1200.,\n",
       "        1234., 1235., 1128., 1238., 1236., 1227., 1153., 1115., 1233.,\n",
       "        1178., 1042., 1232., 1204., 1241., 1242., 1212., 1241., 1237.,\n",
       "        1236., 1224., 1137., 1217., 1241., 1241., 1232., 1241., 1241.,\n",
       "        1227., 1232.,  883., 1159., 1196., 1231., 1236., 1206., 1232.,\n",
       "        1200., 1171., 1234., 1161., 1241., 1234., 1241., 1232., 1237.,\n",
       "        1152., 1114., 1220., 1233., 1181., 1219., 1174., 1192., 1240.,\n",
       "        1236., 1168., 1212., 1235., 1240., 1142., 1184., 1241., 1100.,\n",
       "        1241., 1225., 1215., 1207., 1231., 1234., 1237., 1239., 1233.,\n",
       "        1219., 1233., 1175., 1198., 1242., 1224., 1224., 1228., 1161.,\n",
       "        1241., 1193., 1239., 1230., 1240., 1240., 1240., 1197., 1234.,\n",
       "        1232., 1208., 1226., 1229., 1242., 1236., 1216., 1241., 1208.,\n",
       "        1207., 1234., 1241., 1199., 1215., 1234., 1226., 1198., 1240.,\n",
       "        1181., 1232., 1191., 1237., 1239., 1207., 1206., 1236., 1158.,\n",
       "        1216., 1235., 1144., 1241., 1223., 1217., 1231., 1178., 1197.,\n",
       "        1237., 1239., 1164., 1223., 1137., 1229., 1239., 1209., 1213.,\n",
       "        1223., 1189., 1222., 1241., 1149., 1240., 1228., 1226., 1240.,\n",
       "        1225., 1134., 1122., 1241., 1241., 1222., 1200., 1240., 1123.,\n",
       "        1243., 1227., 1236., 1227., 1234., 1236., 1230., 1176., 1236.,\n",
       "        1227., 1240., 1204., 1131., 1224., 1239., 1204., 1231., 1234.,\n",
       "        1223., 1243., 1239., 1201., 1240., 1236., 1236., 1227., 1239.,\n",
       "        1192., 1232., 1242., 1242., 1162., 1231., 1240., 1224., 1238.,\n",
       "        1236., 1186., 1223., 1239., 1240., 1183., 1231., 1212., 1178.,\n",
       "        1221., 1211., 1236., 1240., 1242., 1206., 1215., 1101., 1224.,\n",
       "        1237., 1239., 1046., 1178., 1188., 1227., 1216., 1170., 1227.,\n",
       "        1240., 1241., 1160., 1227., 1230., 1239., 1214., 1239., 1237.,\n",
       "        1232., 1099., 1235., 1234., 1225., 1190., 1140., 1202., 1223.,\n",
       "        1239., 1232., 1207., 1193., 1162., 1223., 1236., 1201., 1229.,\n",
       "        1229., 1240., 1210., 1147., 1203., 1239., 1242., 1233., 1212.,\n",
       "        1145., 1218., 1234., 1183., 1238., 1130., 1239., 1205., 1026.,\n",
       "        1240., 1231., 1242., 1240., 1116., 1099., 1213., 1235., 1238.,\n",
       "        1155., 1233., 1194., 1229., 1071., 1198., 1170., 1139., 1226.,\n",
       "        1234., 1241., 1232., 1241., 1228., 1241., 1214., 1236., 1241.,\n",
       "        1244., 1183., 1236., 1242., 1227., 1239., 1166., 1241., 1225.,\n",
       "        1188., 1210., 1241., 1240., 1207., 1222., 1202., 1239., 1241.,\n",
       "        1189., 1232., 1240., 1239., 1215., 1186., 1242., 1210., 1238.,\n",
       "        1097., 1158., 1233., 1240., 1234., 1240., 1224., 1082., 1219.,\n",
       "        1224., 1196., 1199., 1209., 1227., 1169., 1204., 1239., 1224.,\n",
       "        1235., 1146., 1241., 1208., 1233., 1220., 1148., 1221., 1188.,\n",
       "        1234., 1231., 1189., 1170., 1198., 1223., 1127., 1238., 1227.,\n",
       "        1236., 1161., 1240., 1224., 1207., 1242., 1237., 1189., 1239.,\n",
       "        1166., 1225., 1207., 1222., 1234., 1239., 1232., 1176., 1215.,\n",
       "        1205., 1239., 1199., 1238., 1201., 1242., 1212., 1235., 1228.,\n",
       "        1190., 1229., 1241., 1242., 1194., 1198., 1241., 1242., 1185.,\n",
       "        1225., 1229., 1219., 1241., 1241., 1239., 1234., 1237., 1232.,\n",
       "        1192., 1181., 1204., 1168., 1214., 1229., 1114., 1241., 1240.,\n",
       "        1231., 1224., 1235., 1208., 1189., 1232., 1213., 1241., 1240.,\n",
       "        1186., 1219., 1241., 1241., 1237., 1230., 1202., 1242., 1242.,\n",
       "        1183., 1215., 1237., 1221., 1241., 1231., 1220., 1215., 1241.,\n",
       "        1235., 1145., 1231., 1240., 1228., 1181., 1170., 1227., 1240.,\n",
       "        1220., 1201., 1235., 1206., 1224., 1233., 1242., 1220., 1232.,\n",
       "        1233., 1219., 1183., 1220., 1234., 1187., 1226., 1240., 1185.,\n",
       "        1238., 1224., 1205., 1159., 1139., 1230., 1234., 1240., 1239.,\n",
       "        1195., 1236., 1218., 1240., 1238., 1222., 1238., 1141., 1209.,\n",
       "        1234., 1233., 1242., 1230., 1218., 1124., 1239., 1218., 1226.,\n",
       "        1213., 1031., 1180., 1240., 1203., 1227., 1234., 1233.],\n",
       "       dtype=float16)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:07:29.387061Z",
     "start_time": "2025-03-25T15:07:29.366701Z"
    }
   },
   "source": [
    "# create input data\n",
    "X = tf.random.normal((4, 3), dtype=tf.float32)  # batch_size=4, input_dim=3\n",
    "\n",
    "# init layer 1\n",
    "W1 = tf.Variable(tf.random.normal((3, 5)), trainable=True)  # input_dim=3, hidden_dim=5\n",
    "b1 = tf.Variable(tf.zeros((5,)), trainable=True)\n",
    "\n",
    "# init layer 2\n",
    "W2 = tf.Variable(tf.random.normal((5, 2)), trainable=True)  # hidden_dim=5, output_dim=2\n",
    "b2 = tf.Variable(tf.zeros((2,)), trainable=True)\n",
    "\n",
    "# Forward pass\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # layer 1\n",
    "    Z1 = tf.matmul(X, W1) + b1  # Shape: (4, 5)\n",
    "    # layer 2\n",
    "    Z2 = tf.matmul(Z1, W2) + b2  # Shape: (4, 2)\n",
    "    # loss\n",
    "    loss = tf.reduce_sum(Z2)  # Z2 reduce to scalar\n",
    "\n",
    "dL_dZ1 = tape.gradient(loss, Z1)  # Shape: (4, 5)\n",
    "grads_Z1 = tape.gradient(Z1, [W1, b1], output_gradients=dL_dZ1)\n",
    "\n",
    "grads_Z1_1 = tape.gradient(loss, [W1, b1])\n",
    "\n",
    "print(\"Shape of Z1:\", Z1.shape)\n",
    "print(\"Shape of dL/dZ1 (output_gradients):\", dL_dZ1.shape)\n",
    "print(\"Gradient of W1:\", grads_Z1[0].shape)\n",
    "print(\"Gradient of b1:\", grads_Z1[1].shape)\n",
    "print(\"\\nGradient of W1 (sample):\", grads_Z1[0].numpy())\n",
    "print(\"Gradient of b1 (sample):\", grads_Z1[1].numpy())\n",
    "print(\"\\nGradient of W1 loss:\", grads_Z1_1[0].numpy())\n",
    "print(\"Gradient of W1 loss:\", grads_Z1_1[1].numpy())\n",
    "\n",
    "grads_loss = tape.gradient(loss, [W1, b1])\n",
    "print(\"\\nGradient of W1 loss:\", grads_loss[0].shape)\n",
    "print(\"Gradient of b1 loss:\", grads_loss[1].shape)\n",
    "\n",
    "del tape"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Z1: (4, 5)\n",
      "Shape of dL/dZ1 (output_gradients): (4, 5)\n",
      "Gradient of W1: (3, 5)\n",
      "Gradient of b1: (5,)\n",
      "\n",
      "Gradient of W1 (sample): [[-1.757903   -1.6067742  -2.539984    0.44706905 -1.0642006 ]\n",
      " [ 1.2829103   1.172617    1.8536698  -0.3262691   0.7766491 ]\n",
      " [ 0.8370626   0.76509947  1.2094669  -0.21288133  0.5067416 ]]\n",
      "Gradient of b1 (sample): [-4.0940866 -3.7421136 -5.9155226  1.0412061 -2.4784813]\n",
      "\n",
      "Gradient of W1 loss: [[-1.757903   -1.6067742  -2.539984    0.44706905 -1.0642006 ]\n",
      " [ 1.2829103   1.172617    1.8536698  -0.3262691   0.7766491 ]\n",
      " [ 0.8370626   0.76509947  1.2094669  -0.21288133  0.5067416 ]]\n",
      "Gradient of W1 loss: [-4.0940866 -3.7421136 -5.9155226  1.0412061 -2.4784813]\n",
      "\n",
      "Gradient of W1 loss: (3, 5)\n",
      "Gradient of b1 loss: (5,)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:14:29.897825Z",
     "start_time": "2025-03-25T15:14:29.874249Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.random.normal((4, 3), dtype=tf.float32)  # batch_size=4, input_dim=3\n",
    "y_true = tf.random.normal((4, 2), dtype=tf.float32)  # output_dim=2\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal((3, 5)), trainable=True)  # input_dim=3, hidden_dim1=5\n",
    "b1 = tf.Variable(tf.zeros((5,)), trainable=True)\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal((5, 4)), trainable=True)  # hidden_dim1=5, hidden_dim2=4\n",
    "b2 = tf.Variable(tf.zeros((4,)), trainable=True)\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal((4, 2)), trainable=True)  # hidden_dim2=4, output_dim=2\n",
    "b3 = tf.Variable(tf.zeros((2,)), trainable=True)\n",
    "\n",
    "# Forward pass\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # layer 1\n",
    "    Z1 = tf.matmul(X, W1) + b1  # Shape: (4, 5)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # layer 2\n",
    "    Z2 = tf.matmul(A1, W2) + b2  # Shape: (4, 4)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # layer 3\n",
    "    Z3 = tf.matmul(A2, W3) + b3  # Shape: (4, 2)\n",
    "    # MSE loss\n",
    "    loss = tf.reduce_mean(tf.square(Z3 - y_true))\n",
    "\n",
    "# Calculate gradient loss respect to Z1 (output_gradients)\n",
    "dL_dZ1 = tape.gradient(loss, Z1)  # Shape: (4, 5)\n",
    "\n",
    "# Calculate gradient Z1 respect to W1 and b1, dL_dZ1 as output_gradients\n",
    "grads_Z1 = tape.gradient(Z1, [W1, b1], output_gradients=dL_dZ1)\n",
    "\n",
    "# Calculate gradient loss directly\n",
    "grads_loss = tape.gradient(loss, [W1, b1])\n",
    "\n",
    "print(\"Shape of Z1:\", Z1.shape)\n",
    "print(\"Shape if dL/dZ1 (output_gradients):\", dL_dZ1.shape)\n",
    "print(\"Gradient of W1 respect to Z1:\", grads_Z1[0].shape)\n",
    "print(\"Gradient of b1 respect Z1:\", grads_Z1[1].shape)\n",
    "print(\"\\nGradient of W1 from loss:\", grads_loss[0].shape)\n",
    "print(\"Gradient of b1 from loss:\", grads_loss[1].shape)\n",
    "\n",
    "print(\"\\nGradient of W1 respect to Z1 (sample):\", grads_Z1[0].numpy())\n",
    "print(\"Gradient of b1 respect to Z1 (sample):\", grads_Z1[1].numpy())\n",
    "\n",
    "print(\"\\nGradient of W1 respect to Z1 ():\", grads_loss[0].numpy())\n",
    "print(\"Gradient of b1 respect to Z1 ():\", grads_loss[1].numpy())\n",
    "\n",
    "del tape"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Z1: (4, 5)\n",
      "Shape if dL/dZ1 (output_gradients): (4, 5)\n",
      "Gradient of W1 respect to Z1: (3, 5)\n",
      "Gradient of b1 respect Z1: (5,)\n",
      "\n",
      "Gradient of W1 from loss: (3, 5)\n",
      "Gradient of b1 from loss: (5,)\n",
      "\n",
      "Gradient of W1 respect to Z1 (sample): [[-3.8292165  -1.0301048   0.          0.         -1.81614   ]\n",
      " [ 1.4133823   1.4539416   0.          0.          4.2164764 ]\n",
      " [-2.8116374   0.03549701  0.          0.          2.036872  ]]\n",
      "Gradient of b1 respect to Z1 (sample): [3.8456235 1.6883235 0.        0.        3.772709 ]\n",
      "\n",
      "Gradient of W1 respect to Z1 (): [[-3.8292165  -1.0301048   0.          0.         -1.81614   ]\n",
      " [ 1.4133823   1.4539416   0.          0.          4.2164764 ]\n",
      " [-2.8116374   0.03549701  0.          0.          2.036872  ]]\n",
      "Gradient of b1 respect to Z1 (): [3.8456235 1.6883235 0.        0.        3.772709 ]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:33:57.131781Z",
     "start_time": "2025-03-25T15:33:57.090071Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# input data\n",
    "X = tf.random.normal((4, 3), dtype=tf.float32)  # batch_size=4, input_dim=3\n",
    "y_true = tf.random.normal((4, 2), dtype=tf.float32)  # output_dim=2\n",
    "m = X.shape[0]\n",
    "\n",
    "# init layer 1\n",
    "W1 = tf.Variable(tf.random.normal((3, 5)), trainable=True)  # input_dim=3, hidden_dim1=5\n",
    "b1 = tf.Variable(tf.zeros((5,)), trainable=True)\n",
    "\n",
    "# init layer 2\n",
    "W2 = tf.Variable(tf.random.normal((5, 4)), trainable=True)  # hidden_dim1=5, hidden_dim2=4\n",
    "b2 = tf.Variable(tf.zeros((4,)), trainable=True)\n",
    "\n",
    "# init layer 3\n",
    "W3 = tf.Variable(tf.random.normal((4, 2)), trainable=True)  # hidden_dim2=4, output_dim=2\n",
    "b3 = tf.Variable(tf.zeros((2,)), trainable=True)\n",
    "\n",
    "# L2 regularization\n",
    "l2_lambda = 0.01\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Forward pass and loss\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # layer 1\n",
    "    Z1 = tf.matmul(X, W1) + b1  # Shape: (4, 5)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    # layer 2\n",
    "    Z2 = tf.matmul(A1, W2) + b2  # Shape: (4, 4)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    # layer 3\n",
    "    Z3 = tf.matmul(A2, W3) + b3  # Shape: (4, 2)\n",
    "    # MSE\n",
    "    mse_loss = tf.reduce_mean(tf.square(Z3 - y_true))\n",
    "    # L2 regularization\n",
    "    l2_loss = (l2_lambda / (2 * m)) * (tf.reduce_sum(tf.square(W1)) +\n",
    "                           tf.reduce_sum(tf.square(W2)) +\n",
    "                           tf.reduce_sum(tf.square(W3)))\n",
    "    # loss\n",
    "    loss = mse_loss + l2_loss\n",
    "\n",
    "# Loss gradient respect to Z1 (output_gradients)\n",
    "dL_dZ1 = tape.gradient(loss, Z1)  # Shape: (4, 5)\n",
    "\n",
    "# Z1 gradient respect to W1 and b1, dL_dZ1 as output_gradients\n",
    "grads_Z1 = tape.gradient(Z1, [W1, b1], output_gradients=dL_dZ1)\n",
    "\n",
    "# gradient before update\n",
    "print(\"Trước khi cập nhật:\")\n",
    "print(\"Gradient of W1 respect to Z1:\", grads_Z1[0].shape)\n",
    "print(\"Gradient of b1 respect to Z1:\", grads_Z1[1].shape)\n",
    "print(\"W1 (sample):\", W1.numpy()[:1])  # In một phần của W1\n",
    "print(\"b1 (sample):\", b1.numpy())\n",
    "\n",
    "# update W1 and b1 with optimizer\n",
    "optimizer.apply_gradients(zip(grads_Z1, [W1, b1]))\n",
    "\n",
    "# after update\n",
    "print(\"\\nafter update:\")\n",
    "print(\"W1 (sample):\", W1.numpy()[:1])\n",
    "print(\"b1 (sample):\", b1.numpy())\n",
    "\n",
    "# Calculate gradient directly from loss for comparison\n",
    "grads_loss = tape.gradient(loss, [W1, b1])\n",
    "print(\"\\nGradient of W1 from loss:\", grads_loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trước khi cập nhật:\n",
      "Gradient of W1 respect to Z1: (3, 5)\n",
      "Gradient of b1 respect to Z1: (5,)\n",
      "W1 (sample): [[ 0.22524154  0.16996656  0.23449701  0.19718769 -0.2524347 ]]\n",
      "b1 (sample): [0. 0. 0. 0. 0.]\n",
      "\n",
      "after update:\n",
      "W1 (sample): [[ 0.22524154  0.17996646  0.22449711  0.19718769 -0.2624346 ]]\n",
      "b1 (sample): [ 0.          0.00999991 -0.00999991  0.         -0.00999991]\n",
      "\n",
      "Gradient of W1 from loss: [<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
      "array([[ 5.6310382e-04, -1.1275313e+00,  1.2115437e+00,  4.9296924e-04,\n",
      "         1.1424298e+00],\n",
      "       [ 4.6409899e-03,  1.3486383e+00, -1.4519565e+00,  2.5297445e-03,\n",
      "        -1.2512712e+00],\n",
      "       [-1.2788365e-03, -6.9765943e-01,  7.4312741e-01, -1.7442016e-04,\n",
      "         6.0380077e-01]], dtype=float32)>, <tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
      "array([ 0.       , -1.5626848,  1.6776757,  0.       ,  1.4808562],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T15:33:59.028741Z",
     "start_time": "2025-03-25T15:33:59.026032Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
