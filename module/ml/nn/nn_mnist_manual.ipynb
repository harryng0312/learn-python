{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.555566Z",
     "start_time": "2024-09-29T15:45:34.820185Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "\n",
    "from math import *\n",
    "from array import array\n",
    "\n",
    "# import keras._tf_keras.keras as keras \n",
    "# from keras._tf_keras.keras\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "# matplotlib.use(\"QTAgg\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:\n",
    "- Train data: 60k 28x28 images\n",
    "- Test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.637743Z",
     "start_time": "2024-09-29T15:45:36.559171Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_path = \"/data/sample/mnist\"\n",
    "training_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-images.idx3-ubyte\"])\n",
    "training_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-labels.idx1-ubyte\"])\n",
    "test_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-images.idx3-ubyte\"])\n",
    "test_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-labels.idx1-ubyte\"])\n",
    "\n",
    "def read_images_labels(images_filepath, labels_filepath) -> tuple:\n",
    "    labels = []\n",
    "    with open(labels_filepath, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        # labels = array(\"B\", file.read())\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    with open(images_filepath, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        image_data = array(\"B\", file.read())       \n",
    "     \n",
    "    images = []\n",
    "    # for i in range(size):\n",
    "    #     images.append([0] * rows * cols)\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(28, 28)\n",
    "        # images[i][:] = img\n",
    "        images.append(img)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "    x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "    return (x_train, y_train),(x_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.676247Z",
     "start_time": "2024-09-29T15:45:36.674348Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(f\"{type(X_train[0])}\")\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.766753Z",
     "start_time": "2024-09-29T15:45:36.683007Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train)/255\n",
    "y_train = np.asarray(y_train)\n",
    "X_test  = np.asarray(X_test)/255\n",
    "y_test  = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear:\n",
    "$ \\begin{align}\n",
    "f(\\mathbf z) &= \\mathbf z \\\\\n",
    "\\rightarrow \\frac{\\partial f(\\mathbf z)}{\\partial \\mathbf z} &=\\mathbf 1 \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.775862Z",
     "start_time": "2024-09-29T15:45:36.773983Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear(z): return z\n",
    "def grad_linear(z): return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ReLU:\n",
    "$\\begin{align}\n",
    "ReLU(\\mathbf z) &= \\max(\\mathbf z, \\mathbf 0) \\\\\n",
    "\\rightarrow \\frac{\\partial ReLU(\\mathbf z)}{\\partial \\mathbf z} &= \\begin{cases}\n",
    "z_i = 1 \\text{ if } z_i > 0 \\\\\n",
    "z_i = 0 \\text{ if } z_i \\leqslant 0 \\\\\n",
    "\\end{cases} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.784254Z",
     "start_time": "2024-09-29T15:45:36.782397Z"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z): return np.maximum(0, z)\n",
    "def grad_relu(z): return np.array([1 if z_i > 0 else 0 for z_i in z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.793843Z",
     "start_time": "2024-09-29T15:45:36.790980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "relu_z = relu(z)\n",
    "relu_z\n",
    "grad_relu_z = grad_relu(relu_z)\n",
    "grad_relu_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sigmoid:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\rightarrow \\frac{\\partial\\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(\\mathbf z)\\cdot\\left(1 - \\sigma(\\mathbf z)\\right) \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.810828Z",
     "start_time": "2024-09-29T15:45:36.808950Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): return 1/(1 + np.exp(-z))\n",
    "def grad_sigmoid(z): return sigmoid(z) * (1 - sigmoid(z))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.820940Z",
     "start_time": "2024-09-29T15:45:36.818431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.00246651, 0.04517666, 0.01766271, 0.25      ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "sigmoid(z)\n",
    "grad_sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Softmax:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{e^{\\mathbf z}}{\\sum_{i=1}^{C}e^{z_i}} \\\\\n",
    "\\rightarrow \\frac{\\partial \\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(z_i) \\cdot (\\delta_{ij} - \\sigma(z_j)) \n",
    "\\rightarrow \\delta_{ij} = \\begin{cases} \n",
    "1 \\text{ if } i = j \\\\\n",
    "0 \\text{ if } i \\neq j  \n",
    "\\end{cases} \\\\\n",
    "&= diag(\\mathbf z) - \\mathbf z * \\mathbf z^T \\\\\n",
    "C &\\text{ is number of class} \\\\\n",
    "diag &\\text{ is diagonal matrix }\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.840417Z",
     "start_time": "2024-09-29T15:45:36.837118Z"
    }
   },
   "outputs": [],
   "source": [
    "# def softmax(z): return np.exp(z)/np.sum(np.exp(z))\n",
    "# def grad_softmax(z): return np.diag(z) - np.outer(z,z)\n",
    "def softmax(Z):\n",
    "    Z_max = np.max(Z, axis=Z.ndim-1, keepdims=True)\n",
    "    Z_exp = np.exp(Z - Z_max)\n",
    "    Z_sum = np.sum(Z_exp, axis=Z.ndim-1, keepdims=True)\n",
    "    return Z_exp / Z_sum\n",
    "\n",
    "def grad_softmax(Z):\n",
    "    S = softmax(Z)  # TÃ­nh softmax cho Z\n",
    "    batch_size, num_classes = S.shape\n",
    "    # Init Jacobian matrix: gradient foreach row\n",
    "    dSoftmax = np.zeros((batch_size, num_classes, num_classes))\n",
    "    for i in range(batch_size):\n",
    "        # S_i is softmax for i-row\n",
    "        s_i = S[i].reshape(-1, 1)  # Transpose to Column vector\n",
    "        # Jacobian matrix for i-row\n",
    "        # dSoftmax[i] = np.diagflat(s_i) - np.dot(s_i, s_i.T)\n",
    "        dSoftmax[i] = np.diagflat(s_i) - s_i @ s_i.T\n",
    "    # dSoftmax in hidden layer - transpose of Jacobian matrix    \n",
    "    return dSoftmax.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.850937Z",
     "start_time": "2024-09-29T15:45:36.847845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 3.34679867e-02,  3.34679867e-02],\n",
       "        [-1.09608994e-06, -1.09608994e-06],\n",
       "        [-8.88170879e-03, -8.88170879e-03],\n",
       "        [-2.41429876e-02, -2.41429876e-02],\n",
       "        [-4.42194243e-04, -4.42194243e-04]],\n",
       "\n",
       "       [[-1.09608994e-06, -1.09608994e-06],\n",
       "        [ 3.16139440e-05,  3.16139440e-05],\n",
       "        [-8.09907007e-06, -8.09907007e-06],\n",
       "        [-2.20155550e-05, -2.20155550e-05],\n",
       "        [-4.03228955e-07, -4.03228955e-07]],\n",
       "\n",
       "       [[-8.88170879e-03, -8.88170879e-03],\n",
       "        [-8.09907007e-06, -8.09907007e-06],\n",
       "        [ 1.90551096e-01,  1.90551096e-01],\n",
       "        [-1.78393890e-01, -1.78393890e-01],\n",
       "        [-3.26739807e-03, -3.26739807e-03]],\n",
       "\n",
       "       [[-2.41429876e-02, -2.41429876e-02],\n",
       "        [-2.20155550e-05, -2.20155550e-05],\n",
       "        [-1.78393890e-01, -1.78393890e-01],\n",
       "        [ 2.11440602e-01,  2.11440602e-01],\n",
       "        [-8.88170879e-03, -8.88170879e-03]],\n",
       "\n",
       "       [[-4.42194243e-04, -4.42194243e-04],\n",
       "        [-4.03228955e-07, -4.03228955e-07],\n",
       "        [-3.26739807e-03, -3.26739807e-03],\n",
       "        [-8.88170879e-03, -8.88170879e-03],\n",
       "        [ 1.25917043e-02,  1.25917043e-02]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([[1,-6, 3, 4, 0],\n",
    "              [1,-6, 3, 4, 0]])\n",
    "softmax(z)\n",
    "grad_softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cross Entropy:\n",
    "$\\begin{align}\n",
    "CrossEntropy = - \\log(\\hat{y}_{true})\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Categorical Crossentropy:\n",
    "$\\begin{align}\n",
    "Y &\\text{ is label in one-hot matrix } N \\times C  \\\\\n",
    "\\hat{Y} &\\text{ is predicted matrix } N \\times C \\\\\n",
    "C &\\text{ is number of classes}\\\\\n",
    "L &= -\\sum_{i=1}^{C} Y_i \\log(\\hat{Y}_{i}) \\\\\n",
    "\\hat{Y}_{i,j} &= \\frac{exp(Z_{i,j})}{\\sum_{k=1}^{C} exp(Z_{i,k})} \\\\\n",
    "\\rightarrow \\mathcal L &= -\\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{C} Y_{i,j} \\log(\\hat{Y}_{i,j}) \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{Y}_{i,true}) \\\\\n",
    "\\hat{Y}_{i,true} &\\text{ is predicted result corresponding to one-hot is 1}\n",
    "\\end{align}$\n",
    "\n",
    "Gradient:\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial Z} &= \\hat{Y}_{i,j} - Y_{i,j} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sparse Categorical Crossentropy:\n",
    "$ \\begin{align}\n",
    "\\hat{Y} &= A_n = softmax(Z) \\\\\n",
    "Z &\\text{ is } n \\times C \\text{ matrix. n is number of samples, C is number of classes} \\\\\n",
    "CrossEntropy_i &= -\\log(\\hat{y}_{i, y_{sparse}}) \\\\\n",
    "CrossEntropy &\\text{ is a vector size n} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_{i,j}} &= \\hat{Y}_{i,j} - \\delta(j, y_{sparse,i}) \n",
    "\\rightarrow \\delta(j, y_{sparse,i}) = \\begin{cases}\n",
    "1 \\text{ if } j = y_{sparse,i} \\\\\n",
    "0 \\text{ if } j \\neq y_{sparse,i}\\\\\n",
    "\\end{cases} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_n} &= \\hat{Y} - Y = \\hat{Y} - SparseLabels \\\\\n",
    "SparseLabels &\\text{ can be considered as one-hot matrix}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.878941Z",
     "start_time": "2024-09-29T15:45:36.875529Z"
    }
   },
   "outputs": [],
   "source": [
    "# should apply the Vectorization\n",
    "def delta_kronecker_matrix(y_train, mY_pred):\n",
    "    \"\"\"\n",
    "    Transform to one-hot encoding\n",
    "    y_train: a vector size n\n",
    "    mY_pred: a matrix (C, n)\n",
    "    \"\"\"\n",
    "    mY_train = np.zeros(shape=mY_pred.shape)\n",
    "    for i in range(len(y_train)): mY_train[i][y_train[i]] = 1\n",
    "    return mY_train\n",
    "\n",
    "def sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    y_pred = np.array([mY_pred[i][y_train[i]] for i in range(len(y_train))])\n",
    "    return -np.sum(np.log(y_pred))\n",
    "\n",
    "def grad_sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    return mY_pred - delta_kronecker_matrix(y_train=y_train, mY_pred=mY_pred)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Loss func\n",
    "    loss = -np.log(y_pred[y_pred.shape[0]-1, y_true])\n",
    "    return loss\n",
    "\n",
    "def grad_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Grad\n",
    "    grad = np.zeros_like(y_pred)\n",
    "    grad[y_pred.shape[0]-1, y_true] = -1 / y_pred[y_pred.shape[0]-1, y_true]  # Grad for true-class\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.889585Z",
     "start_time": "2024-09-29T15:45:36.886216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test detal_kronecker_matrix\n",
    "y_train_validate = np.array([0, 2, 2, 1, 0])\n",
    "mY_pred = np.array([[0.2, 0.1 , 0.3],\n",
    "                    [0.3, 0.2, 0.7],\n",
    "                    [0.3, 0.2, 0.7],\n",
    "                    [0.3, 0.2, 0.2],\n",
    "                    [0.3, 0.2, 0.4]])\n",
    "delta = delta_kronecker_matrix(y_train_validate, mY_pred)\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.922002Z",
     "start_time": "2024-09-29T15:45:36.919689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2039728  0.91629073 0.91629073 1.60943791 1.2039728 ]\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = sparse_categorical_crossentropy(y_train_validate, mY_pred)\n",
    "print(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:36.964878Z",
     "start_time": "2024-09-29T15:45:36.957481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 6, 0, 0],\n",
       "       [7, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "# ex\n",
    "# [[0, 0, 0, 0],\n",
    "#  [5, 8, 0, 0],\n",
    "#  [0, 0, 3, 0],\n",
    "#  [0, 6, 0, 0]]\n",
    "#\n",
    "\n",
    "data = np.array([5, 8, 3, 6, 7])        # values\n",
    "indices = np.array([0, 1, 2, 1, 0])     # col index for each value\n",
    "indptr = np.array([0, 2, 3, 4, 5])      # start - end in data values\n",
    "csr_m = csr_matrix((data, indices, indptr), shape=(4, 4))\n",
    "csr_m.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizers: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. SGD\n",
    "$\\begin{align}\n",
    "\\theta &= \\theta - \\eta \\cdot \\nabla_{\\theta} L(\\theta, x_i, y_i) \\\\\n",
    "\\theta &\\text{ is weight}\\\\\n",
    "\\eta &\\text{ is learning rate}\\\\\n",
    "\\nabla_{\\theta} L(\\theta, x_i, y_i) &\\text{ is gradient respect to }\\theta \\text{ of }(x_i, y_i) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RMSProps:\n",
    "$\\begin{align}\n",
    "v_{t-1} &= \\beta v_{t-1} + (1 + \\beta)g_t^2 \\\\\n",
    "\\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "v_t &\\text{ is velocity at } t \\text{ time} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:37.027546Z",
     "start_time": "2024-09-29T15:45:37.022265Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_RMSProps(w: np.ndarray, learning_rate=0.01, beta=0.99, epsilon=1e-7, loss=None, gradient=None):\n",
    "    \"\"\"\n",
    "    update weight matrix or bias\n",
    "    :param w: weight matrix or bias\n",
    "    :param learning_rate: \n",
    "    :param beta: \n",
    "    :param epsilon: default\n",
    "    :param loss: \n",
    "    :param gradient: \n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    v = 0 \n",
    "    epochs = 1000\n",
    "    \n",
    "    # RMSProp\n",
    "    for epoch in range(epochs):\n",
    "        g = gradient(w)\n",
    "        v = beta * v + (1 - beta) * g ** 2\n",
    "        w = w - learning_rate * g / (np.sqrt(v) + epsilon)\n",
    "    \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}: w = {w}, Loss = {loss(w)}')\n",
    "    print(f'Final w: {w}, Final Loss: {loss(w)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Adagrad\n",
    "$\\begin{align}\n",
    "G_t &= G_{t-1} + g_t^2 \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_t - \\epsilon}} \\cdot g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "G &\\text{ sum of square of gradient} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Adaprops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Adamax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.Adam: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Demo NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load/Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Neural network manually:\n",
    "- Flat 28 x 28 data\n",
    "- There are 03 layers: `[32, \"relu\"] [128, \"sigmoid\"] [10, \"softmax\"]`\n",
    "- Loss func: `SparseCategoricalCrossentropy`, `digits = False`\n",
    "- Optimizer: `RMSProp` with `learning_rate=1e-3`\n",
    "- **(opt)** metrics: `accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Flat input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:37.069838Z",
     "start_time": "2024-09-29T15:45:37.066602Z"
    }
   },
   "outputs": [],
   "source": [
    "def flat_data(imp_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flat data from 02 dim matrix to vector\n",
    "    :param imp_data: (n, m_0, m_1) matrix, n is number of rows \n",
    "    :return: matrix: (n, m_0 * m_1)\n",
    "    \"\"\"\n",
    "    return imp_data.reshape((imp_data.shape[0],imp_data.shape[1]* imp_data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T15:45:37.089165Z",
     "start_time": "2024-09-29T15:45:37.082509Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marr = np.array([[[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]]])\n",
    "x_0 = flat_data(marr)\n",
    "x_0\n",
    "# marr.shape\n",
    "# X_train.shape\n",
    "x_1 = X_train[:5].copy()\n",
    "# x_1.shape\n",
    "x_1 = x_1.reshape((x_1.shape[0], x_1.shape[1] * x_1.shape[2]))\n",
    "x_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:09:49.891583Z",
     "start_time": "2024-09-29T16:09:49.876930Z"
    }
   },
   "outputs": [],
   "source": [
    "class DenseLayer (object):\n",
    "    activation_map = {\"linear\": linear, \"relu\": relu, \"sigmoid\": sigmoid, \"softmax\": softmax}\n",
    "    grad_map = {\"linear\": grad_linear, \"relu\": grad_relu, \"sigmoid\": grad_sigmoid, \"softmax\": grad_softmax}\n",
    "    # weights = None;\n",
    "    # bias = None;\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation = 'linear'):\n",
    "        self._activation_func = self.activation_map['linear']\n",
    "        self._grad_func = self.grad_map['linear']\n",
    "        self._activation = activation.lower()\n",
    "        self._weights = np.random.rand(output_size, input_size)\n",
    "        self._bias = np.random.random()\n",
    "        self._init()\n",
    "        return\n",
    "    def _init(self) -> None:\n",
    "        self._activation_func = DenseLayer.activation_map[self._activation] if self._activation in DenseLayer.activation_map else DenseLayer.activation_map['linear']\n",
    "        self._grad_func = DenseLayer.grad_map[self._activation] if self._activation in DenseLayer.grad_map else DenseLayer.grad_map['linear']\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def activation_func(self):\n",
    "        return self._activation_func\n",
    "    @property\n",
    "    def grad_func(self):\n",
    "        return self._grad_func\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "    @weights.setter\n",
    "    def weights(self, val):\n",
    "        self._weights = val\n",
    "    @property\n",
    "    def bias(self):\n",
    "        return self._bias\n",
    "    @bias.setter\n",
    "    def bias(self, val):\n",
    "        self._bias = val\n",
    "    \n",
    "    pass\n",
    "\n",
    "def fit(X_train, Y_train, epochs=1, learning_rate=1e-3) -> list[np.ndarray] | None:\n",
    "    \"\"\"\n",
    "    :param X_train: matrix(n, m) n - number of rows, m - features\n",
    "    :param Y_train: vector(n)\n",
    "    :param epoch: \n",
    "    :return: list of weight matrix, from 0\n",
    "    \"\"\"\n",
    "    model = list[DenseLayer]\n",
    "    # [32, \"relu\"] [128, \"sigmoid\"] [10, \"softmax\"]\n",
    "    # X_treated shape [m, n]\n",
    "    X_treated = flat_data(X_train).transpose()\n",
    "    if X_treated is not None and len(X_treated) > 0:\n",
    "        input_size = X_treated.shape[0]\n",
    "        denses: list[DenseLayer] = [DenseLayer(input_size=input_size, output_size=32, activation='relu'),\n",
    "                  DenseLayer(input_size=32, output_size=128, activation='relu'),\n",
    "                  DenseLayer(input_size=128, output_size=10, activation='linear')]\n",
    "        A = X_treated\n",
    "        fwd_A = [np.ndarray, len(denses)]   # ndarray(m, n)\n",
    "        fwd_Z = [np.ndarray, len(denses)]   # ndarray(m, n)\n",
    "        # fwd_b = [float, len(denses)]        # bias\n",
    "\n",
    "        grad_A: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        grad_Z: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        grad_W: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        grad_b: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        # fwd_A.append(A)\n",
    "        print(f\"denses: {len(denses)}\")\n",
    "        for idx, dense in enumerate(denses):\n",
    "            fwd_A.clear()\n",
    "            fwd_Z.clear()\n",
    "            # Forward propagation\n",
    "            for dense in denses:\n",
    "                # calculate linear\n",
    "                Z = A\n",
    "                # print(f\"dense.weights:{dense.weights.shape} Z:{Z.shape}\")\n",
    "                Z = dense.weights @ Z + dense.bias\n",
    "                # print(f\"fwd {idx}: {Z}\")\n",
    "                # apply activation function\n",
    "                A = dense.activation_func(Z)    # return ndarray\n",
    "                fwd_A.append(A)\n",
    "                fwd_Z.append(Z)\n",
    "                # fwd_b.append(dense.bias.copy())\n",
    "                pass\n",
    "            # Calculate Cost function\n",
    "            # Here, A has shape(C, input_size)\n",
    "            print(f\"A.shape:{A.shape}\")\n",
    "            cost = sparse_categorical_crossentropy(Y_train, A).mean()\n",
    "            print(f\"Cost {idx}: {cost}\")\n",
    "            # grad_cost = grad_sparse_categorical_crossentropy(Y_train, A)\n",
    "            # Combine the grad_sparse_categorical_crossentropy and softmax (~ from_digit = True)\n",
    "            # dL/dZ = dA/dZ @ dL/dA = Y_pred - Y_train\n",
    "            grad_A[-1] = grad_Z[-1] = (fwd_A[-1] - Y_train).T\n",
    "            # print(f\"grad_Z[-1].shape: {grad_Z[-1].shape} fwd_Z[-1].shape:{fwd_Z[-1].shape}\")\n",
    "            grad_W[-1] = grad_Z[-1] @ fwd_Z[-1]\n",
    "            grad_b[-1] = grad_Z[-1]\n",
    "            print(f\"len(grad_Z): {len(grad_Z)}\")\n",
    "            # Back propagation            \n",
    "            for idx, dense in reversed(list(enumerate(denses))):\n",
    "                # pass last layer\n",
    "                if idx >= len(denses) - 1: continue\n",
    "                # calculate grad of activation function\n",
    "                # calculate dA/dZ = grad_func(A[idx])\n",
    "                # grad_dA_dZ = dense.grad_func(fwd_Z[idx])\n",
    "                # calculate dL/dA = dZ[idx + 1]/dA @ dL/dZ[idx + 1]\n",
    "                # print(f\"in {idx}: {fwd_Z[idx].shape} @ {grad_Z[idx + 1].shape}\")\n",
    "                # grad_A[idx] = fwd_Z[idx] @ grad_Z[idx + 1]\n",
    "                grad_A[idx] = denses[idx + 1].weights @ grad_Z[idx + 1]\n",
    "                # calculate dL/dZ = dA/dZ @ dL/dA\n",
    "                print(f\"in {idx} {dense.grad_func}\")\n",
    "                grad_Z[idx] = dense.grad_func(fwd_Z[idx]) @ grad_A[idx]\n",
    "                # calculate dZ/dW = grad_func(W[idx])\n",
    "                grad_dZ_dW = grad_Z[idx] @ fwd_Z[idx-1] if idx > 0 else X_treated\n",
    "                # calculate dZ/db = grad_func(b[idx])\n",
    "                grad_dZ_db = grad_Z[idx] if idx > 0 else np.ones_like(X_treated)\n",
    "                # calculate grad of linear\n",
    "                # grad_Z = \n",
    "                # update W\n",
    "                # fwd_A[idx] -= learning_rate * grad_dZ_dW \n",
    "                dense.weights -= learning_rate * grad_dZ_dW\n",
    "                # update bias\n",
    "                dense.bias -= learning_rate * grad_dZ_db\n",
    "                pass\n",
    "            pass\n",
    "        model.extend(denses)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:09:50.808245Z",
     "start_time": "2024-09-29T16:09:50.805047Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model: list[np.ndarray], X_test, Y_test):\n",
    "    \"\"\"\n",
    "    :param X_test: matrix(n, m) n - number of rows, m - features\n",
    "    :param Y_test: \n",
    "    :return: [lost, accuracy] \n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    rs = [loss, accuracy]\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:09:52.187007Z",
     "start_time": "2024-09-29T16:09:52.113557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the train data\n",
    "X_train_20 = train_test_split(X_train, test_size=0.2, random_state=42)[1]\n",
    "Y_train_20 = train_test_split(y_train, test_size=0.2, random_state=42)[1]\n",
    "\n",
    "len(X_train_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T16:09:54.072150Z",
     "start_time": "2024-09-29T16:09:53.183007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denses: 3\n",
      "A.shape:(10, 12000)\n",
      "Cost 0: -10.868386346776797\n",
      "len(grad_Z): 3\n",
      "in 1: (128, 12000) @ (12000, 10)\n",
      "in 1 <function grad_relu at 0x0000017E5DBBEB00>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rs \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[43], line 110\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(X_train, Y_train, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# calculate dL/dZ = dA/dZ @ dL/dA\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdense\u001b[38;5;241m.\u001b[39mgrad_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m grad_Z[idx] \u001b[38;5;241m=\u001b[39m \u001b[43mdense\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfwd_Z\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m grad_A[idx]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# calculate dZ/dW = grad_func(W[idx])\u001b[39;00m\n\u001b[0;32m    112\u001b[0m grad_dZ_dW \u001b[38;5;241m=\u001b[39m grad_Z[idx] \u001b[38;5;241m@\u001b[39m fwd_Z[idx\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X_treated\n",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m, in \u001b[0;36mgrad_relu\u001b[1;34m(z)\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_relu\u001b[39m(z): \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m z_i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m z_i \u001b[38;5;129;01min\u001b[39;00m z])\n",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_relu\u001b[39m(z): \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mz_i\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m z_i \u001b[38;5;129;01min\u001b[39;00m z])\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "rs = fit(X_train_20, Y_train_20, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "model = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28,)),\n",
    "    tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.linear)\n",
    "    # tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the train data\n",
    "X_train_20 = train_test_split(X_train, test_size=0.2, random_state=42)[1]\n",
    "Y_train_20 = train_test_split(y_train, test_size=0.2, random_state=42)[1]\n",
    "\n",
    "len(X_train_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = model.fit(x=X_train_20, y=Y_train_20, epochs=50, batch_size=600, workers=8, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_20 = train_test_split(X_train, test_size=0.8, random_state=42)[1]\n",
    "Y_test_20 = train_test_split(y_train, test_size=0.8, random_state=42)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = model.evaluate(X_test_20,  Y_test_20, verbose=2)\n",
    "rs  # [lost, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img_data: np.ndarray) -> tuple:\n",
    "    fig, axes = plt.subplots(figsize=(1.60, 1.20))\n",
    "    axes.imshow(X=img_data, cmap=\"gray\")\n",
    "    return fig, axes\n",
    "\n",
    "# print(y_test[5854])\n",
    "show_image(X_test[4823])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Linear and Activation Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loss/Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
