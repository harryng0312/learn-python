{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.682893Z",
     "start_time": "2024-10-04T07:17:03.135592Z"
    }
   },
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "\n",
    "from math import *\n",
    "from array import array\n",
    "\n",
    "# import keras._tf_keras.keras as keras \n",
    "# from keras._tf_keras.keras\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.python.ops.init_ops_v2 import glorot_uniform\n",
    "\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "# matplotlib.use(\"QTAgg\")\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:\n",
    "- Train data: 60k 28x28 images\n",
    "- Test data: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.811097Z",
     "start_time": "2024-10-04T07:17:07.693540Z"
    }
   },
   "source": [
    "mnist_path = \"/data/sample/mnist\"\n",
    "training_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-images.idx3-ubyte\"])\n",
    "training_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-labels.idx1-ubyte\"])\n",
    "test_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-images.idx3-ubyte\"])\n",
    "test_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-labels.idx1-ubyte\"])\n",
    "\n",
    "def read_images_labels(images_filepath, labels_filepath) -> tuple:\n",
    "    labels = []\n",
    "    with open(labels_filepath, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        # labels = array(\"B\", file.read())\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    with open(images_filepath, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        image_data = array(\"B\", file.read())       \n",
    "     \n",
    "    images = []\n",
    "    # for i in range(size):\n",
    "    #     images.append([0] * rows * cols)\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(28, 28)\n",
    "        # images[i][:] = img\n",
    "        images.append(img)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "    x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "    return (x_train, y_train),(x_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.855544Z",
     "start_time": "2024-10-04T07:17:07.853531Z"
    }
   },
   "source": [
    "# print(f\"{type(X_train[0])}\")\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.950699Z",
     "start_time": "2024-10-04T07:17:07.862777Z"
    }
   },
   "source": [
    "X_train = np.asarray(X_train) / 255\n",
    "y_train = np.asarray(y_train)\n",
    "X_test  = np.asarray(X_test) / 255\n",
    "y_test  = np.asarray(y_test)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear:\n",
    "$ \\begin{align}\n",
    "f(\\mathbf z) &= \\mathbf z \\\\\n",
    "\\rightarrow \\frac{\\partial f(\\mathbf z)}{\\partial \\mathbf z} &=\\mathbf 1 \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.960887Z",
     "start_time": "2024-10-04T07:17:07.959046Z"
    }
   },
   "source": [
    "def linear(z): return z\n",
    "def grad_linear(z): return 1"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ReLU:\n",
    "$\\begin{align}\n",
    "ReLU(\\mathbf z) &= \\max(\\mathbf z, \\mathbf 0) \\\\\n",
    "\\rightarrow \\frac{\\partial ReLU(\\mathbf z)}{\\partial \\mathbf z} &= \\begin{cases}\n",
    "z_i = 1 \\text{ if } z_i > 0 \\\\\n",
    "z_i = 0 \\text{ if } z_i \\leqslant 0 \\\\\n",
    "\\end{cases} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.970785Z",
     "start_time": "2024-10-04T07:17:07.968774Z"
    }
   },
   "source": [
    "def relu(z): return np.maximum(0, z)\n",
    "def grad_relu(z): return np.where(z > 0, 1, 0) # (x > 0).astype(float) # np.array([1 if z_i > 0 else 0 for z_i in z])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.982064Z",
     "start_time": "2024-10-04T07:17:07.978597Z"
    }
   },
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "relu_z = relu(z)\n",
    "relu_z\n",
    "grad_relu_z = grad_relu(relu_z)\n",
    "grad_relu_z"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sigmoid:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\rightarrow \\frac{\\partial\\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(\\mathbf z)\\cdot\\left(1 - \\sigma(\\mathbf z)\\right) \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:07.995862Z",
     "start_time": "2024-10-04T07:17:07.993909Z"
    }
   },
   "source": [
    "def sigmoid(z): return 1/(1 + np.exp(-z))\n",
    "def grad_sigmoid(z): return sigmoid(z) * (1 - sigmoid(z))    "
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.030072Z",
     "start_time": "2024-10-04T07:17:08.027144Z"
    }
   },
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "sigmoid(z)\n",
    "grad_sigmoid(z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.00246651, 0.04517666, 0.01766271, 0.25      ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Softmax:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{e^{\\mathbf z}}{\\sum_{i=1}^{C}e^{z_i}} \\\\\n",
    "\\rightarrow \\frac{\\partial \\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(z_i) \\cdot (\\delta_{ij} - \\sigma(z_j)) \n",
    "\\rightarrow \\delta_{ij} = \\begin{cases} \n",
    "1 \\text{ if } i = j \\\\\n",
    "0 \\text{ if } i \\neq j  \n",
    "\\end{cases} \\\\\n",
    "&= diag(\\mathbf z) - \\mathbf z * \\mathbf z^T \\\\\n",
    "C &\\text{ is number of class} \\\\\n",
    "diag &\\text{ is diagonal matrix }\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.055317Z",
     "start_time": "2024-10-04T07:17:08.052132Z"
    }
   },
   "source": [
    "# def softmax(z): return np.exp(z)/np.sum(np.exp(z))\n",
    "# def grad_softmax(z): return np.diag(z) - np.outer(z,z)\n",
    "def softmax(Z):\n",
    "    Z_max = np.max(Z, axis=-1, keepdims=True)\n",
    "    Z_exp = np.exp(Z - Z_max)\n",
    "    Z_sum = np.sum(Z_exp, axis=-1, keepdims=True)\n",
    "    return Z_exp / Z_sum\n",
    "    # e_x = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    # return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def grad_softmax(Z):\n",
    "    S = softmax(Z)  # TÃ­nh softmax cho Z\n",
    "    batch_size, num_classes = S.shape\n",
    "    # Init Jacobian matrix: gradient foreach row\n",
    "    dSoftmax = np.zeros((batch_size, num_classes, num_classes))\n",
    "    for i in range(batch_size):\n",
    "        # S_i is softmax for i-row\n",
    "        s_i = S[i].reshape(-1, 1)  # Transpose to Column vector\n",
    "        # Jacobian matrix for i-row\n",
    "        # dSoftmax[i] = np.diagflat(s_i) - np.dot(s_i, s_i.T)\n",
    "        dSoftmax[i] = np.diagflat(s_i) - s_i @ s_i.T\n",
    "    # dSoftmax in hidden layer - transpose of Jacobian matrix    \n",
    "    return dSoftmax.transpose()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.066753Z",
     "start_time": "2024-10-04T07:17:08.063659Z"
    }
   },
   "source": [
    "z = np.array([[1,-6, 3, 4, 0],\n",
    "              [1,-6, 3, 7, 0]])\n",
    "# softmax(z)\n",
    "z_max = np.max(z, axis=z.ndim-1, keepdims=True)\n",
    "z_max\n",
    "# np.exp(z-z_max)\n",
    "# grad_softmax(z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [7]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cross Entropy:\n",
    "$\\begin{align}\n",
    "CrossEntropy = - \\log(\\hat{y}_{true})\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Categorical Crossentropy:\n",
    "$\\begin{align}\n",
    "Y &\\text{ is label in one-hot matrix } N \\times C  \\\\\n",
    "\\hat{Y} &\\text{ is predicted matrix } N \\times C \\\\\n",
    "C &\\text{ is number of classes}\\\\\n",
    "L &= -\\sum_{i=1}^{C} Y_i \\log(\\hat{Y}_{i}) \\\\\n",
    "\\hat{Y}_{i,j} &= \\frac{exp(Z_{i,j})}{\\sum_{k=1}^{C} exp(Z_{i,k})} \\\\\n",
    "\\rightarrow \\mathcal L &= -\\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{C} Y_{i,j} \\log(\\hat{Y}_{i,j}) \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{Y}_{i,true}) \\\\\n",
    "\\hat{Y}_{i,true} &\\text{ is predicted result corresponding to one-hot is 1}\n",
    "\\end{align}$\n",
    "\n",
    "Gradient:\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial Z} &= \\hat{Y}_{i,j} - Y_{i,j} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sparse Categorical Crossentropy:\n",
    "$ \\begin{align}\n",
    "\\hat{Y} &= A_n = softmax(Z) \\\\\n",
    "Z &\\text{ is } n \\times C \\text{ matrix. n is number of samples, C is number of classes} \\\\\n",
    "CrossEntropy_i &= -\\log(\\hat{y}_{i, y_{sparse}}) \\\\\n",
    "CrossEntropy &\\text{ is a vector size n} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_{i,j}} &= \\hat{Y}_{i,j} - \\delta(j, y_{sparse,i}) \n",
    "\\rightarrow \\delta(j, y_{sparse,i}) = \\begin{cases}\n",
    "1 \\text{ if } j = y_{sparse,i} \\\\\n",
    "0 \\text{ if } j \\neq y_{sparse,i}\\\\\n",
    "\\end{cases} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_n} &= \\hat{Y} - Y = \\hat{Y} - SparseLabels \\\\\n",
    "SparseLabels &\\text{ can be considered as one-hot matrix}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.119653Z",
     "start_time": "2024-10-04T07:17:08.116088Z"
    }
   },
   "source": [
    "# should apply the Vectorization\n",
    "def delta_kronecker_matrix(y_train, mY_pred):\n",
    "    \"\"\"\n",
    "    Transform to one-hot encoding\n",
    "    y_train: a vector size n\n",
    "    mY_pred: a matrix (C, n)\n",
    "    \"\"\"\n",
    "    mY_train = np.zeros(shape=mY_pred.T.shape)\n",
    "    for i in range(len(y_train)): mY_train[i][y_train[i]] = 1\n",
    "    return mY_train\n",
    "\n",
    "def sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    y_pred = np.array([mY_pred[i][y_train[i]] for i in range(len(y_train))])\n",
    "    return -np.sum(np.log(y_pred))\n",
    "\n",
    "def grad_sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    return mY_pred - delta_kronecker_matrix(y_train=y_train, mY_pred=mY_pred)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Loss func\n",
    "    # batch_size = y_pred.shape[0]\n",
    "    # y_true_indices = (np.arange(batch_size), y_true)\n",
    "    # correct_class_probabilities = y_pred[y_true_indices]\n",
    "    # loss = -np.log(correct_class_probabilities + 1e-9)\n",
    "    # loss = -np.log(y_pred[:y_pred.shape[0], y_true] + 1e-9)\n",
    "    # return loss\n",
    "    batch_size = y_true.shape[0]\n",
    "    y_true_indices = (y_true, np.arange(batch_size))\n",
    "    correct_class_probabilities = y_pred[y_true_indices]\n",
    "    # loss = -np.sum(np.log(correct_class_probabilities + 1e-9)) / batch_size\n",
    "    loss = -np.log(correct_class_probabilities + 1e-7).mean()\n",
    "    return loss\n",
    "\n",
    "def grad_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Grad\n",
    "    grad = np.zeros_like(y_pred)\n",
    "    grad[:y_pred.shape[0], y_true] = -1 / y_pred[:y_pred.shape[0], y_true]  # Grad for true-class\n",
    "    return grad"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.226515Z",
     "start_time": "2024-10-04T07:17:08.224374Z"
    }
   },
   "source": [
    "# test detal_kronecker_matrix\n",
    "y_train_validate = np.array([0, 2, 2, 1, 0])\n",
    "mY_pred = np.array([[0.2, 0.1 , 0.3],\n",
    "                    [0.3, 0.2, 0.7],\n",
    "                    [0.3, 0.2, 0.7],\n",
    "                    [0.3, 0.2, 0.2],\n",
    "                    [0.3, 0.2, 0.4]]).T\n",
    "delta = delta_kronecker_matrix(y_train_validate, mY_pred)\n",
    "# mY_pred[:mY_pred.shape[0], [1, 2]]"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.328310Z",
     "start_time": "2024-10-04T07:17:08.326121Z"
    }
   },
   "source": [
    "cross_entropy = sparse_categorical_crossentropy(y_train_validate, mY_pred)\n",
    "print(cross_entropy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0272393796048616\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.416969Z",
     "start_time": "2024-10-04T07:17:08.407347Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "# ex\n",
    "# [[0, 0, 0, 0],\n",
    "#  [5, 8, 0, 0],\n",
    "#  [0, 0, 3, 0],\n",
    "#  [0, 6, 0, 0]]\n",
    "#\n",
    "\n",
    "data = np.array([5, 8, 3, 6, 7])        # values\n",
    "indices = np.array([0, 1, 2, 1, 0])     # col index for each value\n",
    "indptr = np.array([0, 2, 3, 4, 5])      # start - end in data values\n",
    "csr_m = csr_matrix((data, indices, indptr), shape=(4, 4))\n",
    "csr_m.toarray()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 6, 0, 0],\n",
       "       [7, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizers: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.475731Z",
     "start_time": "2024-10-04T07:17:08.472607Z"
    }
   },
   "source": [
    "def optimize_basicGD(w:np.ndarray, b:np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, learning_rate=1e-2):\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "    return w, b"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. SGD\n",
    "$\\begin{align}\n",
    "\\theta &= \\theta - \\eta \\cdot \\nabla_{\\theta} L(\\theta, x_i, y_i) \\\\\n",
    "\\theta &\\text{ is weight}\\\\\n",
    "\\eta &\\text{ is learning rate}\\\\\n",
    "\\nabla_{\\theta} L(\\theta, x_i, y_i) &\\text{ is gradient respect to }\\theta \\text{ of }(x_i, y_i) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RMSProps:\n",
    "$\\begin{align}\n",
    "v_{t} &= \\beta v_{t-1} + (1 + \\beta)g_t^2 \\\\\n",
    "\\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "v_t &\\text{ is velocity at } t \\text{ time} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.544570Z",
     "start_time": "2024-10-04T07:17:08.538977Z"
    }
   },
   "source": [
    "# def optimize_RMSProps(w: np.ndarray, learning_rate=0.01, beta=0.99, epsilon=1e-7, loss=None, gradient=None):\n",
    "    # \"\"\"\n",
    "    # update weight matrix or bias\n",
    "    # :param w: weight matrix or bias\n",
    "    # :param learning_rate: \n",
    "    # :param beta: \n",
    "    # :param epsilon: default\n",
    "    # :param loss: \n",
    "    # :param gradient: \n",
    "    # :return: None\n",
    "    # \"\"\"\n",
    "    # v = 0 \n",
    "    # epochs = 1000\n",
    "    \n",
    "    # RMSProp\n",
    "    # for epoch in range(epochs):\n",
    "        # g = gradient(w)\n",
    "        # v = beta * v + (1 - beta) * g ** 2\n",
    "        # w = w - learning_rate * g / (np.sqrt(v) + epsilon)\n",
    "    \n",
    "        # if epoch % 100 == 0:\n",
    "        #     print(f'Epoch {epoch}: w = {w}, Loss = {loss(w)}')\n",
    "    # print(f'Final w: {w}, Final Loss: {loss(w)}')\n",
    "\n",
    "def optimize_RMSProps(w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, v_w:np.ndarray=None, v_b:np.ndarray=None,\n",
    "                      learning_rate=0.01, beta=0.9, epsilon=1e-7):\n",
    "    # if v_w is None: v_w = np.zeros_like(w)\n",
    "    # if v_b is None: v_b = np.zeros_like(b)\n",
    "    v_w = beta * v_w + (1 - beta) * grad_w ** 2\n",
    "    v_b = beta * v_b + (1 - beta) * grad_b ** 2\n",
    "    # print(f\"v_w: {v_w.shape} v_b: {v_b.shape}\")\n",
    "    w -= learning_rate * grad_w / (np.sqrt(v_w) + epsilon)\n",
    "    b -= learning_rate * grad_b / (np.sqrt(v_b) + epsilon)\n",
    "    return w, b, v_w, v_b\n",
    "\n",
    "def optimize_RMSPropsL1(w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, v_w:np.ndarray=None, v_b:np.ndarray=None,\n",
    "                      learning_rate=0.01, beta=0.9, epsilon=1e-7, lambda_l1 = 1e-2, loss=0.0):\n",
    "    # l1_loss = lambda_l1 * np.sum(np.abs(w))\n",
    "    l1_loss = lambda_l1 * np.sum(w**2)\n",
    "    loss += l1_loss\n",
    "    grad_w += lambda_l1 * np.sign(w)\n",
    "    # grad_b += lambda_l1 * np.sign(b)\n",
    "    v_w = beta * v_w + (1 - beta) * grad_w ** 2\n",
    "    v_b = beta * v_b + (1 - beta) * grad_b ** 2\n",
    "    # print(f\"v_w: {v_w.shape} v_b: {v_b.shape}\")\n",
    "    w -= learning_rate * grad_w / (np.sqrt(v_w) + epsilon)\n",
    "    b -= learning_rate * grad_b / (np.sqrt(v_b) + epsilon)\n",
    "    return w, b, v_w, v_b, loss + l1_loss"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Adagrad\n",
    "$\\begin{align}\n",
    "G_t &= G_{t-1} + g_t^2 \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_t - \\epsilon}} \\cdot g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "G &\\text{ sum of square of gradient} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Adaprops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Adamax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.Adam: "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.591836Z",
     "start_time": "2024-10-04T07:17:08.588609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optimize_Adam(w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray,\n",
    "                    cache_w: list[np.ndarray], cache_b:list[np.ndarray],\n",
    "                    learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    # if v_w is None: v_w = np.zeros_like(w)\n",
    "    # if v_b is None: v_b = np.zeros_like(b)\n",
    "    cache_w[1] = beta1 * cache_w[1] + (1 - beta1) * grad_w\n",
    "    cache_b[1] = beta1 * cache_b[1] + (1 - beta1) * grad_b\n",
    "    cache_w[0] = beta2 * cache_w[0] + (1 - beta2) * grad_w ** 2\n",
    "    cache_b[0] = beta2 * cache_b[0] + (1 - beta2) * grad_b ** 2\n",
    "    m_w_hat = cache_w[1] / (1 - beta1)\n",
    "    m_b_hat = cache_b[1] / (1 - beta1)\n",
    "    v_w_hat = cache_w[0] / (1 - beta2)\n",
    "    v_b_hat = cache_b[0] / (1 - beta2)\n",
    "    # print(f\"v_w: {v_w.shape} v_b: {v_b.shape}\")\n",
    "    w -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "    b -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "    return w, b, cache_w, cache_b"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Demo NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load/Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Neural network manually:\n",
    "- Flat 28 x 28 data\n",
    "- There are 03 layers: `[32, \"relu\"] [128, \"sigmoid\"] [10, \"softmax\"]`\n",
    "- Loss func: `SparseCategoricalCrossentropy`, `digits = False`\n",
    "- Optimizer: `RMSProp` with `learning_rate=1e-3`\n",
    "- **(opt)** metrics: `accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Flat input data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.606686Z",
     "start_time": "2024-10-04T07:17:08.604066Z"
    }
   },
   "source": [
    "def flat_data(imp_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flat data from 02 dim matrix to vector\n",
    "    :param imp_data: (n, m_0, m_1) matrix, n is number of rows \n",
    "    :return: matrix: (n, m_0 * m_1)\n",
    "    \"\"\"\n",
    "    if len(imp_data.shape) < 3 : return imp_data\n",
    "    return imp_data.reshape((imp_data.shape[0], imp_data.shape[1]* imp_data.shape[2]))"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.636796Z",
     "start_time": "2024-10-04T07:17:08.632455Z"
    }
   },
   "source": [
    "marr = np.array([[[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]],\n",
    "        [[1, 2, 3, 3],[4, 5, 6, 6],[7, 8, 9, 9]]])\n",
    "x_0 = flat_data(marr)\n",
    "x_0\n",
    "# marr.shape\n",
    "# X_train.shape\n",
    "x_1 = X_train[:5].copy()\n",
    "x_1.shape\n",
    "x_1 = x_1.reshape((x_1.shape[0], x_1.shape[1] * x_1.shape[2]))\n",
    "x_1"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.688829Z",
     "start_time": "2024-10-04T07:17:08.677898Z"
    }
   },
   "source": [
    "class DenseLayer (object):\n",
    "    activation_map = {\"linear\": linear, \"relu\": relu, \"sigmoid\": sigmoid, \"softmax\": softmax}\n",
    "    grad_map = {\"linear\": grad_linear, \"relu\": grad_relu, \"sigmoid\": grad_sigmoid, \"softmax\": grad_softmax}\n",
    "    # weights = None;\n",
    "    # bias = None;\n",
    "\n",
    "    def glorot_normal(self, shape, n_in, n_out):\n",
    "        stddev = np.sqrt(2 / (n_in + n_out))\n",
    "        return np.random.normal(loc=0, scale=stddev, size=shape)\n",
    "\n",
    "    def glorot_uniform(self, shape, n_in, n_out):\n",
    "        limit = np.sqrt(6 / (n_in + n_out))\n",
    "        return np.random.uniform(low=-limit, high=limit, size=shape)\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation = 'linear'):\n",
    "        self._activation_func = self.activation_map['linear']\n",
    "        self._grad_func = self.grad_map['linear']\n",
    "        self._activation = activation.lower()\n",
    "        # self._weights = np.random.rand(output_size, input_size) * 1e-2\n",
    "        # self._weights = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
    "        self._weights = self.glorot_uniform(shape=(output_size, input_size), n_in=input_size, n_out=output_size)\n",
    "        # self._bias = np.random.rand(output_size, 1)\n",
    "        # self._bias = np.random.randn(output_size, 1) * np.sqrt(2 / input_size)\n",
    "        self._bias = np.zeros(shape=(output_size, 1))\n",
    "        self._cache_w = [np.zeros_like(self._weights), np.zeros_like(self._weights)]\n",
    "        self._cache_b = [np.zeros_like(self._bias), np.zeros_like(self._bias)]\n",
    "        self._init()\n",
    "        return\n",
    "    def _init(self) -> None:\n",
    "        self._activation_func = DenseLayer.activation_map[self._activation] if self._activation in DenseLayer.activation_map else DenseLayer.activation_map['linear']\n",
    "        self._grad_func = DenseLayer.grad_map[self._activation] if self._activation in DenseLayer.grad_map else DenseLayer.grad_map['linear']\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def activation_func(self):\n",
    "        return self._activation_func\n",
    "    @property\n",
    "    def grad_func(self):\n",
    "        return self._grad_func\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> np.ndarray:\n",
    "        return self._weights\n",
    "    @weights.setter\n",
    "    def weights(self, val):\n",
    "        self._weights = val\n",
    "    @property\n",
    "    def bias(self) -> np.ndarray:\n",
    "        return self._bias\n",
    "    @bias.setter\n",
    "    def bias(self, val):\n",
    "        self._bias = val\n",
    "\n",
    "    @property\n",
    "    def cache_w(self):\n",
    "        return self._cache_w\n",
    "    @cache_w.setter\n",
    "    def cache_w(self, val):\n",
    "        self._cache_w = val\n",
    "    @property\n",
    "    def cache_b(self):\n",
    "        return self._cache_b\n",
    "    @cache_b.setter\n",
    "    def cache_b(self, val):\n",
    "        self._cache_b = val\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.856746Z",
     "start_time": "2024-10-04T07:17:08.844167Z"
    }
   },
   "source": [
    "def fit(X_train, Y_train, epochs=1, learning_rate=1e-3) -> list[DenseLayer]:\n",
    "    \"\"\"\n",
    "    :param X_train: matrix(n, m) n - number of rows, m - features\n",
    "    :param Y_train: vector(n)\n",
    "    :param epoch: \n",
    "    :return: list of weight matrix, from 0\n",
    "    \"\"\"\n",
    "    model: list[DenseLayer] = []\n",
    "    # [32, \"relu\"] [128, \"sigmoid\"] [10, \"softmax\"]\n",
    "    # X_treated shape [m, n]\n",
    "    X_treated = flat_data(X_train).transpose()\n",
    "    if X_treated is not None and len(X_treated) > 0:\n",
    "        input_size = X_treated.shape[0]\n",
    "        denses: list[DenseLayer] = [DenseLayer(input_size=input_size, output_size=32, activation='relu'),\n",
    "                  DenseLayer(input_size=32, output_size=128, activation='relu'),\n",
    "                  DenseLayer(input_size=128, output_size=10, activation='linear')]\n",
    "        # denses: list[DenseLayer] = [DenseLayer(input_size=input_size, output_size=3, activation='relu'),\n",
    "        #         #   DenseLayer(input_size=32, output_size=128, activation='relu'),\n",
    "        #           DenseLayer(input_size=3, output_size=2, activation='linear')]\n",
    "        fwd_A = [np.ndarray, len(denses)]   # ndarray(m, n)\n",
    "        fwd_Z = [np.ndarray, len(denses)]   # ndarray(m, n)\n",
    "        # fwd_b = [float, len(denses)]        # bias\n",
    "\n",
    "        grad_A: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        grad_Z: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        # grad_W: list[np.ndarray] = [np.zeros(shape=(1,1))] * len(denses)\n",
    "        # grad_b: list[np.ndarray] = [np.zeros(shape=(1,))] * len(denses)\n",
    "        # fwd_A.append(A)\n",
    "        # print(f\"denses: {len(denses)}\")\n",
    "        for epoch in range(epochs):\n",
    "            fwd_A.clear()\n",
    "            fwd_Z.clear()\n",
    "            A = X_treated\n",
    "            # fwd_Z.append(X_treated)\n",
    "            # Forward propagation\n",
    "            for dense in denses:\n",
    "                # calculate linear\n",
    "                Z = A\n",
    "                # print(f\"dense.weights:{dense.weights.shape} Z:{Z.shape} bias:{dense.bias.shape}\")\n",
    "                Z = dense.weights @ Z + dense.bias\n",
    "                # print(f\"fwd {idx}: {Z}\")\n",
    "                # apply activation function\n",
    "                A = dense.activation_func(Z)    # return ndarray\n",
    "                fwd_A.append(A)\n",
    "                fwd_Z.append(Z)\n",
    "                # fwd_b.append(dense.bias.copy())\n",
    "                pass\n",
    "            # Calculate Cost function\n",
    "            # Here, A has shape(C, input_size)\n",
    "            A = softmax(A)\n",
    "            loss = sparse_categorical_crossentropy(Y_train, A)\n",
    "            # grad_cost = grad_sparse_categorical_crossentropy(Y_train, A)\n",
    "            # Combine the grad_sparse_categorical_crossentropy and softmax (~ from_digit = True)\n",
    "            # dL/dZ = dA/dZ @ dL/dA = Y_pred - Y_train\n",
    "            # grad_A[-1] = grad_Z[-1] = (fwd_A[-1] - Y_train)/m\n",
    "            # print(f\"grad_Z:{fwd_A[-1].shape} - {Y_train.shape}\")\n",
    "            # grad_Z[-1] = (fwd_A[-1] - Y_train) / Y_train.shape[0]\n",
    "            batch_size = Y_train.shape[0]\n",
    "            grad_Z[-1] = A\n",
    "            grad_Z[-1][Y_train, np.arange(batch_size)] -= 1\n",
    "            grad_Z[-1] /= batch_size\n",
    "            \n",
    "            last_Z = fwd_A[-2] if len(fwd_A) > 1 else X_treated\n",
    "            # print(f\"grad_Z[-1] @ last_Z:{grad_Z[-1].shape} @ {last_Z.shape}\")\n",
    "            denses[-1].weights -= learning_rate * (grad_Z[-1] @ last_Z.T)\n",
    "            denses[-1].bias -= learning_rate * (grad_Z[-1] @ np.ones(shape=(grad_Z[-1].shape[1], 1)))\n",
    "            # denses[-1].weights = (grad_Z[-1] @ last_Z.T)\n",
    "            # denses[-1].bias = (grad_Z[-1] @ np.ones(shape=(grad_Z[-1].shape[1], 1)))\n",
    "            # print(f\"len(grad_Z): {len(grad_Z)}\")\n",
    "            # Back propagation\\\n",
    "            grad_dZdW = [None] * len(denses)\n",
    "            for idx, dense in reversed(list(enumerate(denses))):\n",
    "                # ignore the last layer\n",
    "                if idx >= len(denses) - 1: continue\n",
    "                # calculate grad of activation function\n",
    "                # calculate dA/dZ = grad_func(A[idx])\n",
    "                # grad_dA_dZ = dense.grad_func(fwd_Z[idx])\n",
    "                # calculate dL/dA = dZ[idx + 1]/dA @ dL/dZ[idx + 1]\n",
    "                # print(f\"in {idx}: {fwd_Z[idx].shape} @ {grad_Z[idx + 1].shape}\")\n",
    "                # grad_A[idx] = fwd_Z[idx] @ grad_Z[idx + 1]\n",
    "                # print(f\"in {idx} weights: {denses[idx+1].weights.shape} grad_Z:{grad_Z[idx + 1].shape}\")\n",
    "                grad_A[idx] = denses[idx+1].weights.T @ grad_Z[idx + 1]\n",
    "                # calculate dL/dZ = dA/dZ @ dL/dA\n",
    "                # print(f\"in {idx} grad_A * grad_func = {grad_A[idx].shape} * {dense.grad_func(fwd_Z[idx]).shape}\")\n",
    "                grad_Z[idx] = grad_A[idx] * dense.grad_func(fwd_Z[idx]) # dense.grad_func(fwd_Z[idx]).T @ grad_A[idx]\n",
    "                # calculate dZ/dW = grad_func(W[idx])\n",
    "                last_Z = fwd_Z[idx-1] if idx > 0 else X_treated\n",
    "                # print(f\"in {idx} grad_dZ_dW = {grad_Z[idx].shape} @ {last_Z.shape}\")\n",
    "                grad_dZ_dW = grad_Z[idx] @ last_Z.T\n",
    "                # calculate dZ/db = grad_func(b[idx])\n",
    "                grad_dZ_db = grad_Z[idx] @ np.ones(shape=(grad_Z[idx].shape[1], 1))\n",
    "\n",
    "                grad_dZdW[idx] = (grad_dZ_dW, grad_dZ_db)\n",
    "                # update W and b\n",
    "                # dense.weights, dense.bias = optimize_basicGD(w=dense.weights, b=dense.bias,\n",
    "                #     grad_w=grad_dZ_dW, grad_b=grad_dZ_db, learning_rate=learning_rate)\n",
    "                # dense.weights, dense.bias, dense.cache_w, dense.cache_b = optimize_RMSProps(w=dense.weights, b=dense.bias,\n",
    "                #     grad_w=grad_dZ_dW, grad_b=grad_dZ_db, v_w=dense.cache_w, v_b=dense.cache_b,\n",
    "                #     learning_rate=learning_rate, beta=0.9)\n",
    "                pass\n",
    "            for idx in range(len(denses)):\n",
    "                if idx >= len(denses) - 1: continue\n",
    "                denses[idx].weights, denses[idx].bias = optimize_basicGD(\n",
    "                    w=denses[idx].weights, b=denses[idx].bias,\n",
    "                    grad_w=grad_dZdW[idx][0], grad_b=grad_dZdW[idx][1], learning_rate=learning_rate)\n",
    "                # denses[idx].weights, denses[idx].bias, denses[idx].cache_w, denses[idx].cache_b = optimize_RMSProps(\n",
    "                #     w=denses[idx].weights, b=denses[idx].bias,\n",
    "                #     grad_w=grad_dZdW[idx][0], grad_b=grad_dZdW[idx][1], \n",
    "                #     v_w=denses[idx].cache_w, v_b=denses[idx].cache_b,\n",
    "                #     learning_rate=learning_rate, beta=0.9)\n",
    "                # denses[idx].weights, denses[idx].bias, denses[idx].cache_w, denses[idx].cache_b, dL = optimize_RMSPropsL1(\n",
    "                #     w=denses[idx].weights, b=denses[idx].bias,\n",
    "                #     grad_w=grad_dZdW[idx][0], grad_b=grad_dZdW[idx][1], \n",
    "                #     v_w=denses[idx].cache_w, v_b=denses[idx].cache_b,\n",
    "                #     learning_rate=learning_rate, beta=0.9, loss=loss, lambda_l1=1e-3)\n",
    "                # loss = dL\n",
    "                # denses[idx].weights, denses[idx].bias, denses[idx].cache_w, denses[idx].cache_b = optimize_Adam(\n",
    "                #     w=denses[idx].weights, b=denses[idx].bias,\n",
    "                #     grad_w=grad_dZdW[idx][0], grad_b=grad_dZdW[idx][1], \n",
    "                #     cache_w=denses[idx].cache_w, cache_b=denses[idx].cache_b,\n",
    "                #     learning_rate=learning_rate, beta1=0.9, beta2=0.999)\n",
    "                pass\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                print(f\"Cost {epoch+1:4d}: {loss}\")\n",
    "                pass\n",
    "            pass\n",
    "        model.extend(denses)\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.868930Z",
     "start_time": "2024-10-04T07:17:08.865679Z"
    }
   },
   "source": [
    "def evaluate(model: list[DenseLayer], X_test, Y_test):\n",
    "    \"\"\"\n",
    "    :param X_test: matrix(n, m) n - number of rows, m - features\n",
    "    :param Y_test: vector(n,)\n",
    "    :return: [lost, accuracy] \n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    X_test = flat_data(X_test).transpose()\n",
    "    A = X_test\n",
    "    # print(f\"A:{A.shape}\")\n",
    "    for dense in model:\n",
    "        Z = A\n",
    "        Z = dense.weights @ Z + dense.bias\n",
    "        A = dense.activation_func(Z)\n",
    "        pass\n",
    "    Y_pred = softmax(A)\n",
    "    loss = sparse_categorical_crossentropy(Y_test, Y_pred)\n",
    "    # print(f\"{Y_test.shape} {np.argmax(Y_pred[Y_test, np.arange(Y_test.shape[0])], keepdims=True)}\")\n",
    "    Y_ans = np.argmax(Y_pred, axis=0, keepdims=False)\n",
    "    # print(f\"Y_ans:{Y_ans.shape}\")\n",
    "    accuracy = accuracy_score(Y_test, Y_ans)\n",
    "    rs = [loss, accuracy]\n",
    "    return rs"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T07:17:08.964345Z",
     "start_time": "2024-10-04T07:17:08.878033Z"
    }
   },
   "source": [
    "# split the train data\n",
    "X_train_20 = train_test_split(X_train, test_size=0.2, random_state=42)[1]\n",
    "Y_train_20 = train_test_split(y_train, test_size=0.2, random_state=42)[1]\n",
    "\n",
    "len(X_train_20)\n",
    "X_train_20.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-04T07:17:11.337719Z"
    }
   },
   "source": [
    "rs1 = fit(X_train_20, Y_train_20, epochs=100, learning_rate=1e-3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost   10: 9.393661422041053\n",
      "Cost   20: 9.391073082521839\n",
      "Cost   30: 9.388479299627537\n",
      "Cost   40: 9.385873918236422\n",
      "Cost   50: 9.383263939334455\n",
      "Cost   60: 9.380641980405157\n",
      "Cost   70: 9.378005036861593\n",
      "Cost   80: 9.375363718043916\n",
      "Cost   90: 9.372707365129394\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# split the train data\n",
    "rand_seed = np.random.randint(low=0, high=1000)\n",
    "X_test_20 = train_test_split(X_train, test_size=0.1, random_state=rand_seed)[1]\n",
    "Y_test_20 = train_test_split(y_train, test_size=0.1, random_state=rand_seed)[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "evaluate(rs1, X_test_20, Y_test_20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draft:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ManualNN (object):\n",
    "    \"\"\"\n",
    "    Layers:\n",
    "    - (units=32, activation=tf.keras.activations.relu),\n",
    "    - (units=128, activation=tf.keras.activations.relu),\n",
    "    - (units=10, activation=tf.keras.activations.linear)\n",
    "    Loss: softmax + SparseCategoricalCrossEntropy\n",
    "    Optimizer: basicGD\n",
    "    \"\"\"\n",
    "    def glorot_normal(self, shape, n_in, n_out):\n",
    "        stddev = np.sqrt(2 / (n_in + n_out))\n",
    "        return np.random.normal(loc=0, scale=stddev, size=shape)\n",
    "    \n",
    "    def glorot_uniform(self, shape, n_in, n_out):\n",
    "        limit = np.sqrt(6 / (n_in + n_out))\n",
    "        return np.random.uniform(low=-limit, high=limit, size=shape)\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int, learning_rate=1e-2):\n",
    "        self.list_W = [None]\n",
    "        self.list_b = [None]\n",
    "        self.learning_rate = learning_rate\n",
    "        # - (units=32, activation=tf.keras.activations.relu),\n",
    "        # W = np.random.randn(32, input_size) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(32, input_size), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((32, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        # - (units=128, activation=tf.keras.activations.relu),\n",
    "        # W = np.random.randn(128, 32) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(128, 32), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((128, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        # - (units=10, activation=tf.keras.activations.linear)\n",
    "        # W = np.random.randn(output_size, 128) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(output_size, 128), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((output_size, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        return\n",
    "\n",
    "    def linear(self, x): return x\n",
    "\n",
    "    def relu(self, x): return np.maximum(0, x)\n",
    "    \n",
    "    def grad_relu(self, x): return np.where(x > 0, 1, 0) #return x > 0\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        # return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        x_exp = np.exp(x - x_max)\n",
    "        x_sum = np.sum(x_exp, axis=-1, keepdims=True)\n",
    "        return x_exp / x_sum\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        selector = (y_true, np.arange(y_true.shape[0]))\n",
    "        value = y_pred[selector]\n",
    "        # print(f\"loss: {value} <=0:{np.any(value <= 0)} isnan:{np.any(np.isnan(value))}\")\n",
    "        loss = -np.log(value + 1e-7)\n",
    "        return np.mean(loss)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.a = []\n",
    "        self.z = [0]    # default z0 is not use\n",
    "        # input layer: a[0] = X\n",
    "        self.a.append(X)\n",
    "        # layer 1\n",
    "        tmp_z = self.list_W[1] @ self.a[0] + self.list_b[1]\n",
    "        tmp_a = self.relu(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        # layer 2\n",
    "        tmp_z = self.list_W[2] @ self.a[1] + self.list_b[2]\n",
    "        tmp_a = self.relu(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        # layer 3\n",
    "        tmp_z = self.list_W[3] @ self.a[2] + self.list_b[3]\n",
    "        tmp_a = self.softmax(tmp_z) #self.linear(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        return self.a[-1]\n",
    "    \n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param self: \n",
    "        :param X: (n, m): n features, m samples \n",
    "        :param y_true: (m,): m labels \n",
    "        :param y_pred: (n, m): predicted labels\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        grad_W: list = [None] * 4\n",
    "        grad_b: list = [None] * 4\n",
    "        \n",
    "        # y_pred - y_true\n",
    "        grad_Z3 = y_pred\n",
    "        grad_Z3[y_true, range(m)] -= 1\n",
    "        grad_Z3 /= m\n",
    "        \n",
    "        # grad for layer 3\n",
    "        # grad_W[3] = grad_Z3 @ dZ3/dW[3] = grad_Z3 @ d(W[3]A[2] + b[3])/dW[3]\n",
    "        grad_W[3] = grad_Z3 @ self.a[2].T\n",
    "        # grad_b[3] = grad_Z3 @ np.ones(shape=(grad_Z3.shape[1], 1))\n",
    "        grad_b[3] = np.sum(grad_Z3, axis=1, keepdims=True)\n",
    "        \n",
    "        # grad for layer 2\n",
    "        # grad_A2 = grad_Z3 @ dZ3/dA2 = grad_Z3 @ d(W[3]A[2] + b[3])/dA2 \n",
    "        grad_A2 = self.list_W[3].T @ grad_Z3\n",
    "        # grad_Z2 = grad_Z3 * dA2/dZ2 = grad_relu(Z2) * grad_A2\n",
    "        grad_Z2 = self.grad_relu(self.z[2]) * grad_A2\n",
    "        grad_W[2] = grad_Z2 @ self.a[1].T\n",
    "        grad_b[2] = np.sum(grad_Z2, axis=1, keepdims=True)\n",
    "        \n",
    "        # grad for layer 1\n",
    "        grad_A1 = self.list_W[2].T @ grad_Z2\n",
    "        grad_Z1 = self.grad_relu(self.z[1]) * grad_A1\n",
    "        grad_W[1] = grad_Z1 @ self.a[0].T\n",
    "        grad_b[1] = np.sum(grad_Z1, axis=1, keepdims=True)\n",
    "        \n",
    "        # update W, b\n",
    "        self.list_W[3] -= self.learning_rate * grad_W[3]\n",
    "        self.list_b[3] -= self.learning_rate * grad_b[3]\n",
    "        self.list_W[2] -= self.learning_rate * grad_W[2]\n",
    "        self.list_b[2] -= self.learning_rate * grad_b[2]\n",
    "        self.list_W[1] -= self.learning_rate * grad_W[1]\n",
    "        self.list_b[1] -= self.learning_rate * grad_b[1]\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs=1):\n",
    "        \"\"\"\n",
    "        fit\n",
    "        :param self: \n",
    "        :param X: (n, m): n features, m samples \n",
    "        :param y: (m,): m labels\n",
    "        :param epochs: \n",
    "        :param learning_rate: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X=X)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.backward(X, y, y_pred)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1:4d}, Loss: {loss}\")\n",
    "            pass\n",
    "        return\n",
    "    \n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param X_test: matrix(n, m) n - number of rows, m - features\n",
    "        :param Y_test: vector(n,)\n",
    "        :return: [lost, accuracy] \n",
    "        \"\"\"\n",
    "        X_test = flat_data(X_test).transpose()\n",
    "        A = X_test\n",
    "        # print(f\"A:{A.shape}\")\n",
    "        A = self.forward(A)\n",
    "        Y_pred = softmax(A)\n",
    "        loss = sparse_categorical_crossentropy(y_test, Y_pred)\n",
    "        # print(f\"{Y_test.shape} {np.argmax(Y_pred[Y_test, np.arange(Y_test.shape[0])], keepdims=True)}\")\n",
    "        y_ans = np.argmax(Y_pred, axis=0, keepdims=False)\n",
    "        # print(f\"Y_ans:{Y_ans.shape}\")\n",
    "        accuracy = accuracy_score(y_test, y_ans)\n",
    "        return loss, accuracy\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class ManualNN2 (object):\n",
    "    \"\"\"\n",
    "    Layers:\n",
    "    - (units=32, activation=tf.keras.activations.relu),\n",
    "    - (units=128, activation=tf.keras.activations.relu),\n",
    "    - (units=10, activation=tf.keras.activations.linear)\n",
    "    Loss: softmax + SparseCategoricalCrossEntropy\n",
    "    Optimizer: basicGD\n",
    "    \"\"\"\n",
    "    def glorot_normal(self, shape, n_in, n_out):\n",
    "        stddev = np.sqrt(2 / (n_in + n_out))\n",
    "        return np.random.normal(loc=0, scale=stddev, size=shape)\n",
    "\n",
    "    def glorot_uniform(self, shape, n_in, n_out):\n",
    "        limit = np.sqrt(6 / (n_in + n_out))\n",
    "        return np.random.uniform(low=-limit, high=limit, size=shape)\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, learning_rate=1e-2):\n",
    "        self.list_W = [None]\n",
    "        self.list_b = [None]\n",
    "        self.learning_rate = learning_rate\n",
    "        # - (units=32, activation=tf.keras.activations.relu),\n",
    "        # W = np.random.randn(32, input_size) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(32, input_size), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((32, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        # - (units=128, activation=tf.keras.activations.relu),\n",
    "        # W = np.random.randn(128, 32) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(128, 32), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((128, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        # - (units=10, activation=tf.keras.activations.linear)\n",
    "        # W = np.random.randn(output_size, 128) * learning_rate\n",
    "        W = self.glorot_uniform(shape=(output_size, 128), n_in=input_size, n_out=output_size)\n",
    "        b = np.zeros((output_size, 1))\n",
    "        self.list_W.append(W)\n",
    "        self.list_b.append(b)\n",
    "        return\n",
    "\n",
    "    def linear(self, x): return x\n",
    "\n",
    "    def relu(self, x): return np.maximum(0, x)\n",
    "\n",
    "    def grad_relu(self, x): return np.where(x > 0, 1, 0) #return x > 0\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        # return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        x_exp = np.exp(x - x_max)\n",
    "        # x_exp = np.exp(x)\n",
    "        x_sum = np.sum(x_exp, axis=-1, keepdims=True)\n",
    "        return x_exp / x_sum\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        selector = (y_true, np.arange(y_true.shape[0]))\n",
    "        value = y_pred[selector]\n",
    "        # print(f\"loss: {value} <=0:{np.any(value <= 0)} isnan:{np.any(np.isnan(value))}\")\n",
    "        loss = -np.log(value + 1e-7)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a = []\n",
    "        self.z = [0]    # default z0 is not use\n",
    "        # input layer: a[0] = X\n",
    "        self.a.append(X)\n",
    "        # layer 1\n",
    "        tmp_z = self.list_W[1] @ self.a[0] + self.list_b[1]\n",
    "        tmp_a = self.relu(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        # layer 2\n",
    "        tmp_z = self.list_W[2] @ self.a[1] + self.list_b[2]\n",
    "        tmp_a = self.relu(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        # layer 3\n",
    "        tmp_z = self.list_W[3] @ self.a[2] + self.list_b[3]\n",
    "        tmp_a = self.softmax(tmp_z) #self.linear(tmp_z)\n",
    "        self.z.append(tmp_z)\n",
    "        self.a.append(tmp_a)\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        \n",
    "        :param self: \n",
    "        :param X: (n, m): n features, m samples \n",
    "        :param y_true: (m,): m labels \n",
    "        :param y_pred: (n, m): predicted labels\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        grad_W: list = [None] * 4\n",
    "        grad_b: list = [None] * 4\n",
    "        grad_A: list = [None] * 4\n",
    "        grad_Z: list = [None] * 4\n",
    "\n",
    "        # y_pred - y_true\n",
    "        grad_Z[3] = y_pred\n",
    "        grad_Z[3][y_true, range(m)] -= 1\n",
    "        grad_Z[3] /= m\n",
    "\n",
    "        # grad for layer 3\n",
    "        # grad_W[3] = grad_Z3 @ dZ3/dW[3] = grad_Z3 @ d(W[3]A[2] + b[3])/dW[3]\n",
    "        grad_W[3] = grad_Z[3] @ self.a[2].T\n",
    "        # grad_b[3] = grad_Z3 @ np.ones(shape=(grad_Z3.shape[1], 1))\n",
    "        grad_b[3] = np.sum(grad_Z[3], axis=1, keepdims=True)\n",
    "        \n",
    "        # grad for layer 2\n",
    "        # grad_A2 = grad_Z3 @ dZ3/dA2 = grad_Z3 @ d(W[3]A[2] + b[3])/dA2 \n",
    "        grad_A[2] = self.list_W[3].T @ grad_Z[3]\n",
    "        # grad_Z2 = grad_Z3 * dA2/dZ2 = grad_relu(Z2) * grad_A2\n",
    "        grad_Z[2] = self.grad_relu(self.z[2]) * grad_A[2]\n",
    "        grad_W[2] = grad_Z[2] @ self.a[1].T\n",
    "        grad_b[2] = np.sum(grad_Z[2], axis=1, keepdims=True)\n",
    "        \n",
    "        # grad for layer 1\n",
    "        grad_A[1] = self.list_W[2].T @ grad_Z[2]\n",
    "        grad_Z[1] = self.grad_relu(self.z[1]) * grad_A[1]\n",
    "        grad_W[1] = grad_Z[1] @ self.a[0].T\n",
    "        grad_b[1] = np.sum(grad_Z[1], axis=1, keepdims=True)\n",
    "\n",
    "        self.list_W[3] -= self.learning_rate * grad_W[3]\n",
    "        self.list_b[3] -= self.learning_rate * grad_b[3]\n",
    "        self.list_W[2] -= self.learning_rate * grad_W[2]\n",
    "        self.list_b[2] -= self.learning_rate * grad_b[2]\n",
    "        self.list_W[1] -= self.learning_rate * grad_W[1]\n",
    "        self.list_b[1] -= self.learning_rate * grad_b[1]\n",
    "\n",
    "        return\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, epochs=1):\n",
    "        \"\"\"\n",
    "        fit\n",
    "        :param self: \n",
    "        :param X: (n, m): n features, m samples \n",
    "        :param y: (m,): m labels\n",
    "        :param epochs: \n",
    "        :param learning_rate: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_treated)\n",
    "            loss = self.compute_loss(y, y_pred)\n",
    "            self.backward(X, y, y_pred)\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1:4d}, Loss: {loss}\")\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param X_test: matrix(n, m) n - number of rows, m - features\n",
    "        :param Y_test: vector(n,)\n",
    "        :return: [lost, accuracy] \n",
    "        \"\"\"\n",
    "        X_test = flat_data(X_test).transpose()\n",
    "        A = X_test\n",
    "        # print(f\"A:{A.shape}\")\n",
    "        A = self.forward(A)\n",
    "        Y_pred = softmax(A)\n",
    "        loss = sparse_categorical_crossentropy(y_test, Y_pred)\n",
    "        # print(f\"{Y_test.shape} {np.argmax(Y_pred[Y_test, np.arange(Y_test.shape[0])], keepdims=True)}\")\n",
    "        y_ans = np.argmax(Y_pred, axis=0, keepdims=False)\n",
    "        # print(f\"Y_ans:{Y_ans.shape}\")\n",
    "        accuracy = accuracy_score(y_test, y_ans)\n",
    "        return loss, accuracy\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_treated = flat_data(X_train_20).transpose()\n",
    "nn = ManualNN2(input_size=X_treated.shape[0], output_size=10, learning_rate=1e-3)\n",
    "nn.fit(X_treated, Y_train_20, epochs=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rs2 = nn.evaluate(X_test_20, Y_test_20)\n",
    "rs2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\\\\n",
    "\\tag{15}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "a = np.array([[1, 2, 3, 3],\n",
    "              [4, 5, 6, 6],\n",
    "              [7, 8, 9, 9]])\n",
    "b = np.array([1, 2, 3])\n",
    "a[np.arange(a.shape[0]), b]\n",
    "# a"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample\n",
    "model = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28,)),\n",
    "    tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.linear)\n",
    "    # tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3), #tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3), #tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.fit(x=X_train_20, y=Y_train_20, epochs=100, batch_size=Y_train_20.shape[0], workers=1, use_multiprocessing=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "rs3 = model.evaluate(X_test_20, Y_test_20, verbose=0)\n",
    "rs3  # [lost, accuracy]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_image(img_data: np.ndarray) -> tuple:\n",
    "    fig, axes = plt.subplots(figsize=(1.60, 1.20))\n",
    "    axes.imshow(X=img_data, cmap=\"gray\")\n",
    "    return fig, axes\n",
    "\n",
    "# print(y_test[5854])\n",
    "show_image(X_test[4823])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "show_image(X_train[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Linear and Activation Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loss/Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
