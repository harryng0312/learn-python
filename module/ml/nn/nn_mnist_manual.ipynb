{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:43:22.773377Z",
     "start_time": "2024-08-14T14:43:18.130577Z"
    }
   },
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "\n",
    "from array import array\n",
    "# import keras._tf_keras.keras as keras \n",
    "# from keras._tf_keras.keras\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "# matplotlib.use(\"QTAgg\")\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data:\n",
    "- Train data: 60k 28x28 images\n",
    "- Test data: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:43:24.713335Z",
     "start_time": "2024-08-14T14:43:24.565340Z"
    }
   },
   "source": [
    "mnist_path = \"/data/sample/mnist\"\n",
    "training_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-images.idx3-ubyte\"])\n",
    "training_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-labels.idx1-ubyte\"])\n",
    "test_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-images.idx3-ubyte\"])\n",
    "test_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-labels.idx1-ubyte\"])\n",
    "\n",
    "def read_images_labels(images_filepath, labels_filepath) -> tuple:\n",
    "    labels = []\n",
    "    with open(labels_filepath, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        # labels = array(\"B\", file.read())\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    with open(images_filepath, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        image_data = array(\"B\", file.read())       \n",
    "     \n",
    "    images = []\n",
    "    # for i in range(size):\n",
    "    #     images.append([0] * rows * cols)\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(28, 28)\n",
    "        # images[i][:] = img\n",
    "        images.append(img)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "    x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "    return (x_train, y_train),(x_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# print(f\"{type(X_train[0])}\")\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-14T14:43:30.288545Z",
     "start_time": "2024-08-14T14:43:30.195428Z"
    }
   },
   "source": [
    "X_train = np.asarray(X_train)/255\n",
    "y_train = np.asarray(y_train)\n",
    "X_test  = np.asarray(X_test)/255\n",
    "y_test  = np.asarray(y_test)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Activation functions:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.1. Linear:\n",
    "$ \\begin{align}\n",
    "f(\\mathbf z) &= \\mathbf x \\\\\n",
    "\\rightarrow \\frac{\\partial f(\\mathbf z)}{\\partial \\mathbf z} &=\\mathbf 1 \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2. ReLU:\n",
    "$\\begin{align}\n",
    "ReLU(\\mathbf z) &= \\max(\\mathbf z, \\mathbf 0) \\\\\n",
    "\\rightarrow \\frac{\\partial ReLU(\\mathbf z)}{\\partial \\mathbf z} &= \\begin{cases}\n",
    "x_i = 1 \\text{ if } x_i > 0 \\\\\n",
    "x_i = 0 \\text{ if } x_i \\leqslant 0 \\\\\n",
    "\\end{cases} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.2. Sigmoid:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\rightarrow \\frac{\\partial\\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(\\mathbf z)\\cdot\\left(1 - \\sigma(\\mathbf z)\\right) \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1.3. Softmax:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{e^{\\mathbf z}}{\\sum_{i=1}^{C}e^{z_i}} \\\\\n",
    "\\rightarrow \\frac{\\partial \\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(z_i) \\cdot (\\delta_{ij} - \\sigma(z_j)) \n",
    "\\rightarrow \\delta_{ij} = \\begin{cases} \n",
    "1 \\text{ if } i = j \\\\\n",
    "0 \\text{ if } i \\neq j  \n",
    "\\end{cases} \\\\\n",
    "&= diag(\\mathbf z) - \\mathbf z * \\mathbf z^T \\\\\n",
    "C &\\text{ is number of class} \\\\\n",
    "diag &\\text{ is diagonal matrix }\n",
    "\\end{align}$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loss function:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1. Cross Entropy:\n",
    "$\\begin{align}\n",
    "CrossEntropy = - \\log(\\hat{y}_{true})\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2. Categorical Crossentropy:\n",
    "$\\begin{align}\n",
    "Y &\\text{ is label in one-hot matrix} \\\\\n",
    "\\hat{Y} &\\text{ is predicted matrix} \\\\\n",
    "C &\\text{ is number of classes}\\\\\n",
    "L &= -\\sum_{i=1}^{C} Y_i \\log(\\hat{Y}_{i}) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3. Sparse Categorical Crossentropy:\n",
    "$\\begin{align}\n",
    "\\hat{Y} &= A_n = softmax(Z) \\\\\n",
    "Z &\\text{ is } n \\times C \\text{ matrix. n is number of samples, C is number of classes} \\\\\n",
    "CrossEntropy_i &= -\\log(\\hat{y}_{i, y_{sparse}}) \\\\\n",
    "CrossEntropy &\\text{ is a vector size n} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_{i,j}} &= \\hat{Y}_{i,j} - \\delta(j, y_{sparse,i}) \n",
    "\\rightarrow \\delta(j, y_{sparse,i}) = \\begin{cases}\n",
    "1 \\text{ if } j = y_{sparse,i} \\\\\n",
    "0 \\text{ if } j \\neq y_{sparse,i}\\\\\n",
    "\\end{cases} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z} &= \\hat{Y} - SparseLabels \\\\\n",
    "SparseLabels &\\text{ can be considered as one-hot matrix}\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# model = tf.keras.models.Sequential(layers=[\n",
    "#     tf.keras.layers.Flatten(input_shape=(28, 28,)),\n",
    "#     tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu),\n",
    "#     tf.keras.layers.Dense(units=128, activation=tf.keras.activations.sigmoid),\n",
    "#     # tf.keras.layers.Dropout(rate=0.2),\n",
    "#     tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)\n",
    "# ])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# predictions = model(X_train[0]).numpy()\n",
    "# predictions\n",
    "# tf.nn.softmax(predictions).numpy()\n",
    "# model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# loss_fn = tf.keras.losses.CategoricalHinge()\n",
    "# loss_fn = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "#               loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.FalseNegatives()])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# X_train, X_test = np.asarray(X_train) / 255.0, np.asarray(X_test) / 255.0\n",
    "# print(X_test)\n",
    "model.fit(x=X_train, y=y_train, epochs=50, batch_size=600, workers=8, use_multiprocessing=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model.evaluate(X_test,  y_test, verbose=2)\n",
    "c = 0\n",
    "cp = 0\n",
    "for i in range(100):\n",
    "    test_indx = rd.randint(0, len(y_test)-1)\n",
    "    x_test_ = np.asarray([X_test[test_indx]])\n",
    "\n",
    "    # test_indx = rd.randint(0, len(y_train)-1)\n",
    "    # x_test_ = np.asarray([X_train[test_indx]])\n",
    "\n",
    "    result = model.predict(x=x_test_, verbose=0)\n",
    "    # result = tf.nn.softmax(result).numpy()\n",
    "    y_test_ = y_test\n",
    "    # if result.max() >= 0.5:\n",
    "    if result.argmax() != y_test_[test_indx]:\n",
    "        c+=1\n",
    "        print(f\"- [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} solve:{y_test_[test_indx]}\")\n",
    "    else:\n",
    "        print(f\"+ [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} solve:{y_test_[test_indx]}\")\n",
    "    # else:\n",
    "    #     print(f\"= [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} solve:{y_test_[test_indx]}\")\n",
    "    #     cp+=1\n",
    "print(f\"error: {c} can not pred:{cp}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def show_image(img_data: np.ndarray) -> tuple:\n",
    "    fig, axes = plt.subplots(figsize=(1.60, 1.20))\n",
    "    axes.imshow(X=img_data, cmap=\"gray\")\n",
    "    return fig, axes\n",
    "\n",
    "# print(y_test[5854])\n",
    "show_image(X_test[4823])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Trong quá trình **backpropagation**, khi sử dụng hàm softmax trong lớp đầu ra của một mạng nơ-ron, ta cần tính toán gradient của hàm mất mát (thường là categorical cross-entropy) đối với các trọng số. Điều này yêu cầu tính toán đạo hàm của hàm softmax.\n",
    "\n",
    "### 1. **Hàm Softmax và Hàm Mất Mát Cross-Entropy**\n",
    "\n",
    "Giả sử đầu ra của mạng nơ-ron là một vector \\( \\mathbf{z} = [z_1, z_2, \\dots, z_n] \\). Hàm softmax được định nghĩa như sau:\n",
    "\n",
    "\\[\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "\\]\n",
    "\n",
    "Giả sử nhãn đúng là \\( y \\), hàm mất mát cross-entropy có dạng:\n",
    "\n",
    "\\[\n",
    "L = -\\sum_{i=1}^{n} y_i \\log(\\sigma(z_i))\n",
    "\\]\n",
    "\n",
    "### 2. **Đạo Hàm Của Hàm Mất Mát Đối Với Đầu Ra Của Softmax**\n",
    "\n",
    "Để thực hiện backpropagation, ta cần tính đạo hàm của hàm mất mát \\( L \\) đối với mỗi đầu ra của softmax \\( z_i \\):\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_i}\n",
    "\\]\n",
    "\n",
    "Sử dụng quy tắc dây chuyền, ta có:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial z_i} = \\sum_{j=1}^{n} \\frac{\\partial L}{\\partial \\sigma(z_j)} \\cdot \\frac{\\partial \\sigma(z_j)}{\\partial z_i}\n",
    "\\]\n",
    "\n",
    "#### 2.1 **Đạo Hàm Của Hàm Mất Mát Với Softmax**\n",
    "\n",
    "Đạo hàm của hàm mất mát \\( L \\) đối với đầu ra của softmax \\( \\sigma(z_j) \\) là:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial \\sigma(z_j)} = \\sigma(z_j) - y_j\n",
    "\\]\n",
    "\n",
    "#### 2.2 **Đạo Hàm Của Softmax**\n",
    "\n",
    "Đạo hàm của hàm softmax được tính như sau:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\sigma(z_j)}{\\partial z_i} = \n",
    "\\begin{cases} \n",
    "\\sigma(z_i) \\cdot (1 - \\sigma(z_i)) & \\text{nếu } i = j \\\\\n",
    "-\\sigma(z_i) \\cdot \\sigma(z_j) & \\text{nếu } i \\neq j \n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "### 3. **Vectorization (Tính Toán Dạng Vector)**\n",
    "\n",
    "Trong thực hành, tính toán đạo hàm của hàm softmax được thực hiện thông qua vectorization để tối ưu hóa hiệu suất. Giả sử \\( \\mathbf{\\sigma} \\) là vector chứa các giá trị softmax \\( \\sigma(z_1), \\sigma(z_2), \\dots, \\sigma(z_n) \\), ta có:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial \\mathbf{\\sigma}}{\\partial \\mathbf{z}} = \\text{Jacobian}(\\mathbf{\\sigma}) = \\mathbf{S} - \\mathbf{\\sigma} \\cdot \\mathbf{\\sigma}^T\n",
    "\\]\n",
    "\n",
    "trong đó:\n",
    "\n",
    "- \\( \\mathbf{S} \\) là ma trận chéo (diagonal matrix) với các phần tử \\( \\sigma(z_i) \\) trên đường chéo chính.\n",
    "- \\( \\mathbf{\\sigma} \\cdot \\mathbf{\\sigma}^T \\) là tích ngoài (outer product) của vector \\( \\mathbf{\\sigma} \\).\n",
    "\n",
    "Cụ thể hơn:\n",
    "\n",
    "\\[\n",
    "\\text{Jacobian}(\\mathbf{\\sigma}) = \\text{diag}(\\sigma) - \\sigma \\cdot \\sigma^T\n",
    "\\]\n",
    "\n",
    "### 4. **Gradient Đối Với Vector Đầu Ra**\n",
    "\n",
    "Cuối cùng, gradient của hàm mất mát đối với vector \\( \\mathbf{z} \\) (đầu ra trước softmax) có thể được biểu diễn dưới dạng vectorized:\n",
    "\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial \\mathbf{z}} = \\mathbf{\\sigma} - \\mathbf{y}\n",
    "\\]\n",
    "\n",
    "Đây là dạng vectorized của gradient, rất quan trọng trong quá trình huấn luyện mô hình với backpropagation vì nó cho phép tính toán gradient một cách hiệu quả, đặc biệt là khi làm việc với các tập dữ liệu lớn và các mô hình có nhiều lớp.\n",
    "\n",
    "### Tóm Lược\n",
    "\n",
    "- **Đạo hàm của hàm softmax** có thể được biểu diễn dưới dạng ma trận Jacobian.\n",
    "- Trong quá trình **backpropagation**, gradient của hàm mất mát đối với đầu ra trước softmax \\( z_i \\) có dạng vectorized: \\( \\frac{\\partial L}{\\partial \\mathbf{z}} = \\mathbf{\\sigma} - \\mathbf{y} \\).\n",
    "- Vectorization giúp tính toán gradient nhanh chóng và hiệu quả hơn khi huấn luyện các mô hình học sâu."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
