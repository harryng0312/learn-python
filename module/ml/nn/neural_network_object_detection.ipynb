{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.lines as mlines\n","import matplotlib.animation as animation\n","import time\n","import struct\n","import tensorflow as tf\n","import random as rd\n","import pickle as pickle\n","\n","from array import array\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# my project\n","from module.conf import PROJECT_DIR\n","\n","# %matplotlib tk\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_images_labels(data_filepath) -> tuple:\n","    labels = []\n","    images = []\n","    for path in data_filepath:\n","        with open(file = path, mode=\"rb\") as f:\n","            dict_data = pickle.load(file=f, encoding=\"bytes\")\n","            dd = dict_data[b'data']\n","            for ind, val in enumerate(dd):\n","                label = dict_data[b'labels'][ind]\n","                img = np.asarray(dd[ind], dtype=np.uint8).reshape(3, 32, 32).transpose(1, 2, 0)\n","                labels.append(label)\n","                images.append(img)\n","                pass\n","            pass\n","        pass\n","    return np.asarray(images), np.asarray(labels)\n","\n","def load_data() -> tuple:\n","    cifar_path = \"/data/sample/cifar-10-batches-py\"\n","    label_name_filepath = \"\".join([PROJECT_DIR, cifar_path, \"/batches.meta\"])\n","    training_data_filepaths = [\n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_1\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_2\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_3\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_4\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_5\"]), \n","    ]\n","    test_data_filepaths = [\"\".join([PROJECT_DIR, cifar_path, \"/test_batch\"])]\n","    x_train, y_train = read_images_labels(training_data_filepaths[:])\n","    x_test, y_test = read_images_labels(test_data_filepaths)\n","    with open(file=label_name_filepath, mode=\"rb\") as f:\n","        label_names = pickle.load(file=f, encoding=\"bytes\")[b'label_names']\n","        pass\n","    return (x_train, y_train),(x_test, y_test), label_names\n","\n","(X_train, y_train), (X_test, y_test), label_names = load_data()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# print(X_train.dtype)\n","X_train = X_train.astype(dtype=np.float64) / 255\n","X_test = X_test.astype(dtype=np.float64) / 255\n","# for i in range(len(X_train)): X_train[i] /= 255\n","# for i in range(len(X_test)): X_test[i] /= 255 "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"module 'keras.api._v2.keras' has no attribute 'models'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m      2\u001b[0m model\u001b[39m.\u001b[39madd(tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m32\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m3\u001b[39m)))\n\u001b[1;32m      3\u001b[0m \u001b[39m#------------------------------------\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Conv Block 1: 32 Filters, MaxPool.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m#------------------------------------\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras' has no attribute 'models'"]}],"source":["model = tf.keras.Sequential()\n","#------------------------------------\n","# Conv Block 1: 32 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 2: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 3: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    \n","#------------------------------------\n","# Flatten the convolutional features.\n","#------------------------------------\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(512, activation='relu'))\n","model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.categorical_crossentropy\n","# loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","#               loss=tf.keras.losses.BinaryCrossentropy(),\n","#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.FalseNegatives()])\n","# model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","#               loss=loss_fn,\n","#               metrics=[\"accuracy\", \"mae\"])\n","# model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=1e-3),\n","#             loss=loss_fn,\n","#             metrics=[\"accuracy\"])\n","\n","model.compile(optimizer='rmsprop', \n","              loss='sparse_categorical_crossentropy', \n","              metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(x=X_train, y=y_train, epochs=32, batch_size=500, verbose=1)\n","# model.fit(x=X_train, y=y_train, epochs=16, batch_size=512, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate(X_test, y_test, verbose=2)\n","result = model.predict(x=np.asarray([X_test[0]], dtype=np.float64), verbose=1)\n","# result = tf.nn.softmax(result).numpy()\n","print(f\"{result}\")\n","print(f\"{result.argmax()} {y_test[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["c = 0\n","cp = 0\n","for i in range(100):\n","    test_indx = rd.randint(0, len(y_test)-1)\n","    x_test_ = np.asarray([X_test[test_indx]])\n","\n","    # test_indx = rd.randint(0, len(y_train)-1)\n","    # x_test_ = np.asarray([X_train[test_indx]])\n","\n","    result = model.predict(x=x_test_, verbose=0) \n","#     result = tf.nn.softmax(result).numpy()\n","    y_test_ = y_test\n","    if result.max() >= 0.5:\n","        if result.argmax() != y_test_[test_indx]:\n","            c+=1\n","            print(f\"- [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} {label_names[result.argmax()]} solve:{y_test_[test_indx]} {label_names[y_test_[test_indx]]}\")\n","    else:\n","        print(f\"can not predict:{test_indx}: {result.max()}\")\n","        cp+=1\n","print(f\"error: {c} can not pred:{cp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ind = 14\n","fig, ax = plt.subplots(figsize=(1.6, 1.2))\n","ax.imshow(X=X_train[ind])\n","plt.show()\n","print(f\"label[{y_train[ind]}]:{label_names[y_train[ind]]}\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n"," PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.config.list_physical_devices()"]}],"metadata":{"kernelspec":{"display_name":"learn-python","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
