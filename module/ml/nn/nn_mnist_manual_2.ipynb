{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.765466Z",
     "start_time": "2024-10-12T15:32:31.756550Z"
    }
   },
   "source": [
    "from typing import List\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "import struct\n",
    "import tensorflow as tf\n",
    "import random as rd\n",
    "\n",
    "from math import *\n",
    "from array import array\n",
    "\n",
    "# import keras._tf_keras.keras as keras \n",
    "# from keras._tf_keras.keras\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.python.ops.init_ops_v2 import glorot_uniform\n",
    "\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR\n",
    "\n",
    "# matplotlib.use(\"QTAgg\")\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:\n",
    "- Train data: 60k 28x28 images\n",
    "- Test data: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.876460Z",
     "start_time": "2024-10-12T15:32:31.792634Z"
    }
   },
   "source": [
    "mnist_path = \"/data/sample/mnist\"\n",
    "training_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-images.idx3-ubyte\"])\n",
    "training_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/train-labels.idx1-ubyte\"])\n",
    "test_images_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-images.idx3-ubyte\"])\n",
    "test_labels_filepath = \"\".join([PROJECT_DIR, mnist_path, \"/t10k-labels.idx1-ubyte\"])\n",
    "\n",
    "def read_images_labels(images_filepath, labels_filepath) -> tuple:\n",
    "    labels = []\n",
    "    with open(labels_filepath, 'rb') as file:\n",
    "        magic, size = struct.unpack(\">II\", file.read(8))\n",
    "        if magic != 2049:\n",
    "            raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "        # labels = array(\"B\", file.read())\n",
    "        labels = array(\"B\", file.read())\n",
    "\n",
    "    with open(images_filepath, 'rb') as file:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        if magic != 2051:\n",
    "            raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "        image_data = array(\"B\", file.read())       \n",
    "     \n",
    "    images = []\n",
    "    # for i in range(size):\n",
    "    #     images.append([0] * rows * cols)\n",
    "    for i in range(size):\n",
    "        img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "        img = img.reshape(28, 28)\n",
    "        # images[i][:] = img\n",
    "        images.append(img)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def load_data() -> tuple:\n",
    "    x_train, y_train = read_images_labels(training_images_filepath, training_labels_filepath)\n",
    "    x_test, y_test = read_images_labels(test_images_filepath, test_labels_filepath)\n",
    "    return (x_train, y_train),(x_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.889010Z",
     "start_time": "2024-10-12T15:32:31.886758Z"
    }
   },
   "source": [
    "# print(f\"{type(X_train[0])}\")\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.941416Z",
     "start_time": "2024-10-12T15:32:31.907315Z"
    }
   },
   "source": [
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_test  = np.asarray(X_test)\n",
    "y_test  = np.asarray(y_test)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Linear:\n",
    "$ \\begin{align}\n",
    "f(\\mathbf z) &= \\mathbf z \\\\\n",
    "\\rightarrow \\frac{\\partial f(\\mathbf z)}{\\partial \\mathbf z} &=\\mathbf 1 \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.951632Z",
     "start_time": "2024-10-12T15:32:31.949855Z"
    }
   },
   "source": [
    "def linear(z): return z\n",
    "def grad_linear(z): return 1"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. ReLU:\n",
    "$\\begin{align}\n",
    "ReLU(\\mathbf z) &= \\max(\\mathbf z, \\mathbf 0) \\\\\n",
    "\\rightarrow \\frac{\\partial ReLU(\\mathbf z)}{\\partial \\mathbf z} &= \\begin{cases}\n",
    "z_i = 1 \\text{ if } z_i > 0 \\\\\n",
    "z_i = 0 \\text{ if } z_i \\leqslant 0 \\\\\n",
    "\\end{cases} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.961082Z",
     "start_time": "2024-10-12T15:32:31.959383Z"
    }
   },
   "source": [
    "def relu(z): return np.maximum(0, z)\n",
    "def grad_relu(z): return np.where(z > 0, 1, 0) # (x > 0).astype(float) # np.array([1 if z_i > 0 else 0 for z_i in z])"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.972909Z",
     "start_time": "2024-10-12T15:32:31.970244Z"
    }
   },
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "relu_z = relu(z)\n",
    "relu_z\n",
    "grad_relu_z = grad_relu(relu_z)\n",
    "grad_relu_z"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Sigmoid:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "\\rightarrow \\frac{\\partial\\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(\\mathbf z)\\cdot\\left(1 - \\sigma(\\mathbf z)\\right) \\\\ \n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:31.986044Z",
     "start_time": "2024-10-12T15:32:31.983941Z"
    }
   },
   "source": [
    "def sigmoid(z): return 1/(1 + np.exp(-z))\n",
    "def grad_sigmoid(z): return sigmoid(z) * (1 - sigmoid(z))    "
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.003336Z",
     "start_time": "2024-10-12T15:32:31.999931Z"
    }
   },
   "source": [
    "z = np.array([1,-6, 3, 4, 0])\n",
    "sigmoid(z)\n",
    "grad_sigmoid(z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19661193, 0.00246651, 0.04517666, 0.01766271, 0.25      ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Softmax:\n",
    "$\\begin{align}\n",
    "\\sigma(\\mathbf z) &= \\frac{e^{\\mathbf z}}{\\sum_{i=1}^{C}e^{z_i}} \\\\\n",
    "\\rightarrow \\frac{\\partial \\sigma(\\mathbf z)}{\\partial \\mathbf z} &= \\sigma(z_i) \\cdot (\\delta_{ij} - \\sigma(z_j)) \n",
    "\\rightarrow \\delta_{ij} = \\begin{cases} \n",
    "1 \\text{ if } i = j \\\\\n",
    "0 \\text{ if } i \\neq j  \n",
    "\\end{cases} \\\\\n",
    "&= diag(\\mathbf z) - \\mathbf z * \\mathbf z^T \\\\\n",
    "C &\\text{ is number of class} \\\\\n",
    "diag &\\text{ is diagonal matrix }\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.028448Z",
     "start_time": "2024-10-12T15:32:32.025Z"
    }
   },
   "source": [
    "# def softmax(z): return np.exp(z)/np.sum(np.exp(z))\n",
    "# def grad_softmax(z): return np.diag(z) - np.outer(z,z)\n",
    "def softmax(Z):\n",
    "    Z_max = np.max(Z, axis=0, keepdims=True)\n",
    "    Z_exp = np.exp(Z - Z_max)\n",
    "    Z_sum = np.sum(Z_exp, axis=0, keepdims=True)\n",
    "    return Z_exp / Z_sum\n",
    "    # e_x = np.exp(Z - np.max(Z, axis=-1, keepdims=True))\n",
    "    # return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "def grad_softmax(Z):\n",
    "    S = softmax(Z)  # TÃ­nh softmax cho Z\n",
    "    batch_size, num_classes = S.shape\n",
    "    # Init Jacobian matrix: gradient foreach row\n",
    "    dSoftmax = np.zeros((batch_size, num_classes, num_classes))\n",
    "    for i in range(batch_size):\n",
    "        # S_i is softmax for i-row\n",
    "        s_i = S[i].reshape(-1, 1)  # Transpose to Column vector\n",
    "        # Jacobian matrix for i-row\n",
    "        # dSoftmax[i] = np.diagflat(s_i) - np.dot(s_i, s_i.T)\n",
    "        dSoftmax[i] = np.diagflat(s_i) - s_i @ s_i.T\n",
    "    # dSoftmax in hidden layer - transpose of Jacobian matrix    \n",
    "    return dSoftmax.transpose()"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.041741Z",
     "start_time": "2024-10-12T15:32:32.037462Z"
    }
   },
   "source": [
    "z = np.array([[1,-6, 3, 4, 0],\n",
    "              [1,-6, 3, 7, 0]])\n",
    "# softmax(z)\n",
    "z_max = np.max(z, axis=z.ndim-1, keepdims=True)\n",
    "z_max\n",
    "# np.exp(z-z_max)\n",
    "# grad_softmax(z)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [7]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Cross Entropy:\n",
    "$\\begin{align}\n",
    "CrossEntropy = - \\log(\\hat{y}_{true})\\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Categorical Crossentropy:\n",
    "$\\begin{align}\n",
    "Y &\\text{ is label in one-hot matrix } N \\times C  \\\\\n",
    "\\hat{Y} &\\text{ is predicted matrix } N \\times C \\\\\n",
    "C &\\text{ is number of classes}\\\\\n",
    "L &= -\\sum_{i=1}^{C} Y_i \\log(\\hat{Y}_{i}) \\\\\n",
    "\\hat{Y}_{i,j} &= \\frac{exp(Z_{i,j})}{\\sum_{k=1}^{C} exp(Z_{i,k})} \\\\\n",
    "\\rightarrow \\mathcal L &= -\\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j=1}^{C} Y_{i,j} \\log(\\hat{Y}_{i,j}) \\\\\n",
    "&= -\\frac{1}{N} \\sum_{i=1}^{N} \\log(\\hat{Y}_{i,true}) \\\\\n",
    "\\hat{Y}_{i,true} &\\text{ is predicted result corresponding to one-hot is 1}\n",
    "\\end{align}$\n",
    "\n",
    "Gradient:\n",
    "$\\begin{align}\n",
    "\\frac{\\partial L}{\\partial Z} &= \\hat{Y}_{i,j} - Y_{i,j} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Sparse Categorical Crossentropy:\n",
    "$ \\begin{align}\n",
    "\\hat{Y} &= A_n = softmax(Z) \\\\\n",
    "Z &\\text{ is } n \\times C \\text{ matrix. n is number of samples, C is number of classes} \\\\\n",
    "CrossEntropy_i &= -\\log(\\hat{y}_{i, y_{sparse}}) \\\\\n",
    "CrossEntropy &\\text{ is a vector size n} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_{i,j}} &= \\hat{Y}_{i,j} - \\delta(j, y_{sparse,i}) \n",
    "\\rightarrow \\delta(j, y_{sparse,i}) = \\begin{cases}\n",
    "1 \\text{ if } j = y_{sparse,i} \\\\\n",
    "0 \\text{ if } j \\neq y_{sparse,i}\\\\\n",
    "\\end{cases} \\\\\n",
    "\\rightarrow \\frac{\\partial L}{\\partial Z_n} &= \\hat{Y} - Y = \\hat{Y} - SparseLabels \\\\\n",
    "SparseLabels &\\text{ can be considered as one-hot matrix}\n",
    "\\end{align} $"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.079447Z",
     "start_time": "2024-10-12T15:32:32.073988Z"
    }
   },
   "source": [
    "# should apply the Vectorization\n",
    "def delta_kronecker_matrix(y_train, mY_pred):\n",
    "    \"\"\"\n",
    "    Transform to one-hot encoding\n",
    "    y_train: a vector size n\n",
    "    mY_pred: a matrix (C, n)\n",
    "    \"\"\"\n",
    "    mY_train = np.zeros(shape=mY_pred.T.shape)\n",
    "    for i in range(len(y_train)): mY_train[i][y_train[i]] = 1\n",
    "    return mY_train\n",
    "\n",
    "def sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    y_pred = np.array([mY_pred[i][y_train[i]] for i in range(len(y_train))])\n",
    "    return -np.sum(np.log(y_pred))\n",
    "\n",
    "def grad_sparse_categorical_crossentropy_Z(y_train, mY_pred):\n",
    "    return mY_pred - delta_kronecker_matrix(y_train=y_train, mY_pred=mY_pred)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    batch_size = y_true.shape[0]\n",
    "    y_true_indices = (y_true, np.arange(batch_size))\n",
    "    correct_class_probabilities = y_pred[y_true_indices]\n",
    "    loss = -np.log(correct_class_probabilities + 1e-7).mean()\n",
    "    return loss\n",
    "\n",
    "def grad_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Grad\n",
    "    grad = np.zeros_like(y_pred)\n",
    "    grad[:y_pred.shape[0], y_true] = -1 / y_pred[:y_pred.shape[0], y_true]  # Grad for true-class\n",
    "    return grad"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.109779Z",
     "start_time": "2024-10-12T15:32:32.104181Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "# ex\n",
    "# [[0, 0, 0, 0],\n",
    "#  [5, 8, 0, 0],\n",
    "#  [0, 0, 3, 0],\n",
    "#  [0, 6, 0, 0]]\n",
    "#\n",
    "\n",
    "data = np.array([5, 8, 3, 6, 7])        # values\n",
    "indices = np.array([0, 1, 2, 1, 0])     # col index for each value\n",
    "indptr = np.array([0, 2, 3, 4, 5])      # start - end in data values\n",
    "csr_m = csr_matrix((data, indices, indptr), shape=(4, 4))\n",
    "csr_m.toarray()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 8, 0, 0],\n",
       "       [0, 0, 3, 0],\n",
       "       [0, 6, 0, 0],\n",
       "       [7, 0, 0, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizers: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.142906Z",
     "start_time": "2024-10-12T15:32:32.140191Z"
    }
   },
   "source": [
    "def optimize_basicGD(w:np.ndarray, b:np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, learning_rate=1e-2):\n",
    "    w -= learning_rate * grad_w\n",
    "    b -= learning_rate * grad_b\n",
    "    return w, b"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. SGD\n",
    "$\\begin{align}\n",
    "\\theta &= \\theta - \\eta \\cdot \\nabla_{\\theta} L(\\theta, x_i, y_i) \\\\\n",
    "\\theta &\\text{ is weight}\\\\\n",
    "\\eta &\\text{ is learning rate}\\\\\n",
    "\\nabla_{\\theta} L(\\theta, x_i, y_i) &\\text{ is gradient respect to }\\theta \\text{ of }(x_i, y_i) \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. RMSProps:\n",
    "$\\begin{align}\n",
    "v_{t} &= \\beta v_{t-1} + (1 + \\beta)g_t^2 \\\\\n",
    "\\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "v_t &\\text{ is velocity at } t \\text{ time} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.187908Z",
     "start_time": "2024-10-12T15:32:32.183633Z"
    }
   },
   "source": [
    "def optimize_RMSProps(w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, v_w:np.ndarray=None, v_b:np.ndarray=None,\n",
    "                      learning_rate=0.01, beta=0.9, epsilon=1e-7):\n",
    "    # if v_w is None: v_w = np.zeros_like(w)\n",
    "    # if v_b is None: v_b = np.zeros_like(b)\n",
    "    v_w = beta * v_w + (1 - beta) * grad_w ** 2\n",
    "    v_b = beta * v_b + (1 - beta) * grad_b ** 2\n",
    "    # print(f\"v_w: {v_w.shape} v_b: {v_b.shape}\")\n",
    "    w -= learning_rate * grad_w / (np.sqrt(v_w) + epsilon)\n",
    "    b -= learning_rate * grad_b / (np.sqrt(v_b) + epsilon)\n",
    "    return w, b, v_w, v_b\n",
    "\n",
    "def optimize_RMSPropsL1(w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray, v_w:np.ndarray=None, v_b:np.ndarray=None,\n",
    "                      learning_rate=0.01, beta=0.9, epsilon=1e-7, lambda_l1 = 1e-2, loss=0.0):\n",
    "    # l1_loss = lambda_l1 * np.sum(np.abs(w))\n",
    "    l1_loss = lambda_l1 * np.sum(w**2)\n",
    "    loss += l1_loss\n",
    "    grad_w += lambda_l1 * np.sign(w)\n",
    "    # grad_b += lambda_l1 * np.sign(b)\n",
    "    v_w = beta * v_w + (1 - beta) * grad_w ** 2\n",
    "    v_b = beta * v_b + (1 - beta) * grad_b ** 2\n",
    "    # print(f\"v_w: {v_w.shape} v_b: {v_b.shape}\")\n",
    "    w -= learning_rate * grad_w / (np.sqrt(v_w) + epsilon)\n",
    "    b -= learning_rate * grad_b / (np.sqrt(v_b) + epsilon)\n",
    "    return w, b, v_w, v_b, loss + l1_loss"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Adagrad\n",
    "$\\begin{align}\n",
    "G_t &= G_{t-1} + g_t^2 \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\frac{\\eta}{\\sqrt{G_t - \\epsilon}} \\cdot g_t \\\\\n",
    "\\eta &\\text{ is learning rate} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "G &\\text{ sum of square of gradient} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Adaprops:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Adamax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.Adam: \n",
    "$\\begin{align}\n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n",
    "\\hat m_t &= \\frac{m_t}{1 - \\beta_1^t} \\\\\n",
    "\\hat v_t &= \\frac{v_t}{1 - \\beta_2^t} \\\\\n",
    "\\theta_t &= \\theta_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\\\\n",
    "g_t &\\text{ is gradient at } t \\text{ time} \\\\\n",
    "\\epsilon &\\text{ is very small number - avoid device by 0} \\\\\n",
    "\\theta &\\text{ is weight matrix or bias vector} \\\\\n",
    "G &\\text{ sum of square of gradient} \\\\\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Demo NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load/Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Neural network manually:\n",
    "- Flat 28 x 28 data\n",
    "- There are 03 layers: `[32, \"relu\"] [128, \"sigmoid\"] [10, \"softmax\"]`\n",
    "- Loss func: `SparseCategoricalCrossentropy`, `digits = False`\n",
    "- Optimizer: `RMSProp` with `learning_rate=1e-3`\n",
    "- **(opt)** metrics: `accuracy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Flat input data:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.213159Z",
     "start_time": "2024-10-12T15:32:32.210296Z"
    }
   },
   "source": [
    "def flat_data(imp_data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flat data from 02 dim matrix to vector\n",
    "    :param imp_data: (n, m_0, m_1) matrix, n is number of rows \n",
    "    :return: matrix: (n, m_0 * m_1)\n",
    "    \"\"\"\n",
    "    # scaler = MinMaxScaler(feature_range=(0, 1)) --> better accuracy, faster convergence\n",
    "    # scaler = StandardScaler(with_mean=True, with_std=True) --> better accuracy, faster convergence\n",
    "    scaler = StandardScaler(with_mean=False, with_std=False) # dont substract mean, dont devide by std ~ X, not scale\n",
    "    if len(imp_data.shape) < 3 : return scaler.fit_transform(imp_data)\n",
    "    return scaler.fit_transform(imp_data.reshape((-1, imp_data.shape[1] * imp_data.shape[2])).T).T"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Loop"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.235156Z",
     "start_time": "2024-10-12T15:32:32.228769Z"
    }
   },
   "source": [
    "class DenseLayer (object):\n",
    "    activation_map = {\"linear\": linear, \"relu\": relu, \"sigmoid\": sigmoid, \"softmax\": softmax}\n",
    "    grad_map = {\"linear\": grad_linear, \"relu\": grad_relu, \"sigmoid\": grad_sigmoid, \"softmax\": grad_softmax}\n",
    "\n",
    "    def glorot_normal(self, shape, n_in, n_out):\n",
    "        stddev = np.sqrt(2 / (n_in + n_out))\n",
    "        return np.random.normal(loc=0, scale=stddev, size=shape)\n",
    "\n",
    "    def glorot_uniform(self, shape, n_in, n_out):\n",
    "        limit = np.sqrt(6 / (n_in + n_out))\n",
    "        return np.random.uniform(low=-limit, high=limit, size=shape)\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation = 'linear'):\n",
    "        self._activation_func = self.activation_map['linear']\n",
    "        self._grad_func = self.grad_map['linear']\n",
    "        self._activation = activation.lower()\n",
    "        # self._weights = np.random.rand(output_size, input_size) * 1e-2\n",
    "        # self._weights = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
    "        self._weights = self.glorot_uniform(shape=(output_size, input_size), n_in=input_size, n_out=output_size)\n",
    "        # self._bias = np.random.rand(output_size, 1)\n",
    "        # self._bias = np.random.randn(output_size, 1) * np.sqrt(2 / input_size)\n",
    "        self._bias = np.zeros(shape=(output_size, 1))\n",
    "        self._cache_w = [np.zeros_like(self._weights), np.zeros_like(self._weights)]\n",
    "        self._cache_b = [np.zeros_like(self._bias), np.zeros_like(self._bias)]\n",
    "        self._init()\n",
    "        return\n",
    "    \n",
    "    def _init(self) -> None:\n",
    "        self._activation_func = DenseLayer.activation_map[self._activation] if self._activation in DenseLayer.activation_map else DenseLayer.activation_map['linear']\n",
    "        self._grad_func = DenseLayer.grad_map[self._activation] if self._activation in DenseLayer.grad_map else DenseLayer.grad_map['linear']\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def activation_func(self):\n",
    "        return self._activation_func\n",
    "    @property\n",
    "    def grad_func(self):\n",
    "        return self._grad_func\n",
    "\n",
    "    @property\n",
    "    def weights(self) -> np.ndarray:\n",
    "        return self._weights\n",
    "    @weights.setter\n",
    "    def weights(self, val):\n",
    "        self._weights = val\n",
    "    @property\n",
    "    def bias(self) -> np.ndarray:\n",
    "        return self._bias\n",
    "    @bias.setter\n",
    "    def bias(self, val):\n",
    "        self._bias = val\n",
    "\n",
    "    @property\n",
    "    def cache_w(self):\n",
    "        return self._cache_w\n",
    "    @cache_w.setter\n",
    "    def cache_w(self, val):\n",
    "        self._cache_w = val\n",
    "    @property\n",
    "    def cache_b(self):\n",
    "        return self._cache_b\n",
    "    @cache_b.setter\n",
    "    def cache_b(self, val):\n",
    "        self._cache_b = val\n",
    "\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.259754Z",
     "start_time": "2024-10-12T15:32:32.245549Z"
    }
   },
   "source": [
    "class NNClassification(object):\n",
    "    \"\"\"\n",
    "    Layers:\n",
    "    - (units=32, activation=tf.keras.activations.relu),\n",
    "    - (units=128, activation=tf.keras.activations.relu),\n",
    "    - (units=10, activation=tf.keras.activations.linear)\n",
    "    Loss: softmax + SparseCategoricalCrossEntropy\n",
    "    Optimizer: Adam\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, output_size: int, learning_rate=1e-3, lambda_ = 1e-3):\n",
    "        self.Z = []\n",
    "        self.A = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_ = lambda_\n",
    "        self.layers: list[DenseLayer] = [None, # dont count\n",
    "            DenseLayer(input_size=input_size, output_size=128, activation='relu'),\n",
    "            DenseLayer(input_size=128, output_size=64, activation='relu'),\n",
    "            DenseLayer(input_size=64, output_size=10, activation='softmax')]\n",
    "        # - (units=32, activation=tf.keras.activations.relu),\n",
    "        # - (units=128, activation=tf.keras.activations.relu),\n",
    "        # - (units=10, activation=tf.keras.activations.linear)\n",
    "        return\n",
    "    \n",
    "    def glorot_normal(self, shape, n_in, n_out):\n",
    "        stddev = np.sqrt(2 / (n_in + n_out))\n",
    "        return np.random.normal(loc=0, scale=stddev, size=shape)\n",
    "\n",
    "    def glorot_uniform(self, shape, n_in, n_out):\n",
    "        limit = np.sqrt(6 / (n_in + n_out))\n",
    "        return np.random.uniform(low=-limit, high=limit, size=shape)\n",
    "\n",
    "    def linear(self, x): return x\n",
    "\n",
    "    def relu(self, x): return np.maximum(0, x)\n",
    "\n",
    "    def grad_relu(self, x): return np.where(x > 0, 1, 0) #return x > 0\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x_max = np.max(x, axis=-1, keepdims=True)\n",
    "        x_exp = np.exp(x - x_max)\n",
    "        x_sum = np.sum(x_exp, axis=-1, keepdims=True)\n",
    "        return x_exp / x_sum\n",
    "\n",
    "    def optimize_Adam(self, w: np.ndarray, b: np.ndarray, grad_w: np.ndarray, grad_b: np.ndarray,\n",
    "                      cache_w: list[np.ndarray], cache_b:list[np.ndarray], no_of_samples: int,\n",
    "                      learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if cache_w is None: cache_w = [np.zeros_like(w), np.zeros_like(w)]\n",
    "        if cache_b is None: cache_b = [np.zeros_like(b), np.zeros_like(b)]\n",
    "        grad_w += self.lambda_ * w / no_of_samples\n",
    "        cache_w[1] = beta1 * cache_w[1] + (1 - beta1) * grad_w\n",
    "        cache_b[1] = beta1 * cache_b[1] + (1 - beta1) * grad_b\n",
    "        cache_w[0] = beta2 * cache_w[0] + (1 - beta2) * grad_w ** 2\n",
    "        cache_b[0] = beta2 * cache_b[0] + (1 - beta2) * grad_b ** 2\n",
    "        m_w_hat = cache_w[1] / (1 - beta1)\n",
    "        m_b_hat = cache_b[1] / (1 - beta1)\n",
    "        v_w_hat = cache_w[0] / (1 - beta2)\n",
    "        v_b_hat = cache_b[0] / (1 - beta2)\n",
    "        updated_w = w - learning_rate * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "        updated_b = b - learning_rate * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "        return updated_w, updated_b, cache_w, cache_b\n",
    "\n",
    "    def compute_loss(self, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        samples = y_true.shape[0]\n",
    "        correct_logprobs = -np.log(y_pred[y_true, range(samples)])\n",
    "        return np.mean(correct_logprobs) + self.lambda_/(2*samples) * np.sum(np.square(self.layers[-1].weights))\n",
    "    \n",
    "    def forward_propagate(self, epoch, X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        forward propagation\n",
    "        :param X: (n, m) training data \n",
    "        :return: last value\n",
    "        \"\"\"\n",
    "        self.A.clear()\n",
    "        self.Z.clear()\n",
    "        self.A.append(X)\n",
    "        self.Z.append(None)\n",
    "        \n",
    "        for idx in range(1, len(self.layers)):\n",
    "            layer = self.layers[idx]\n",
    "            tmp_z = layer.weights @ self.A[idx - 1] + layer.bias\n",
    "            tmp_a = layer.activation_func(tmp_z)\n",
    "            self.Z.append(tmp_z)\n",
    "            self.A.append(tmp_a)\n",
    "            pass\n",
    "        return self.A[-1]\n",
    "    \n",
    "    def backward_propagate(self, epoch, y) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        backward propagation\n",
    "        :param X: (n, m) n features, m samples\n",
    "        :param y: (n) labels\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        dA = [None] * len(self.layers)\n",
    "        dZ = [None] * len(self.layers)\n",
    "        dW = [None] * len(self.layers)\n",
    "        db = [None] * len(self.layers)\n",
    "        \n",
    "        no_of_samples = y.shape[0] # X.shape[1]\n",
    "        \n",
    "        # calculate dL/dZ[-1] = y_pred - y = A[-1] - y\n",
    "        dZ[-1] = self.A[-1]\n",
    "        dZ[-1][y, range(no_of_samples)] -= 1\n",
    "        dZ[-1] /= no_of_samples\n",
    "        \n",
    "        # dW[-1] = dL/dZ[-1] @ dZ[-1]/dW[-1] = dZ[-1] @ A[-2]\n",
    "        dW[-1] = dZ[-1] @ self.A[-2].T\n",
    "        db[-1] = np.sum(dZ[-1], axis=1, keepdims=True) # dZ[-1] @ np.ones((dZ[-1].shape[1], 1))\n",
    "        dWb = [None] * len(self.layers)\n",
    "        dWb[-1] = (dW[-1], db[-1])\n",
    "        for idx in range(len(self.layers) - 2, 0, -1):\n",
    "            layer = self.layers[idx]\n",
    "            # calculate dA, dZ, dW, db\n",
    "            dA[idx] = self.layers[idx + 1].weights.T @ dZ[idx + 1]\n",
    "            dZ[idx] = dA[idx] * layer.grad_func(self.Z[idx])\n",
    "            dW[idx] = dZ[idx] @ self.A[idx - 1].T\n",
    "            db[idx] = np.sum(dZ[idx], axis=1, keepdims=True)\n",
    "            dWb[idx]= (dW[idx], db[idx])\n",
    "            pass\n",
    "        return dWb\n",
    "    \n",
    "    def update_weights(self, dWb: np.ndarray, no_of_samples: int) -> None:\n",
    "        if dWb is None: return\n",
    "        for idx, (layer, dWb_itm) in enumerate(zip(self.layers[1:], dWb[1:])):\n",
    "            #     layer = self.layers[idx]\n",
    "            # layer.weights, layer.bias, layer.cache_w, layer.cache_b = self.optimize_Adam(\n",
    "            #     w=layer.weights, b=layer.bias,\n",
    "            #     grad_w=dWb[idx][0], grad_b=dWb[idx][1],\n",
    "            #     cache_w=layer.cache_w, cache_b=layer.cache_b,\n",
    "            #     learning_rate=self.learning_rate, beta1=0.9, beta2=0.999)\n",
    "            \n",
    "            layer.weights, layer.bias, layer.cache_w, layer.cache_b = self.optimize_Adam(\n",
    "                w=layer.weights, b=layer.bias,\n",
    "                grad_w=dWb_itm[0], grad_b=dWb_itm[1],\n",
    "                cache_w=layer.cache_w, cache_b=layer.cache_b, no_of_samples=no_of_samples,\n",
    "                learning_rate=self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-7)\n",
    "            pass\n",
    "        return\n",
    "    \n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray, epochs: int=1, batch_size: int=32, min_delta:float = -1) -> None:\n",
    "        \"\"\"\n",
    "        \n",
    "        :param X_train: matrix(n, m) n - features, m - number of samples\n",
    "        :param Y_train: vector(n), - labels in sparse\n",
    "        :param epochs: epochs\n",
    "        :param batch_size:\n",
    "        :param min_delta:\n",
    "        :return: list of weight matrix, from 0\n",
    "        \"\"\"\n",
    "        # read and normalize data\n",
    "        noOfSamples = X.shape[1]\n",
    "        loss = 0.0\n",
    "        accuracy = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            permu = np.random.permutation(noOfSamples)\n",
    "            X_shuffled = X[:, permu]\n",
    "            Y_shuffled = Y[permu]\n",
    "            last_loss = loss\n",
    "            last_accuracy = accuracy\n",
    "            for idx in range(0, noOfSamples, batch_size):\n",
    "                X_batch = X_shuffled[:, idx:idx + batch_size]\n",
    "                Y_batch = Y_shuffled[idx:idx + batch_size]\n",
    "                # forward\n",
    "                Y_pred = self.forward_propagate(epoch, X_batch)\n",
    "                # calculate cost and accuracy\n",
    "                # last_accuracy = accuracy\n",
    "                loss = self.compute_loss(Y_batch, Y_pred)\n",
    "                accuracy = accuracy_score(Y_batch, np.argmax(Y_pred, axis=0, keepdims=False))\n",
    "                # backward\n",
    "                dWb = self.backward_propagate(epoch, Y_batch)\n",
    "                # update weight\n",
    "                self.update_weights(dWb, X_batch.shape[1])\n",
    "            # print result\n",
    "            if np.abs(accuracy) > 1 - min_delta:\n",
    "                print(f\"Epochs[{epoch+1}] Loss: {loss:4f} - Accuracy: {accuracy:6f}, Prev Accuracy: {last_accuracy:6f}\")\n",
    "                return\n",
    "            if (epoch + 1) % 1 == 0:\n",
    "                print(f\"Epochs[{epoch+1}] Loss: {loss:4f} - Accuracy: {accuracy:6f}\")\n",
    "                pass\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \n",
    "        :param X: (n, m) n features, m samples \n",
    "        :return: (n) y_pred\n",
    "        \"\"\"\n",
    "        return self.forward_propagate(0, X)\n",
    "\n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        \"\"\"\n",
    "        :param X_test: matrix(n, m) n - number of features, m - number of samples\n",
    "        :param Y_test: vector(n), labels in sparse\n",
    "        :return: [lost, accuracy] \n",
    "        \"\"\"\n",
    "        Y_pred = self.predict(X_test)\n",
    "        loss = self.compute_loss(Y_test, Y_pred)\n",
    "        accuracy = accuracy_score(Y_test, np.argmax(Y_pred, axis=0, keepdims=False))\n",
    "        return loss, accuracy\n",
    "    \n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:32:32.285138Z",
     "start_time": "2024-10-12T15:32:32.268084Z"
    }
   },
   "source": [
    "# split the train data\n",
    "X_train_20 = train_test_split(X_train, test_size=0.2, random_state=42)[1]\n",
    "Y_train_20 = train_test_split(y_train, test_size=0.2, random_state=42)[1]\n",
    "\n",
    "len(X_train_20)\n",
    "X_train_20.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:33:58.611691Z",
     "start_time": "2024-10-12T15:32:32.293332Z"
    }
   },
   "source": [
    "train_X = flat_data(X_train_20).T\n",
    "n = train_X.shape[0]\n",
    "m = train_X.shape[1]\n",
    "nn = NNClassification(input_size=n, output_size=10, learning_rate=1e-3)\n",
    "# rs1 = nn.fit(train_X, Y_train_20, epochs=50, batch_size=64, min_delta=1e-3)\n",
    "nn.fit(train_X, Y_train_20, epochs=50, batch_size=m)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs[1] Loss: 106.594470 - Accuracy: 0.065167\n",
      "Epochs[2] Loss: 73.286007 - Accuracy: 0.147000\n",
      "Epochs[3] Loss: 43.859319 - Accuracy: 0.226833\n",
      "Epochs[4] Loss: 26.702277 - Accuracy: 0.370750\n",
      "Epochs[5] Loss: 19.974856 - Accuracy: 0.496167\n",
      "Epochs[6] Loss: 15.570646 - Accuracy: 0.581250\n",
      "Epochs[7] Loss: 12.095761 - Accuracy: 0.641833\n",
      "Epochs[8] Loss: 10.409321 - Accuracy: 0.674417\n",
      "Epochs[9] Loss: 9.182378 - Accuracy: 0.701083\n",
      "Epochs[10] Loss: 8.306579 - Accuracy: 0.722417\n",
      "Epochs[11] Loss: 7.428924 - Accuracy: 0.742500\n",
      "Epochs[12] Loss: 6.469560 - Accuracy: 0.769917\n",
      "Epochs[13] Loss: 5.812111 - Accuracy: 0.787333\n",
      "Epochs[14] Loss: 5.433566 - Accuracy: 0.799167\n",
      "Epochs[15] Loss: 5.086721 - Accuracy: 0.808333\n",
      "Epochs[16] Loss: 4.702103 - Accuracy: 0.818250\n",
      "Epochs[17] Loss: 4.321432 - Accuracy: 0.826833\n",
      "Epochs[18] Loss: 3.995695 - Accuracy: 0.835417\n",
      "Epochs[19] Loss: 3.724837 - Accuracy: 0.842583\n",
      "Epochs[20] Loss: 3.492523 - Accuracy: 0.850417\n",
      "Epochs[21] Loss: 3.273596 - Accuracy: 0.855500\n",
      "Epochs[22] Loss: 3.077504 - Accuracy: 0.862917\n",
      "Epochs[23] Loss: 2.922192 - Accuracy: 0.865250\n",
      "Epochs[24] Loss: 2.806786 - Accuracy: 0.867333\n",
      "Epochs[25] Loss: 2.707650 - Accuracy: 0.869250\n",
      "Epochs[26] Loss: 2.599994 - Accuracy: 0.871917\n",
      "Epochs[27] Loss: 2.485025 - Accuracy: 0.875083\n",
      "Epochs[28] Loss: 2.375669 - Accuracy: 0.879667\n",
      "Epochs[29] Loss: 2.278454 - Accuracy: 0.882083\n",
      "Epochs[30] Loss: 2.187468 - Accuracy: 0.885250\n",
      "Epochs[31] Loss: 2.100554 - Accuracy: 0.888500\n",
      "Epochs[32] Loss: 2.016566 - Accuracy: 0.891917\n",
      "Epochs[33] Loss: 1.936961 - Accuracy: 0.895167\n",
      "Epochs[34] Loss: 1.863078 - Accuracy: 0.896750\n",
      "Epochs[35] Loss: 1.792107 - Accuracy: 0.899833\n",
      "Epochs[36] Loss: 1.723956 - Accuracy: 0.902500\n",
      "Epochs[37] Loss: 1.659593 - Accuracy: 0.904583\n",
      "Epochs[38] Loss: 1.600145 - Accuracy: 0.905167\n",
      "Epochs[39] Loss: 1.545599 - Accuracy: 0.907083\n",
      "Epochs[40] Loss: 1.495089 - Accuracy: 0.908583\n",
      "Epochs[41] Loss: 1.447601 - Accuracy: 0.911000\n",
      "Epochs[42] Loss: 1.402725 - Accuracy: 0.911750\n",
      "Epochs[43] Loss: 1.359350 - Accuracy: 0.912667\n",
      "Epochs[44] Loss: 1.317355 - Accuracy: 0.914667\n",
      "Epochs[45] Loss: 1.276215 - Accuracy: 0.915583\n",
      "Epochs[46] Loss: 1.235975 - Accuracy: 0.918000\n",
      "Epochs[47] Loss: 1.196775 - Accuracy: 0.918750\n",
      "Epochs[48] Loss: 1.159571 - Accuracy: 0.920917\n",
      "Epochs[49] Loss: 1.124926 - Accuracy: 0.922750\n",
      "Epochs[50] Loss: 1.092319 - Accuracy: 0.923333\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:33:58.637139Z",
     "start_time": "2024-10-12T15:33:58.619169Z"
    }
   },
   "source": [
    "# split the train data\n",
    "rand_seed = np.random.randint(low=0, high=1000)\n",
    "X_test_20 = train_test_split(X_train, test_size=0.1, random_state=rand_seed)[1]\n",
    "Y_test_20 = train_test_split(y_train, test_size=0.1, random_state=rand_seed)[1]"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:33:59.330048Z",
     "start_time": "2024-10-12T15:33:58.645764Z"
    }
   },
   "source": "nn.evaluate(flat_data(X_test_20).T, Y_test_20)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.8225434092920505, 0.8923333333333333)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\left(A^{[1]}\\right)^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[2]} } &= \n",
    "\\frac{1}{m}\\left(A^{[2]}-Y\\right)\\mathbf{1},\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial W^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)X^T,\\\\\n",
    "\\frac{\\partial \\mathcal{L} }{ \\partial b^{[1]}} &= \\frac{1}{m}\\left(\\left(W^{[2]}\\right)^T \\left(A^{[2]} - Y\\right)\\cdot \\left(A^{[1]}\\cdot\\left(1-A^{[1]}\\right)\\right)\\right)\\mathbf{1},\\\\\n",
    "\\tag{15}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:33:59.368401Z",
     "start_time": "2024-10-12T15:33:59.337661Z"
    }
   },
   "source": [
    "# Sample\n",
    "model = tf.keras.models.Sequential(layers=[\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28,)),\n",
    "    tf.keras.layers.Dense(units=128, activation=tf.keras.activations.relu),\n",
    "    tf.keras.layers.Dense(units=64, activation=tf.keras.activations.relu),\n",
    "    # tf.keras.layers.Dropout(rate=0.2),\n",
    "    # tf.keras.layers.Dense(units=10, activation=tf.keras.activations.linear)\n",
    "    tf.keras.layers.Dense(units=10, activation=tf.keras.activations.softmax)\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3), #tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\"])\n",
    "# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-3), #tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "#               loss=loss_fn,\n",
    "#               metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), #tf.keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=loss_fn,\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.579299Z",
     "start_time": "2024-10-12T15:33:59.376301Z"
    }
   },
   "source": [
    "#\n",
    "# model.fit(x=X_train_20, y=Y_train_20, epochs=50, batch_size=32, workers=8, use_multiprocessing=True)\n",
    "model.fit(x=X_train_20, y=Y_train_20, epochs=50, batch_size=X_train_20.shape[0], workers=8, use_multiprocessing=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 97.5646 - accuracy: 0.1098\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 22:33:59.392440: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 60.0266 - accuracy: 0.1748\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 39.6471 - accuracy: 0.2508\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 28.8825 - accuracy: 0.3532\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 21.9124 - accuracy: 0.4611\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 17.7390 - accuracy: 0.5334\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.0122 - accuracy: 0.5763\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.7935 - accuracy: 0.6118\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.9482 - accuracy: 0.6462\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.5238 - accuracy: 0.6702\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.5464 - accuracy: 0.6946\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.8821 - accuracy: 0.7096\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.3254 - accuracy: 0.7231\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.7285 - accuracy: 0.7369\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.0930 - accuracy: 0.7513\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.5031 - accuracy: 0.7682\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.0174 - accuracy: 0.7816\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.6424 - accuracy: 0.7911\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.3643 - accuracy: 0.7991\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.1318 - accuracy: 0.8056\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.9326 - accuracy: 0.8118\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.7578 - accuracy: 0.8167\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.5941 - accuracy: 0.8223\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.4354 - accuracy: 0.8250\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.2796 - accuracy: 0.8315\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.1319 - accuracy: 0.8356\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.9933 - accuracy: 0.8405\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.8657 - accuracy: 0.8438\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.7439 - accuracy: 0.8483\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6266 - accuracy: 0.8531\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5147 - accuracy: 0.8566\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.4121 - accuracy: 0.8611\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3226 - accuracy: 0.8630\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2415 - accuracy: 0.8649\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.1648 - accuracy: 0.8677\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0901 - accuracy: 0.8706\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0172 - accuracy: 0.8733\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9460 - accuracy: 0.8759\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.8781 - accuracy: 0.8793\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8140 - accuracy: 0.8812\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.7541 - accuracy: 0.8823\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6986 - accuracy: 0.8840\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6462 - accuracy: 0.8845\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5964 - accuracy: 0.8865\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.5491 - accuracy: 0.8879\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5028 - accuracy: 0.8891\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4571 - accuracy: 0.8921\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4124 - accuracy: 0.8942\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3691 - accuracy: 0.8960\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3276 - accuracy: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13db42350>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.713054Z",
     "start_time": "2024-10-12T15:34:00.587201Z"
    }
   },
   "source": [
    "rs3 = model.evaluate(X_test_20, Y_test_20, verbose=0)\n",
    "rs3  # [lost, accuracy]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.048210859298706, 0.8709999918937683]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.752696Z",
     "start_time": "2024-10-12T15:34:00.720875Z"
    }
   },
   "source": [
    "def show_image(img_data: np.ndarray) -> tuple:\n",
    "    fig, axes = plt.subplots(figsize=(1.60, 1.20))\n",
    "    axes.imshow(X=img_data, cmap=\"gray\")\n",
    "    return fig, axes\n",
    "\n",
    "# print(y_test[5854])\n",
    "# show_image(X_test[4823])\n",
    "fig, axes = show_image(X_test[5854])\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 160x120 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.764975Z",
     "start_time": "2024-10-12T15:34:00.760382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sample = flat_data(X_test[1200:1201]).T\n",
    "# print(X_test[1200:1201].reshape((-1, X_test[1200:1201].shape[1] * X_test[1200:1201].shape[2])))\n",
    "# print(test_sample.T)\n",
    "nnpredict = nn.predict(test_sample)\n",
    "nnpred = np.argmax(nnpredict, axis=0)\n",
    "print(nnpred)\n",
    "\n",
    "test_sample = flat_data(X_test[1203:1204]).T\n",
    "# print(test_sample.T)\n",
    "nnpredict = nn.predict(test_sample)\n",
    "nnpred = np.argmax(nnpredict, axis=0, keepdims=False)\n",
    "print(nnpred)\n",
    "# print(f\"{nn.predict(flat_data(X_test[1000:1001]).T).T} \\n {nn.predict(flat_data(X_test[1000:1001]).T).T}\")\n",
    "\n",
    "# nn.evaluate(flat_data(X_test_20).T, Y_test_20)\n",
    "# print(f\"{nn.predict(flat_data(X_test[1000:1001]).T).T} \\n {nn.predict(flat_data(X_test[1000:1001]).T).T}\")\n",
    "\n",
    "# nn.evaluate(flat_data(X_test_20).T, Y_test_20)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n",
      "[0]\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:35:57.728265Z",
     "start_time": "2024-10-12T15:35:57.687491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfpredict = model.predict(X_test[1200:1201], verbose=0)\n",
    "tfpred = np.argmax(tfpredict, axis=1, keepdims=False)\n",
    "print(tfpred)\n",
    "\n",
    "tfpredict = model.predict(X_test[1203:1204], verbose=0)\n",
    "tfpred = np.argmax(tfpredict, axis=1, keepdims=False)\n",
    "print(tfpred)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.842333Z",
     "start_time": "2024-10-12T15:34:00.827703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# np.array([y_test[1201]])\n",
    "nn.evaluate(flat_data(X_test[1201:1301]).T, np.array(y_test[1201:1301]))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.42678231239895, 0.77)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.875973Z",
     "start_time": "2024-10-12T15:34:00.851182Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(X_test[1201:1301], y_test[1201:1301], verbose=0)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.037131309509277, 0.7400000095367432]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2. Linear and Activation Function:"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loss/Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-12T15:34:00.894884Z",
     "start_time": "2024-10-12T15:34:00.892986Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
