{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.lines as mlines\n","import matplotlib.animation as animation\n","import time\n","import struct\n","import tensorflow as tf\n","import random as rd\n","import pickle as pickle\n","\n","from array import array\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","\n","# my project\n","from module.conf import PROJECT_DIR\n","\n","# %matplotlib tk\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_images_labels(data_filepath) -> tuple:\n","    labels = []\n","    images = []\n","    for path in data_filepath:\n","        with open(file = path, mode=\"rb\") as f:\n","            dict_data = pickle.load(file=f, encoding=\"bytes\")\n","            dd = dict_data[b'data']\n","            for ind, val in enumerate(dd):\n","                label = dict_data[b'labels'][ind]\n","                img = np.asarray(dd[ind], dtype=np.uint8).reshape(3, 32, 32).transpose(1, 2, 0)\n","                labels.append(label)\n","                images.append(img)\n","                pass\n","            pass\n","        pass\n","    return np.asarray(images), np.asarray(labels)\n","\n","def load_data() -> tuple:\n","    cifar_path = \"/data/sample/cifar-10-batches-py\"\n","    label_name_filepath = \"\".join([PROJECT_DIR, cifar_path, \"/batches.meta\"])\n","    training_data_filepaths = [\n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_1\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_2\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_3\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_4\"]), \n","        \"\".join([PROJECT_DIR, cifar_path, \"/data_batch_5\"]), \n","    ]\n","    test_data_filepaths = [\"\".join([PROJECT_DIR, cifar_path, \"/test_batch\"])]\n","    x_train, y_train = read_images_labels(training_data_filepaths[:])\n","    x_test, y_test = read_images_labels(test_data_filepaths)\n","    with open(file=label_name_filepath, mode=\"rb\") as f:\n","        label_names = pickle.load(file=f, encoding=\"bytes\")[b'label_names']\n","        pass\n","    return (x_train, y_train),(x_test, y_test), label_names\n","\n","(X_train, y_train), (X_test, y_test), label_names = load_data()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# print(X_train.dtype)\n","X_train = X_train.astype(dtype=np.float64) / 255\n","X_test = X_test.astype(dtype=np.float64) / 255\n","# for i in range(len(X_train)): X_train[i] /= 255\n","# for i in range(len(X_test)): X_test[i] /= 255 "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 32)        896       \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 2048)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               1049088   \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                5130      \n","                                                                 \n","=================================================================\n","Total params: 1,341,226\n","Trainable params: 1,341,226\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Input(shape=(32, 32, 3)))\n","#------------------------------------\n","# Conv Block 1: 32 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 2: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","\n","#------------------------------------\n","# Conv Block 3: 64 Filters, MaxPool.\n","#------------------------------------\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation='relu'))\n","model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    \n","#------------------------------------\n","# Flatten the convolutional features.\n","#------------------------------------\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(512, activation='relu'))\n","model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","\n","model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# loss_fn = tf.keras.losses.categorical_crossentropy\n","loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","# loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n","#               loss=tf.keras.losses.BinaryCrossentropy(),\n","#               metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.FalseNegatives()])\n","model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","              loss=loss_fn,\n","              metrics=[\"accuracy\", \"mae\"])\n","# model.compile(optimizer=tf.keras.optimizers.legacy.RMSprop(learning_rate=1e-3),\n","#             loss=loss_fn,\n","#             metrics=[\"accuracy\"])\n","\n","# model.compile(optimizer='rmsprop', \n","#               loss='sparse_categorical_crossentropy', \n","#               metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(x=X_train, y=y_train, epochs=32, batch_size=500, verbose=1)\n","# model.fit(x=X_train, y=y_train, epochs=16, batch_size=512, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate(X_test, y_test, verbose=2)\n","result = model.predict(x=np.asarray([X_test[0]], dtype=np.float64), verbose=1)\n","# result = tf.nn.softmax(result).numpy()\n","print(f\"{result}\")\n","print(f\"{result.argmax()} {y_test[0]}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["c = 0\n","cp = 0\n","for i in range(100):\n","    test_indx = rd.randint(0, len(y_test)-1)\n","    x_test_ = np.asarray([X_test[test_indx]])\n","\n","    # test_indx = rd.randint(0, len(y_train)-1)\n","    # x_test_ = np.asarray([X_train[test_indx]])\n","\n","    result = model.predict(x=x_test_, verbose=0) \n","#     result = tf.nn.softmax(result).numpy()\n","    y_test_ = y_test\n","    if result.max() >= 0.5:\n","        if result.argmax() != y_test_[test_indx]:\n","            c+=1\n","            print(f\"- [{i}]:img[{test_indx}]:{result}\\npred:{result.max()}\\npredict:{result.argmax()} {label_names[result.argmax()]} solve:{y_test_[test_indx]} {label_names[y_test_[test_indx]]}\")\n","    else:\n","        print(f\"can not predict:{test_indx}: {result.max()}\")\n","        cp+=1\n","print(f\"error: {c} can not pred:{cp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ind = 14\n","fig, ax = plt.subplots(figsize=(1.6, 1.2))\n","ax.imshow(X=X_train[ind])\n","plt.show()\n","print(f\"label[{y_train[ind]}]:{label_names[y_train[ind]]}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n"," PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tf.config.list_physical_devices()"]},{"cell_type":"markdown","metadata":{},"source":["2. Resnet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n","resnet_model = tf.keras.applications.ResNet50(input_shape=(32, 32, 3), include_top=True, weights=None, classes=10)\n","resnet_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-3),\n","              loss=loss_fn,\n","              metrics=[\"accuracy\", \"mae\"])\n","resnet_model.fit(x=X_train, y=y_train, epochs=5, batch_size=64, verbose=1)\n","\n","# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","# model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n","# model.fit(x_train, y_train, epochs=5, batch_size=64)"]}],"metadata":{"kernelspec":{"display_name":"learn-python","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
