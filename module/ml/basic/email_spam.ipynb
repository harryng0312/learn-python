{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-25T14:52:49.975020Z",
     "start_time": "2025-02-25T14:52:29.240087Z"
    }
   },
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# my project\n",
    "from module.conf import PROJECT_DIR"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T14:54:46.899116Z",
     "start_time": "2025-02-25T14:54:46.884288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv(PROJECT_DIR + \"/data/basic/email/spam.csv\", encoding=\"utf-8\")[['Category', 'Message']]  # v1 = label, v2 = text\n",
    "data.columns = ['label', 'text']\n",
    "\n",
    "# convert label Spam/Ham to number (Spam = 1, Ham = 0)\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['label'])\n",
    "\n",
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)"
   ],
   "id": "a02c7cc9bb3fe954",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:00:19.938917Z",
     "start_time": "2025-02-25T15:00:19.935399Z"
    }
   },
   "cell_type": "code",
   "source": "len(X_train)",
   "id": "5611240d7b449f4c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4457"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:00:30.363283Z",
     "start_time": "2025-02-25T15:00:30.360562Z"
    }
   },
   "cell_type": "code",
   "source": "sum(y_train)",
   "id": "6560c3e62ecb1064",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "598"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:22:38.896768Z",
     "start_time": "2025-02-25T15:22:38.823100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)  # get the 5000 most popular words\n",
    "# vectorizer = TfidfVectorizer(max_features=10_000)  # get all words\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer.transform(X_test).toarray()"
   ],
   "id": "72d45fe8d5b7e452",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:22:39.067654Z",
     "start_time": "2025-02-25T15:22:39.064657Z"
    }
   },
   "cell_type": "code",
   "source": "len(vectorizer.vocabulary_)",
   "id": "df655ad0f242bc0d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:22:29.569371Z",
     "start_time": "2025-02-25T15:22:29.564688Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_tfidf.shape",
   "id": "bc2ac0886d30fde3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4457, 7701)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:02:25.056704Z",
     "start_time": "2025-02-25T15:02:25.052116Z"
    }
   },
   "cell_type": "code",
   "source": "X_test_tfidf[:5]",
   "id": "cc6fec97af8a5283",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:06:43.429316Z",
     "start_time": "2025-02-25T15:06:43.378515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification (Spam / Ham)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "id": "9063284cd2bdddb6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:06:52.802507Z",
     "start_time": "2025-02-25T15:06:45.086257Z"
    }
   },
   "cell_type": "code",
   "source": "model.fit(X_train_tfidf, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf, y_test))",
   "id": "96d310445e5a7e18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:06:45.142094: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 [==============================] - 1s 6ms/step - loss: 0.2030 - accuracy: 0.9116 - val_loss: 0.0624 - val_accuracy: 0.9821\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0549 - val_accuracy: 0.9865\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0568 - val_accuracy: 0.9865\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.0613 - val_accuracy: 0.9910\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 3.9830e-04 - accuracy: 1.0000 - val_loss: 0.0635 - val_accuracy: 0.9883\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 2.2333e-04 - accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9919\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 1.2040e-04 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9919\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 8.7122e-05 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9919\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.2133e-05 - accuracy: 1.0000 - val_loss: 0.0716 - val_accuracy: 0.9919\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 1s 5ms/step - loss: 5.2956e-05 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 0.9919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16221d060>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:06:54.561814Z",
     "start_time": "2025-02-25T15:06:54.473945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss, accuracy = model.evaluate(X_test_tfidf, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "id": "25708b3ce8423b44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9919\n",
      "Accuracy: 99.19%\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LSTM",
   "id": "c5b1bb195ae3960b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:09:47.713150Z",
     "start_time": "2025-02-25T15:08:46.628377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, SpatialDropout1D\n",
    "\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "# Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 128, input_length=max_len),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_lstm.fit(X_train_pad, y_train, epochs=5, batch_size=64, validation_data=(X_test_pad, y_test))"
   ],
   "id": "6daa8da96a90393f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "70/70 [==============================] - 13s 170ms/step - loss: 0.2349 - accuracy: 0.9217 - val_loss: 0.0614 - val_accuracy: 0.9812\n",
      "Epoch 2/5\n",
      "70/70 [==============================] - 12s 171ms/step - loss: 0.0436 - accuracy: 0.9881 - val_loss: 0.0449 - val_accuracy: 0.9892\n",
      "Epoch 3/5\n",
      "70/70 [==============================] - 12s 173ms/step - loss: 0.0224 - accuracy: 0.9942 - val_loss: 0.0411 - val_accuracy: 0.9892\n",
      "Epoch 4/5\n",
      "70/70 [==============================] - 12s 173ms/step - loss: 0.0103 - accuracy: 0.9973 - val_loss: 0.0434 - val_accuracy: 0.9910\n",
      "Epoch 5/5\n",
      "70/70 [==============================] - 12s 174ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0417 - val_accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169e0ffa0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:10:55.555054Z",
     "start_time": "2025-02-25T15:10:54.864771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss, accuracy = model_lstm.evaluate(X_test_pad, y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "id": "c26bac751c127df4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 1s 19ms/step - loss: 0.0417 - accuracy: 0.9910\n",
      "Accuracy: 99.10%\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2907c6e0d8c97c1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. DF (Inverse Document Frequency): in TF-IDF.\n",
    "\n",
    "$$IDF(t) = \\log\\left(\\frac{N}{1 + DF(t)}\\right)$$\n",
    "\n",
    "- N: number of document.\n",
    "- IDF(t): number of document what contains t.\n",
    "- Add 1 to avoid devide by 0 error.\n",
    "\n",
    "2. TF (Term Frequency): another version of IDF, apply to process a word appear more times in a corpus.\n",
    "\n",
    "$$TF(t, d) = \\log\\left(\\frac{|d|}{1 + TF(t, d)}\\right)$$\n",
    "\n",
    "- |d|: number of words in corpus.\n",
    "- TF(t, d): the number of times word t appears in corpus d."
   ],
   "id": "99912128cc12e341"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:47:46.782952Z",
     "start_time": "2025-02-25T15:47:46.778804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "def compute_idf(corpus):\n",
    "    \"\"\" Calculate IDF \"\"\"\n",
    "    N = len(corpus)\n",
    "    word_doc_count = Counter()\n",
    "    for doc in corpus:\n",
    "        # uni_words = nltk.word_tokenize(doc)\n",
    "        unique_words = set(doc.split())\n",
    "        for word in unique_words:\n",
    "            word_doc_count[word] += 1\n",
    "            pass\n",
    "        pass\n",
    "    rs = {word : np.log(N / (1 + count)) for word, count in word_doc_count.items()}\n",
    "    return rs"
   ],
   "id": "70b84fc14cda96b7",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:47:46.924537Z",
     "start_time": "2025-02-25T15:47:46.921770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "documents = [\n",
    "    \"học máy là một nhánh của trí tuệ nhân tạo\",\n",
    "    \"học sâu là một phần của học máy\",\n",
    "    \"trí tuệ nhân tạo có nhiều ứng dụng trong đời sống\"\n",
    "]\n",
    "idf_values = compute_idf(documents)\n",
    "print(idf_values)"
   ],
   "id": "14a64b05dd45670",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tạo': 0.0, 'nhánh': 0.4054651081081644, 'nhân': 0.0, 'một': 0.0, 'là': 0.0, 'của': 0.0, 'trí': 0.0, 'tuệ': 0.0, 'máy': 0.0, 'học': 0.0, 'sâu': 0.4054651081081644, 'phần': 0.4054651081081644, 'nhiều': 0.4054651081081644, 'có': 0.4054651081081644, 'ứng': 0.4054651081081644, 'đời': 0.4054651081081644, 'dụng': 0.4054651081081644, 'sống': 0.4054651081081644, 'trong': 0.4054651081081644}\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:49:01.842076Z",
     "start_time": "2025-02-25T15:49:01.838411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_itf(document):\n",
    "    \"\"\" Calculate ITF \"\"\"\n",
    "    words = document.split()\n",
    "    total_words = len(words)\n",
    "    word_count = Counter(words)\n",
    "\n",
    "    # TF\n",
    "    itf_scores = {word: np.log(total_words / (1 + count)) for word, count in word_count.items()}\n",
    "    return itf_scores"
   ],
   "id": "91d5f2801a908e72",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:49:02.629148Z",
     "start_time": "2025-02-25T15:49:02.625359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_doc = \"học máy là một lĩnh vực của trí tuệ nhân tạo học máy rất quan trọng\"\n",
    "itf_values = compute_itf(sample_doc)\n",
    "print(itf_values)"
   ],
   "id": "f6e01b4fe5f2d092",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'học': 1.6739764335716716, 'máy': 1.6739764335716716, 'là': 2.0794415416798357, 'một': 2.0794415416798357, 'lĩnh': 2.0794415416798357, 'vực': 2.0794415416798357, 'của': 2.0794415416798357, 'trí': 2.0794415416798357, 'tuệ': 2.0794415416798357, 'nhân': 2.0794415416798357, 'tạo': 2.0794415416798357, 'rất': 2.0794415416798357, 'quan': 2.0794415416798357, 'trọng': 2.0794415416798357}\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:59:24.649365Z",
     "start_time": "2025-02-25T15:59:24.643693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer1 = TfidfVectorizer(max_features=None)\n",
    "vectorizer1.fit(documents)\n",
    "X_train_tfidf_1 = vectorizer.transform(documents).toarray()\n",
    "# X_train_tfidf_1 = vectorizer1.fit_transform(documents).toarray()\n",
    "X_test_tfidf_2 = vectorizer.transform(documents).toarray()"
   ],
   "id": "cc7fb2294b8c4ed8",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:59:25.586990Z",
     "start_time": "2025-02-25T15:59:25.582400Z"
    }
   },
   "cell_type": "code",
   "source": "X_train_tfidf_1",
   "id": "36cdb15484316348",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.30529678, 0.        , 0.30529678, 0.30529678,\n",
       "        0.30529678, 0.30529678, 0.        , 0.40142857, 0.30529678,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.30529678,\n",
       "        0.30529678, 0.30529678, 0.        , 0.        ],\n",
       "       [0.        , 0.29542622, 0.        , 0.59085245, 0.29542622,\n",
       "        0.29542622, 0.29542622, 0.        , 0.        , 0.        ,\n",
       "        0.38844998, 0.38844998, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.32767345, 0.        , 0.32767345, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32767345, 0.        , 0.24920411,\n",
       "        0.        , 0.        , 0.32767345, 0.32767345, 0.24920411,\n",
       "        0.24920411, 0.24920411, 0.32767345, 0.32767345]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T15:54:43.733887Z",
     "start_time": "2025-02-25T15:54:43.715806Z"
    }
   },
   "cell_type": "code",
   "source": "vectorizer1",
   "id": "bb4ec5c4f5639e69",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[91], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mvectorizer1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocabulary_\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TfidfVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c562dda39aa68d72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
